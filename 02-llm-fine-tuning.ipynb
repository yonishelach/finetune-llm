{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57360a3",
   "metadata": {},
   "source": [
    "# Train, Test, and Redeploy Our LLM\n",
    "\n",
    "This notebook demonstrates how to train a Hugging Face LLM, fine-tunning it to a specific data and build an automated ML pipeline for data collection and preparation, training, evaluation, testing and deployment of an application pipeline with the new LLM.\n",
    "\n",
    "The [**previous notebook**](./01-serving.ipynb) used the standard `GPT2-Medium` from Hugging Face models which generated ppoorly when asking itt about MLOps, butt it is all about tto change.\n",
    "\n",
    "The tutorial has two main steps:\n",
    "1. [Define MLRun project and set all mlrun function](#project-setup)\n",
    "2. [Run full LLM life-cycle workflow](#full-workflow)\n",
    "3. [Try the new model with Gradio](#use-gradio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e3fb3",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"project-setup\"></a>\n",
    "## 1. Define MLRun project and set all mlrun function\n",
    "\n",
    "Create or load an MLRun project that holds all your functions and configuration ([**project_setup.py**](./src/project_setup.py))\n",
    "\n",
    "The project contains the following files:\n",
    "* [data_collection.py](./src/data_collection.py) - Collect all text from given list of urls.\n",
    "* [data_preprocess.py](./src/data_preprocess.py) - Preprocess the data and save it as a `pd.DataFrame` dataset arttifact.\n",
    "* [training.py]() - Train and evaluate using HuggingFace `Trainer` API and DeepSpeed empowered by **MLRun's auto-logging** (`apply_mlrun` function).\n",
    "* [serving.py](./src/serving.py) - Multiple model servers and serving steps to build the **Serving Graph** from notebook 01.\n",
    "* [testing.py](./src/testing.py) - Stress testing the serving graph.\n",
    "\n",
    "And a training pipeline (in [**training_workflow.py**](./src/training_workflow.py))\n",
    "\n",
    "The training and evaluation function we will use is [hugging_face_classifier_trainer](https://www.mlrun.org/hub/). It is taken from [**MLRun's Functions Hub**](https://docs.mlrun.org/en/stable/runtimes/load-from-hub.html) - a collection of ready to be imported functions for variety of use cases. We import the function during the project setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26da2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-03-06 09:49:51,927 [info] loaded project huggingface-demo from MLRun DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Names with underscore '_' are about to be deprecated, use dashes '-' instead. Replacing underscores with dashes.\n",
      "Names with underscore '_' are about to be deprecated, use dashes '-' instead. Replacing underscores with dashes.\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "from src.project_setup import create_and_set_project\n",
    "\n",
    "project = create_and_set_project(\n",
    "    git_source=\"git://github.com/yonishelach/learn-docs.git#main\",\n",
    "    name=\"mlopspedia\",\n",
    "    default_image=\"yonishelach/mlrun-hf-gpu\",\n",
    "    user_project=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743c879",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"full-workflow\"></a>\n",
    "## 2. Run full LLM life-cycle workflow\n",
    "\n",
    "Run the training pipeline (in [training_workflow.py](./src/training_workflow.py)) by using `project.run(workflow name, ...)`:\n",
    "\n",
    "    collect_html_to_text_files -> prepare_dataset -> train -> (serving graph) -> model_server_tester\n",
    "                                                           -> evaluate\n",
    "                                                          \n",
    "* `collect_html_to_text_files` (Data Collection) - Collect all text from given html urls into `.txt` files.\n",
    "* `prepare_dataset` (Preprocess Data) - Join the `.txt` files, reformatting the text into our \"Subject - Content\" prompt template. We made every header (`<h>` tags) a *subject* of a prompt, and the text (`<p>` tags) under it as its *content*.\n",
    "* `train` - Fine-tune the LLM on the data. We'll run the training on **OpenMPI** and we will use **DeepSpeed** for distributing the model and data between multiple workers, splitting the work between nodes and GPUs.\n",
    "* `evaluate` - Evaluate our model using the *Perplexity* metric.\n",
    "* Deployment - Deploy the same serving graph we saw on notebook 01: `-> preprocess -> llm -> postprocess -> toxicity classifier ->`\n",
    "* `model_server_tester` (Stress Test) - Send data to our serving endpoint and get a performance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_run = project.run(\n",
    "    name=\"training_workflow\",\n",
    "    arguments={\n",
    "        \"dataset_name\": \"Shayanvsf/US_Airline_Sentiment\",\n",
    "        \"pretrained_tokenizer\": \"distilbert-base-uncased\",\n",
    "        \"pretrained_model\": \"distilbert-base-uncased\",\n",
    "        \"TRAIN_output_dir\": \"finetuning-sentiment-model-3000-samples\",\n",
    "        \"TRAIN_num_train_epochs\": 1,\n",
    "        \"TRAIN_evaluation_strategy\": \"epoch\",\n",
    "        \"CLASS_num_labels\": 2\n",
    "    },\n",
    "    watch=True,\n",
    "    dirty=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c1f92",
   "metadata": {},
   "source": [
    "Here we can see how the workflow looks on our UI & Also the result of the trainer and the server_tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e633bf5",
   "metadata": {},
   "source": [
    "<img src=\"./images/workflow.png\" alt=\"workflow\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513acacf",
   "metadata": {},
   "source": [
    "<img src=\"./images/latancy.png\" alt=\"latancy\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f13288",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"use-gradio\"></a>\n",
    "## Try the new model with Gradio\n",
    "\n",
    "Once the pipeline completes, you can try the model using the function `invoke()` method or Gradio. You can get the new function object using the project `get_function()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e7767",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function = project.get_function(\"serving-trained-onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93080c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = \"i love flying\"\n",
    "response = serving_function.invoke(path='/predict', body=body)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e510d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "serving_function._get_state()\n",
    "serving_url = serving_function._resolve_invocation_url(\"\", False)\n",
    "\n",
    "def sentiment(text):\n",
    "    # call the serving function with the input text\n",
    "    resp = requests.post(serving_url, json={\"text\": text})\n",
    "    return resp.json()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    input_box = [gr.Textbox(label=\"Text to analyze\", placeholder=\"Please insert text\", value=\"It was Terrible fight\")]\n",
    "    output = [gr.Textbox(label=\"Sentiment result\"), gr.Textbox(label=\"Sentiment score\")]\n",
    "    greet_btn = gr.Button(\"Submit\")\n",
    "    greet_btn.click(fn=sentiment, inputs=input_box, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8599145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1295d88",
   "metadata": {},
   "source": [
    "**Done !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-extended",
   "language": "python",
   "name": "conda-env-mlrun-extended-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
