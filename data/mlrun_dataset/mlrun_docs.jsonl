{"text": "API by module#\nMLRun is organized into the following modules. The most common functions are exposed in the mlrun module, which is the recommended starting point.\nmlrun.frameworks\nmlrun\nmlrun.artifacts\nmlrun.config\nmlrun.datastore\nmlrun.db\nmlrun.execution\nmlrun.feature_store\nmlrun.model\nmlrun.platforms\nmlrun.projects\nmlrun.run\nmlrun.runtimes\nmlrun.serving\nstorey.transformations - Graph transformations\nSee also the index of all functions and classes."}
{"text": "mlrun.artifacts#\nmlrun.artifacts.get_model(model_dir, suffix='')[source]#\nreturn model file, model spec object, and list of extra data items\nthis function will get the model file, metadata, and extra data\nthe returned model file is always local, when using remote urls\n(such as v3io://, s3://, store://, ..) it will be copied locally.\nreturned extra data dict (of key, DataItem objects) allow reading additional model files/objects\ne.g. use DataItem.get() or .download(target) .as_df() to read\nexample:\nmodel_file, model_artifact, extra_data = get_model(models_path, suffix='.pkl')\nmodel = load(open(model_file, \"rb\"))\ncategories = extra_data['categories'].as_df()\nParameters\nmodel_dir – model dir or artifact path (store://..) or DataItem\nsuffix – model filename suffix (when using a dir)\nReturns\nmodel filename, model artifact object, extra data dict\nmlrun.artifacts.update_model(model_artifact, parameters: Optional[dict] = None, metrics: Optional[dict] = None, extra_data: Optional[dict] = None, inputs: Optional[List[mlrun.features.Feature]] = None, outputs: Optional[List[mlrun.features.Feature]] = None, feature_vector: Optional[str] = None, feature_weights: Optional[list] = None, key_prefix: str = '', labels: Optional[dict] = None, write_spec_copy=True, store_object: bool = True)[source]#\nUpdate model object attributes\nthis method will edit or add attributes to a model object\nexample:\nupdate_model(model_path, metrics={'speed': 100},\nextra_data={'my_data': b'some text', 'file': 's3://mybucket/..'})\nParameters\nmodel_artifact – model artifact object or path (store://..) or DataItem\nparameters – parameters dict\nmetrics – model metrics e.g. accuracy\nextra_data – extra data items key, value dict\n(value can be: path string | bytes | artifact)\ninputs – list of input features (feature vector schema)\noutputs – list of output features (output vector schema)\nfeature_vector – feature store feature vector uri (store://feature-vectors/<project>/<name>[:tag])\nfeature_weights – list of feature weights, one per input column\nkey_prefix – key prefix to add to metrics and extra data items\nlabels – metadata labels\nwrite_spec_copy – write a YAML copy of the spec to the target dir\nstore_object – Whether to store the model artifact updated."}
{"text": "mlrun.config#\nConfiguration system.\nConfiguration can be in either a configuration file specified by\nMLRUN_CONFIG_FILE environment variable or by environment variables.\nEnvironment variables are in the format “MLRUN_httpdb__port=8080”. This will be\nmapped to config.httpdb.port. Values should be in JSON format.\nclass mlrun.config.Config(cfg=None)[source]#\nBases: object\nproperty dask_kfp_image#\nSee kfp_image property docstring for why we’re defining this property\nproperty dbpath#\nstatic decode_base64_config_and_load_to_object(attribute_path: str, expected_type=<class 'dict'>)[source]#\ndecodes and loads the config attribute to expected type\n:param attribute_path: the path in the default_config e.g. preemptible_nodes.node_selector\n:param expected_type: the object type valid values are : dict, list etc…\n:return: the expected type instance\ndump_yaml(stream=None)[source]#\nclassmethod from_dict(dict_)[source]#\nstatic get_build_args()[source]#\nget_default_function_node_selector() → dict[source]#\nstatic get_default_function_pod_requirement_resources(requirement: str, with_gpu: bool = True)[source]#\nParameters\nrequirement – kubernetes requirement resource one of the following : requests, limits\nwith_gpu – whether to return requirement resources with nvidia.com/gpu field (e.g. you cannot specify\nGPU requests without specifying GPU limits) https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/\nReturns\na dict containing the defaults resources (cpu, memory, nvidia.com/gpu)\nget_default_function_pod_resources(with_gpu_requests=False, with_gpu_limits=False)[source]#\nget_default_function_security_context() → dict[source]#\nstatic get_hub_url()[source]#\nstatic get_parsed_igz_version() → Optional[semver.VersionInfo][source]#\nget_preemptible_node_selector() → dict[source]#\nget_preemptible_tolerations() → list[source]#\nstatic get_security_context_enrichment_group_id(user_unix_id: int) → int[source]#\nstatic get_storage_auto_mount_params()[source]#\nstatic get_valid_function_priority_class_names()[source]#\nproperty iguazio_api_url#\nwe want to be able to run with old versions of the service who runs the API (which doesn’t configure this\nvalue) so we’re doing best effort to try and resolve it from other configurations\nTODO: Remove this hack when 0.6.x is old enough\nis_api_running_on_k8s()[source]#\nis_nuclio_detected()[source]#\nstatic is_pip_ca_configured()[source]#\nis_preemption_nodes_configured()[source]#\nstatic is_running_on_iguazio() → bool[source]#\nproperty kfp_image#\nWhen this configuration is not set we want to set it to mlrun/mlrun, but we need to use the enrich_image method.\nThe problem is that the mlrun.utils.helpers module is importing the config (this) module, so we must import the\nmodule inside this function (and not on initialization), and then calculate this property value here.\nstatic reload()[source]#\nresolve_chief_api_url() → str[source]#\nresolve_kfp_url(namespace=None)[source]#\nresolve_runs_monitoring_missing_runtime_resources_debouncing_interval()[source]#\nstatic resolve_ui_url()[source]#\nto_dict()[source]#\nupdate(cfg)[source]#\nverify_security_context_enrichment_mode_is_allowed()[source]#\nproperty version#\nmlrun.config.read_env(env=None, prefix='MLRUN_')[source]#\nRead configuration from environment"}
{"text": "mlrun.datastore#\nclass mlrun.datastore.BigQuerySource(name: str = '', table: Optional[str] = None, max_results_for_table: Optional[int] = None, query: Optional[str] = None, materialization_dataset: Optional[str] = None, chunksize: Optional[int] = None, key_field: Optional[str] = None, time_field: Optional[str] = None, schedule: Optional[str] = None, start_time=None, end_time=None, gcp_project: Optional[str] = None, spark_options: Optional[dict] = None)[source]#\nBases: mlrun.datastore.sources.BaseSourceDriver\nReads Google BigQuery query results as input source for a flow.\nexample:\n# use sql query\nquery_string = \"SELECT * FROM `the-psf.pypi.downloads20210328` LIMIT 5000\"\nsource = BigQuerySource(\"bq1\", query=query_string,\ngcp_project=\"my_project\",\nmaterialization_dataset=\"dataviews\")\n# read a table\nsource = BigQuerySource(\"bq2\", table=\"the-psf.pypi.downloads20210328\", gcp_project=\"my_project\")\nParameters\nname – source name\ntable – table name/path, cannot be used together with query\nquery – sql query string\nmaterialization_dataset – for query with spark, The target dataset for the materialized view.\nThis dataset should be in same location as the view or the queried tables.\nmust be set to a dataset where the GCP user has table creation permission\nchunksize – number of rows per chunk (default large single chunk)\nkey_field – the column to be used as the key for events. Can be a list of keys.\ntime_field – the column to be parsed as the timestamp for events. Defaults to None\nschedule – string to configure scheduling of the ingestion job. For example ‘*/30 * * * *’ will\ncause the job to run every 30 minutes\nstart_time – filters out data before this time\nend_time – filters out data after this time\ngcp_project – google cloud project name\nspark_options – additional spark read options\nis_iterator()[source]#\nkind = 'bigquery'#\nsupport_spark = True#\nsupport_storey = False#\nto_dataframe()[source]#\nto_spark_df(session, named_view=False)[source]#\nclass mlrun.datastore.CSVSource(name: str = '', path: Optional[str] = None, attributes: Optional[Dict[str, str]] = None, key_field: Optional[str] = None, time_field: Optional[str] = None, schedule: Optional[str] = None, parse_dates: Optional[Union[List[int], List[str]]] = None)[source]#\nBases: mlrun.datastore.sources.BaseSourceDriver\nReads CSV file as input source for a flow.\nParameters\nname – name of the source\npath – path to CSV file\nkey_field – the CSV field to be used as the key for events. May be an int (field index) or string\n(field name) if with_header is True. Defaults to None (no key). Can be a list of keys.\ntime_field – the CSV field to be parsed as the timestamp for events. May be an int (field index) or\nstring (field name) if with_header is True. Defaults to None (no timestamp field). The field will be parsed\nfrom isoformat (ISO-8601 as defined in datetime.fromisoformat()). In case the format is not isoformat,\ntimestamp_format (as defined in datetime.strptime()) should be passed in attributes.\nschedule – string to configure scheduling of the ingestion job.\nattributes – additional parameters to pass to storey. For example:\nattributes={“timestamp_format”: ‘%Y%m%d%H’}\nparse_dates – Optional. List of columns (names or integers, other than time_field) that will be\nattempted to parse as date column.\nget_spark_options()[source]#\nis_iterator()[source]#\nkind = 'csv'#\nsupport_spark = True#\nsupport_storey = True#\nto_dataframe()[source]#\nto_spark_df(session, named_view=False)[source]#\nto_step(key_field=None, time_field=None, context=None)[source]#\nclass mlrun.datastore.CSVTarget(name: str = '', path=None, attributes: Optional[Dict[str, str]] = None, after_step=None, columns=None, partitioned: bool = False, key_bucketing_number: Optional[int] = None, partition_cols: Optional[List[str]] = None, time_partitioning_granularity: Optional[str] = None, after_state=None, max_events: Optional[int] = None, flush_after_seconds: Optional[int] = None, storage_options: Optional[Dict[str, str]] = None)[source]#\nBases: mlrun.datastore.targets.BaseStoreTarget\nadd_writer_state(graph, after, features, key_columns=None, timestamp_key=None)[source]#\nadd_writer_step(graph, after, features, key_columns=None, timestamp_key=None, featureset_status=None)[source]#\nas_df(columns=None, df_module=None, entities=None, start_time=None, end_time=None, time_column=None, **kwargs)[source]#\nreturn the target data as dataframe\nget_spark_options(key_column=None, timestamp_key=None, overwrite=True)[source]#\nis_offline = True#\nis_single_file()[source]#\nkind: str = 'csv'#\nprepare_spark_df(df)[source]#\nsuffix = '.csv'#\nsupport_spark = True#\nsupport_storey = True#\nclass mlrun.datastore.DataItem(key: str, store: mlrun.datastore.base.DataStore, subpath: str, url: str = '', meta=None, artifact_url=None)[source]#\nBases: object\nData input/output class abstracting access to various local/remote data sources\nDataItem objects are passed into functions and can be used inside the function, when a function run completes\nusers can access the run data via the run.artifact(key) which returns a DataItem object.\nusers can also convert a data url (e.g. s3://bucket/key.csv) to a DataItem using mlrun.get_dataitem(url).\nExample:\n# using data item inside a function\ndef my_func(context, data: DataItem):\ndf = data.as_df()\n# reading run results using DataItem (run.artifact())\ntrain_run = train_iris_func.run(inputs={'dataset': dataset},\nparams={'label_column': 'label'})\ntrain_run.artifact('confusion-matrix').show()\ntest_set = train_run.artifact('test_set').as_df()\n# create and use DataItem from uri\ndata = mlrun.get_dataitem('http://xyz/data.json').get()\nproperty artifact_url#\nDataItem artifact url (when its an artifact) or url for simple dataitems\nas_df(columns=None, df_module=None, format='', **kwargs)[source]#\nreturn a dataframe object (generated from the dataitem).\nParameters\ncolumns – optional, list of columns to select\ndf_module – optional, py module used to create the DataFrame (e.g. pd, dd, cudf, ..)\nformat – file format, if not specified it will be deducted from the suffix\ndelete()[source]#\ndelete the object from the datastore\ndownload(target_path)[source]#\ndownload to the target dir/path\nParameters\ntarget_path – local target path for the downloaded item\nget(size=None, offset=0, encoding=None)[source]#\nread all or a byte range and return the content\nParameters\nsize – number of bytes to get\noffset – fetch from offset (in bytes)\nencoding – encoding (e.g. “utf-8”) for converting bytes to str\nproperty key#\nDataItem key\nproperty kind#\nDataItem store kind (file, s3, v3io, ..)\nlistdir()[source]#\nreturn a list of child file names\nlocal()[source]#\nget the local path of the file, download to tmp first if its a remote object\nls()[source]#\nreturn a list of child file names\nproperty meta#\nArtifact Metadata, when the DataItem is read from the artifacts store\nopen(mode)[source]#\nreturn fsspec file handler, if supported\nput(data, append=False)[source]#\nwrite/upload the data, append is only supported by some datastores\nParameters\ndata – data (bytes/str) to write\nappend – append data to the end of the object, NOT SUPPORTED BY SOME OBJECT STORES!\nshow(format=None)[source]#\nshow the data object content in Jupyter\nParameters\nformat – format to use (when there is no/wrong suffix), e.g. ‘png’\nstat()[source]#\nreturn FileStats class (size, modified, content_type)\nproperty store#\nDataItem store object\nproperty suffix#\nDataItem suffix (file extension) e.g. ‘.png’\nupload(src_path)[source]#\nupload the source file (src_path)\nParameters\nsrc_path – source file path to read from and upload\nproperty url#\n//bucket/path\nType\nDataItem url e.g. /dir/path, s3\nclass mlrun.datastore.HttpSource(name: Optional[str] = None, path: Optional[str] = None, attributes: Optional[Dict[str, str]] = None, key_field: Optional[str] = None, time_field: Optional[str] = None, workers: Optional[int] = None)[source]#\nBases: mlrun.datastore.sources.OnlineSource\nadd_nuclio_trigger(function)[source]#\nkind = 'http'#\nclass mlrun.datastore.KafkaSource(brokers=None, topics=None, group='serving', initial_offset='earliest', partitions=None, sasl_user=None, sasl_pass=None, attributes=None, **kwargs)[source]#\nBases: mlrun.datastore.sources.OnlineSource\nSets kafka source for the flow\nSets kafka source for the flow\nParameters\nbrokers – list of broker IP addresses\ntopics – list of topic names on which to listen.\ngroup – consumer group. Default “serving”\ninitial_offset – from where to consume the stream. Default earliest\npartitions – Optional, A list of partitions numbers for which the function receives events.\nsasl_user – Optional, user name to use for sasl authentications\nsasl_pass – Optional, password to use for sasl authentications\nattributes – Optional, extra attributes to be passed to kafka trigger\nadd_nuclio_trigger(function)[source]#\nkind = 'kafka'#\nclass mlrun.datastore.NoSqlTarget(*args, **kwargs)[source]#\nBases: mlrun.datastore.targets.NoSqlBaseTarget\nget_table_object()[source]#\nget storey Table object\nkind: str = 'nosql'#\nsupport_spark = True#\nwriter_step_name = 'NoSqlTarget'#\nclass mlrun.datastore.ParquetSource(name: str = '', path: Optional[str] = None, attributes: Optional[Dict[str, str]] = None, key_field: Optional[str] = None, time_field: Optional[str] = None, schedule: Optional[str] = None, start_time: Optional[Union[datetime.datetime, str]] = None, end_time: Optional[Union[datetime.datetime, str]] = None)[source]#\nBases: mlrun.datastore.sources.BaseSourceDriver\nReads Parquet file/dir as input source for a flow.\nParameters\nname – name of the source\npath – path to Parquet file or directory\nkey_field – the column to be used as the key for events. Can be a list of keys.\ntime_field – the column to be parsed as the timestamp for events. Defaults to None\nstart_filter – datetime. If not None, the results will be filtered by partitions and\n‘filter_column’ > start_filter. Default is None\nend_filter – datetime. If not None, the results will be filtered by partitions\n‘filter_column’ <= end_filter. Default is None\nfilter_column – Optional. if not None, the results will be filtered by this column and\nstart_filter & end_filter\nschedule – string to configure scheduling of the ingestion job. For example ‘*/30 * * * *’ will\ncause the job to run every 30 minutes\nstart_time – filters out data before this time\nend_time – filters out data after this time\nattributes – additional parameters to pass to storey.\nproperty end_time#\nget_spark_options()[source]#\nkind = 'parquet'#\nproperty start_time#\nsupport_spark = True#\nsupport_storey = True#\nto_dataframe()[source]#\nto_step(key_field=None, time_field=None, start_time=None, end_time=None, context=None)[source]#\nclass mlrun.datastore.ParquetTarget(name: str = '', path=None, attributes: Optional[Dict[str, str]] = None, after_step=None, columns=None, partitioned: Optional[bool] = None, key_bucketing_number: Optional[int] = None, partition_cols: Optional[List[str]] = None, time_partitioning_granularity: Optional[str] = None, after_state=None, max_events: Optional[int] = 10000, flush_after_seconds: Optional[int] = 900, storage_options: Optional[Dict[str, str]] = None)[source]#\nBases: mlrun.datastore.targets.BaseStoreTarget\nparquet target storage driver, used to materialize feature set/vector data into parquet files\nParameters\nname – optional, target name. By default will be called ParquetTarget\npath – optional, Output path. Can be either a file or directory.\nThis parameter is forwarded as-is to pandas.DataFrame.to_parquet().\nDefault location v3io:///projects/{project}/FeatureStore/{name}/parquet/\nattributes – optional, extra attributes for storey.ParquetTarget\nafter_step – optional, after what step in the graph to add the target\ncolumns – optional, which columns from data to write\npartitioned – optional, whether to partition the file, False by default,\nif True without passing any other partition field, the data will be partitioned by /year/month/day/hour\nkey_bucketing_number – optional, None by default will not partition by key,\n0 will partition by the key as is, any other number X will create X partitions and hash the keys to one of them\npartition_cols – optional, name of columns from the data to partition by\ntime_partitioning_granularity – optional. the smallest time unit to partition the data by.\nFor example “hour” will yield partitions of the format /year/month/day/hour\nmax_events – optional. Maximum number of events to write at a time.\nAll events will be written on flow termination,\nor after flush_after_seconds (if flush_after_seconds is set). Default 10k events\nflush_after_seconds – optional. Maximum number of seconds to hold events before they are written.\nAll events will be written on flow termination, or after max_events are accumulated (if max_events is set).\nDefault 15 minutes\nadd_writer_state(graph, after, features, key_columns=None, timestamp_key=None)[source]#\nadd_writer_step(graph, after, features, key_columns=None, timestamp_key=None, featureset_status=None)[source]#\nas_df(columns=None, df_module=None, entities=None, start_time=None, end_time=None, time_column=None, **kwargs)[source]#\nreturn the target data as dataframe\nget_dask_options()[source]#\nget_spark_options(key_column=None, timestamp_key=None, overwrite=True)[source]#\nis_offline = True#\nis_single_file()[source]#\nkind: str = 'parquet'#\nsupport_append = True#\nsupport_dask = True#\nsupport_spark = True#\nsupport_storey = True#\nclass mlrun.datastore.StreamSource(name='stream', group='serving', seek_to='earliest', shards=1, retention_in_hours=24, extra_attributes: Optional[dict] = None, **kwargs)[source]#\nBases: mlrun.datastore.sources.OnlineSource\nSets stream source for the flow. If stream doesn’t exist it will create it\nSets stream source for the flow. If stream doesn’t exist it will create it\nParameters\nname – stream name. Default “stream”\ngroup – consumer group. Default “serving”\nseek_to – from where to consume the stream. Default earliest\nshards – number of shards in the stream. Default 1\nretention_in_hours – if stream doesn’t exist and it will be created set retention time. Default 24h\nextra_attributes – additional nuclio trigger attributes (key/value dict)\nadd_nuclio_trigger(function)[source]#\nkind = 'v3ioStream'#\nclass mlrun.datastore.StreamTarget(name: str = '', path=None, attributes: Optional[Dict[str, str]] = None, after_step=None, columns=None, partitioned: bool = False, key_bucketing_number: Optional[int] = None, partition_cols: Optional[List[str]] = None, time_partitioning_granularity: Optional[str] = None, after_state=None, max_events: Optional[int] = None, flush_after_seconds: Optional[int] = None, storage_options: Optional[Dict[str, str]] = None)[source]#\nBases: mlrun.datastore.targets.BaseStoreTarget\nadd_writer_state(graph, after, features, key_columns=None, timestamp_key=None)[source]#\nadd_writer_step(graph, after, features, key_columns=None, timestamp_key=None, featureset_status=None)[source]#\nas_df(columns=None, df_module=None, **kwargs)[source]#\nreturn the target data as dataframe\nis_online = False#\nis_table = False#\nkind: str = 'stream'#\nsupport_append = True#\nsupport_spark = False#\nsupport_storey = True#\nmlrun.datastore.get_store_resource(uri, db=None, secrets=None, project=None)[source]#\nget store resource object by uri"}
{"text": "mlrun.db#\nclass mlrun.db.httpdb.HTTPRunDB(base_url, user='', password='', token='')[source]#\nBases: mlrun.db.base.RunDBInterface\nInterface for accessing and manipulating the mlrun persistent store, maintaining the full state\nand catalog of objects that MLRun uses. The HTTPRunDB class serves as a client-side proxy to the MLRun\nAPI service which maintains the actual data-store, accesses the server through REST APIs.\nThe class provides functions for accessing and modifying the various objects that are used by MLRun in its\noperation. The functions provided follow some standard guidelines, which are:\nEvery object in MLRun exists in the context of a project (except projects themselves). When referencing an object\nthrough any API, a project name must be provided. The default for most APIs is for an empty project name, which\nwill be replaced by the name of the default project (usually default). Therefore, if performing an API to\nlist functions, for example, and not providing a project name - the result will not be functions from all\nprojects but rather from the default project.\nMany objects can be assigned labels, and listed/queried by label. The label parameter for query APIs allows for\nlisting objects that:\nHave a specific label, by asking for label=\"<label_name>\". In this case the actual value of the label\ndoesn’t matter and every object with that label will be returned\nHave a label with a specific value. This is done by specifying label=\"<label_name>=<label_value>\". In this\ncase only objects whose label matches the value will be returned\nMost objects have a create method as well as a store method. Create can only be called when such an\ndoes not exist yet, while store allows for either creating a new object or overwriting an existing object.\nSome objects have a versioned option, in which case overwriting the same object with a different version of\nit does not delete the previous version, but rather creates a new version of the object and keeps both versions.\nVersioned objects usually have a uid property which is based on their content and allows to reference a\nspecific version of an object (other than tagging objects, which also allows for easy referencing).\nMany objects have both a store function and a patch function. These are used in the same way as the\ncorresponding REST verbs - a store is passed a full object and will basically perform a PUT operation,\nreplacing the full object (if it exists) while patch receives just a dictionary containing the differences to\nbe applied to the object, and will merge those changes to the existing object. The patch\noperation also has a strategy assigned to it which determines how the merge logic should behave.\nThe strategy can be either replace or additive. For further details on those strategies, refer\nto https://pypi.org/project/mergedeep/\nabort_run(uid, project='', iter=0)[source]#\nAbort a running run - will remove the run’s runtime resources and mark its state as aborted\napi_call(method, path, error=None, params=None, body=None, json=None, headers=None, timeout=45, version=None)[source]#\nPerform a direct REST API call on the mlrun API server.\nCaution\nFor advanced usage - prefer using the various APIs exposed through this class, rather than\ndirectly invoking REST calls.\nParameters\nmethod – REST method (POST, GET, PUT…)\npath – Path to endpoint executed, for example \"projects\"\nerror – Error to return if API invocation fails\nbody – Payload to be passed in the call. If using JSON objects, prefer using the json param\njson – JSON payload to be passed in the call\nheaders – REST headers, passed as a dictionary: {\"<header-name>\": \"<header-value>\"}\ntimeout – API call timeout\nversion – API version to use, None (the default) will mean to use the default value from config,\nfor un-versioned api set an empty string.\nReturns\nPython HTTP response object\nconnect(secrets=None)[source]#\nConnect to the MLRun API server. Must be called prior to executing any other method.\nThe code utilizes the URL for the API server from the configuration - mlconf.dbpath.\nFor example:\nmlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\ndb = get_run_db().connect()\ncreate_feature_set(feature_set: Union[dict, mlrun.api.schemas.feature_store.FeatureSet, mlrun.feature_store.feature_set.FeatureSet], project='', versioned=True) → dict[source]#\nCreate a new FeatureSet and save in the mlrun DB. The\nfeature-set must not previously exist in the DB.\nParameters\nfeature_set – The new FeatureSet to create.\nproject – Name of project this feature-set belongs to.\nversioned – Whether to maintain versions for this feature-set. All versions of a versioned object\nwill be kept in the DB and can be retrieved until explicitly deleted.\nReturns\nThe FeatureSet object (as dict).\ncreate_feature_vector(feature_vector: Union[dict, mlrun.api.schemas.feature_store.FeatureVector, mlrun.feature_store.feature_vector.FeatureVector], project='', versioned=True) → dict[source]#\nCreate a new FeatureVector and save in the mlrun DB.\nParameters\nfeature_vector – The new FeatureVector to create.\nproject – Name of project this feature-vector belongs to.\nversioned – Whether to maintain versions for this feature-vector. All versions of a versioned object\nwill be kept in the DB and can be retrieved until explicitly deleted.\nReturns\nThe FeatureVector object (as dict).\ncreate_marketplace_source(source: Union[dict, mlrun.api.schemas.marketplace.IndexedMarketplaceSource])[source]#\nAdd a new marketplace source.\nMLRun maintains an ordered list of marketplace sources (“sources”) Each source has\nits details registered and its order within the list. When creating a new source, the special order -1\ncan be used to mark this source as last in the list. However, once the source is in the MLRun list,\nits order will always be >0.\nThe global marketplace source always exists in the list, and is always the last source\n(order = -1). It cannot be modified nor can it be moved to another order in the list.\nThe source object may contain credentials which are needed to access the datastore where the source is stored.\nThese credentials are not kept in the MLRun DB, but are stored inside a kubernetes secret object maintained by\nMLRun. They are not returned through any API from MLRun.\nExample:\nimport mlrun.api.schemas\n# Add a private source as the last one (will be #1 in the list)\nprivate_source = mlrun.api.schemas.IndexedMarketplaceSource(\norder=-1,\nsource=mlrun.api.schemas.MarketplaceSource(\nmetadata=mlrun.api.schemas.MarketplaceObjectMetadata(name=\"priv\", description=\"a private source\"),\nspec=mlrun.api.schemas.MarketplaceSourceSpec(path=\"/local/path/to/source\", channel=\"development\")\n)\n)\ndb.create_marketplace_source(private_source)\n# Add another source as 1st in the list - will push previous one to be #2\nanother_source = mlrun.api.schemas.IndexedMarketplaceSource(\norder=1,\nsource=mlrun.api.schemas.MarketplaceSource(\nmetadata=mlrun.api.schemas.MarketplaceObjectMetadata(name=\"priv-2\", description=\"another source\"),\nspec=mlrun.api.schemas.MarketplaceSourceSpec(\npath=\"/local/path/to/source/2\",\nchannel=\"development\",\ncredentials={...}\n)\n)\n)\ndb.create_marketplace_source(another_source)\nParameters\nsource – The source and its order, of type\nIndexedMarketplaceSource, or in dictionary form.\nReturns\nThe source object as inserted into the database, with credentials stripped.\ncreate_or_patch_model_endpoint(project: str, endpoint_id: str, model_endpoint: mlrun.api.schemas.model_endpoints.ModelEndpoint, access_key: Optional[str] = None)[source]#\nCreates or updates a KV record with the given model_endpoint record\nParameters\nproject – The name of the project\nendpoint_id – The id of the endpoint\nmodel_endpoint – An object representing the model endpoint\naccess_key – V3IO access key, when None, will be look for in environ\ncreate_project(project: Union[dict, mlrun.projects.project.MlrunProject, mlrun.api.schemas.project.Project]) → mlrun.projects.project.MlrunProject[source]#\nCreate a new project. A project with the same name must not exist prior to creation.\ncreate_project_secrets(project: str, provider: Union[str, mlrun.api.schemas.secret.SecretProviderName] = SecretProviderName.kubernetes, secrets: Optional[dict] = None)[source]#\nCreate project-context secrets using either vault or kubernetes provider.\nWhen using with Vault, this will create needed Vault structures for storing secrets in project-context, and\nstore a set of secret values. The method generates Kubernetes service-account and the Vault authentication\nstructures that are required for function Pods to authenticate with Vault and be able to extract secret values\npassed as part of their context.\nNote\nThis method used with Vault is currently in technical preview, and requires a HashiCorp Vault\ninfrastructure properly set up and connected to the MLRun API server.\nWhen used with Kubernetes, this will make sure that the project-specific k8s secret is created, and will\npopulate it with the secrets provided, replacing their values if they exist.\nParameters\nproject – The project context for which to generate the infra and store secrets.\nprovider – The name of the secrets-provider to work with. Accepts a\nSecretProviderName enum.\nsecrets – A set of secret values to store.\nExample:\nsecrets = {'password': 'myPassw0rd', 'aws_key': '111222333'}\ndb.create_project_secrets(\n\"project1\",\nprovider=mlrun.api.schemas.SecretProviderName.kubernetes,\nsecrets=secrets\n)\ncreate_schedule(project: str, schedule: mlrun.api.schemas.schedule.ScheduleInput)[source]#\nCreate a new schedule on the given project. The details on the actual object to schedule as well as the\nschedule itself are within the schedule object provided.\nThe ScheduleCronTrigger follows the guidelines in\nhttps://apscheduler.readthedocs.io/en/v3.6.3/modules/triggers/cron.html.\nIt also supports a from_crontab() function that accepts a\ncrontab-formatted string (see https://en.wikipedia.org/wiki/Cron for more information on the format).\nExample:\nfrom mlrun.api import schemas\n# Execute the get_data_func function every Tuesday at 15:30\nschedule = schemas.ScheduleInput(\nname=\"run_func_on_tuesdays\",\nkind=\"job\",\nscheduled_object=get_data_func,\ncron_trigger=schemas.ScheduleCronTrigger(day_of_week='tue', hour=15, minute=30),\n)\ndb.create_schedule(project_name, schedule)\ncreate_user_secrets(user: str, provider: Union[str, mlrun.api.schemas.secret.SecretProviderName] = SecretProviderName.vault, secrets: Optional[dict] = None)[source]#\nCreate user-context secret in Vault. Please refer to create_project_secrets() for more details\nand status of this functionality.\nNote\nThis method is currently in technical preview, and requires a HashiCorp Vault infrastructure\nproperly set up and connected to the MLRun API server.\nParameters\nuser – The user context for which to generate the infra and store secrets.\nprovider – The name of the secrets-provider to work with. Currently only vault is supported.\nsecrets – A set of secret values to store within the Vault.\ndel_artifact(key, tag=None, project='')[source]#\nDelete an artifact.\ndel_artifacts(name=None, project=None, tag=None, labels=None, days_ago=0)[source]#\nDelete artifacts referenced by the parameters.\nParameters\nname – Name of artifacts to delete. Note that this is a like query, and is case-insensitive. See\nlist_artifacts() for more details.\nproject – Project that artifacts belong to.\ntag – Choose artifacts who are assigned this tag.\nlabels – Choose artifacts which are labeled.\ndays_ago – This parameter is deprecated and not used.\ndel_run(uid, project='', iter=0)[source]#\nDelete details of a specific run from DB.\nParameters\nuid – Unique ID for the specific run to delete.\nproject – Project that the run belongs to.\niter – Iteration within a specific task.\ndel_runs(name=None, project=None, labels=None, state=None, days_ago=0)[source]#\nDelete a group of runs identified by the parameters of the function.\nExample:\ndb.del_runs(state='completed')\nParameters\nname – Name of the task which the runs belong to.\nproject – Project to which the runs belong.\nlabels – Filter runs that are labeled using these specific label values.\nstate – Filter only runs which are in this state.\ndays_ago – Filter runs whose start time is newer than this parameter.\ndelete_artifacts_tags(artifacts, project: str, tag_name: str)[source]#\nDelete tag from a list of artifacts.\nParameters\nartifacts – The artifacts to delete the tag from. Can be a list of Artifact\nobjects or dictionaries, or a single object.\nproject – Project which contains the artifacts.\ntag_name – The tag to set on the artifacts.\ndelete_feature_set(name, project='', tag=None, uid=None)[source]#\nDelete a FeatureSet object from the DB.\nIf tag or uid are specified, then just the version referenced by them will be deleted. Using both\nis not allowed.\nIf none are specified, then all instances of the object whose name is name will be deleted.\ndelete_feature_vector(name, project='', tag=None, uid=None)[source]#\nDelete a FeatureVector object from the DB.\nIf tag or uid are specified, then just the version referenced by them will be deleted. Using both\nis not allowed.\nIf none are specified, then all instances of the object whose name is name will be deleted.\ndelete_function(name: str, project: str = '')[source]#\nDelete a function belonging to a specific project.\ndelete_marketplace_source(source_name: str)[source]#\nDelete a marketplace source from the DB.\nThe source will be deleted from the list, and any following sources will be promoted - for example, if the\n1st source is deleted, the 2nd source will become #1 in the list.\nThe global marketplace source cannot be deleted.\nParameters\nsource_name – Name of the marketplace source to delete.\ndelete_model_endpoint_record(project: str, endpoint_id: str, access_key: Optional[str] = None)[source]#\nDeletes the KV record of a given model endpoint, project and endpoint_id are used for lookup\nParameters\nproject – The name of the project\nendpoint_id – The id of the endpoint\naccess_key – V3IO access key, when None, will be look for in environ\ndelete_objects_tag(project: str, tag_name: str, tag_objects: Union[mlrun.api.schemas.tag.TagObjects, dict])[source]#\nDelete a tag from a list of objects.\nParameters\nproject – Project which contains the objects.\ntag_name – The tag to delete from the objects.\ntag_objects – The objects to delete the tag from.\ndelete_project(name: str, deletion_strategy: Union[str, mlrun.api.schemas.constants.DeletionStrategy] = DeletionStrategy.restricted)[source]#\nDelete a project.\nParameters\nname – Name of the project to delete.\ndeletion_strategy – How to treat child objects of the project. Possible values are:\nrestrict (default) - Project must not have any child objects when deleted. If using this mode while\nchild objects exist, the operation will fail.\ncascade - Automatically delete all child objects when deleting the project.\ndelete_project_secrets(project: str, provider: Union[str, mlrun.api.schemas.secret.SecretProviderName] = SecretProviderName.kubernetes, secrets: Optional[List[str]] = None)[source]#\nDelete project-context secrets from Kubernetes.\nParameters\nproject – The project name.\nprovider – The name of the secrets-provider to work with. Currently only kubernetes is supported.\nsecrets – A list of secret names to delete. An empty list will delete all secrets assigned\nto this specific project.\ndelete_runtime(kind: str, label_selector: Optional[str] = None, force: bool = False, grace_period: Optional[int] = None)[source]#\nDeprecated use delete_runtime_resources() (with kind filter) instead\ndelete_runtime_object(kind: str, object_id: str, label_selector: Optional[str] = None, force: bool = False, grace_period: Optional[int] = None)[source]#\nDeprecated use delete_runtime_resources() (with kind and object_id filter) instead\ndelete_runtime_resources(project: Optional[str] = None, label_selector: Optional[str] = None, kind: Optional[str] = None, object_id: Optional[str] = None, force: bool = False, grace_period: Optional[int] = None) → Dict[str, Dict[str, mlrun.api.schemas.runtime_resource.RuntimeResources]][source]#\nDelete all runtime resources which are in terminal state.\nParameters\nproject – Delete only runtime resources of a specific project, by default None, which will delete only\nfrom the projects you’re authorized to delete from.\nlabel_selector – Delete only runtime resources matching the label selector.\nkind – The kind of runtime to delete. May be one of [‘dask’, ‘job’, ‘spark’, ‘remote-spark’, ‘mpijob’]\nobject_id – The identifier of the mlrun object to delete its runtime resources. for most function\nruntimes, runtime resources are per Run, for which the identifier is the Run’s UID. For dask runtime, the\nruntime resources are per Function, for which the identifier is the Function’s name.\nforce – Force deletion - delete the runtime resource even if it’s not in terminal state or if the grace\nperiod didn’t pass.\ngrace_period – Grace period given to the runtime resource before they are actually removed, counted from\nthe moment they moved to terminal state.\nReturns\nGroupedByProjectRuntimeResourcesOutput listing the runtime resources\nthat were removed.\ndelete_runtimes(label_selector: Optional[str] = None, force: bool = False, grace_period: Optional[int] = None)[source]#\nDeprecated use delete_runtime_resources() instead\ndelete_schedule(project: str, name: str)[source]#\nDelete a specific schedule by name.\nstatic get_api_path_prefix(version: Optional[str] = None) → str[source]#\nParameters\nversion – API version to use, None (the default) will mean to use the default value from mlconf,\nfor un-versioned api set an empty string.\nget_background_task(name: str) → mlrun.api.schemas.background_task.BackgroundTask[source]#\nRetrieve updated information on a background task being executed.\nget_base_api_url(path: str, version: Optional[str] = None) → str[source]#\nget_builder_status(func: mlrun.runtimes.base.BaseRuntime, offset=0, logs=True, last_log_timestamp=0, verbose=False)[source]#\nRetrieve the status of a build operation currently in progress.\nParameters\nfunc – Function object that is being built.\noffset – Offset into the build logs to retrieve logs from.\nlogs – Should build logs be retrieved.\nlast_log_timestamp – Last timestamp of logs that were already retrieved. Function will return only logs\nlater than this parameter.\nverbose – Add verbose logs into the output.\nReturns\nThe following parameters:\nText of builder logs.\nTimestamp of last log retrieved, to be used in subsequent calls to this function.\nThe function also updates internal members of the func object to reflect build process info.\nget_feature_set(name: str, project: str = '', tag: Optional[str] = None, uid: Optional[str] = None) → mlrun.feature_store.feature_set.FeatureSet[source]#\nRetrieve a ~mlrun.feature_store.FeatureSet` object. If both tag and uid are not specified, then\nthe object tagged latest will be retrieved.\nParameters\nname – Name of object to retrieve.\nproject – Project the FeatureSet belongs to.\ntag – Tag of the specific object version to retrieve.\nuid – uid of the object to retrieve (can only be used for versioned objects).\nget_feature_vector(name: str, project: str = '', tag: Optional[str] = None, uid: Optional[str] = None) → mlrun.feature_store.feature_vector.FeatureVector[source]#\nReturn a specific feature-vector referenced by its tag or uid. If none are provided, latest tag will\nbe used.\nget_function(name, project='', tag=None, hash_key='')[source]#\nRetrieve details of a specific function, identified by its name and potentially a tag or function hash.\nget_log(uid, project='', offset=0, size=- 1)[source]#\nRetrieve a log.\nParameters\nuid – Log unique ID\nproject – Project name for which the log belongs\noffset – Retrieve partial log, get up to size bytes starting at offset offset\nfrom beginning of log\nsize – See offset. If set to -1 (the default) will retrieve all data to end of log.\nReturns\nThe following objects:\nstate - The state of the runtime object which generates this log, if it exists. In case no known state\nexists, this will be unknown.\ncontent - The actual log content.\nget_marketplace_catalog(source_name: str, channel: Optional[str] = None, version: Optional[str] = None, tag: Optional[str] = None, force_refresh: bool = False)[source]#\nRetrieve the item catalog for a specified marketplace source.\nThe list of items can be filtered according to various filters, using item’s metadata to filter.\nParameters\nsource_name – Name of the source.\nchannel – Filter items according to their channel. For example development.\nversion – Filter items according to their version.\ntag – Filter items based on tag.\nforce_refresh – Make the server fetch the catalog from the actual marketplace source,\nrather than rely on cached information which may exist from previous get requests. For example,\nif the source was re-built,\nthis will make the server get the updated information. Default is False.\nReturns\nMarketplaceCatalog object, which is essentially a list\nof MarketplaceItem entries.\nget_marketplace_item(source_name: str, item_name: str, channel: str = 'development', version: Optional[str] = None, tag: str = 'latest', force_refresh: bool = False)[source]#\nRetrieve a specific marketplace item.\nParameters\nsource_name – Name of source.\nitem_name – Name of the item to retrieve, as it appears in the catalog.\nchannel – Get the item from the specified channel. Default is development.\nversion – Get a specific version of the item. Default is None.\ntag – Get a specific version of the item identified by tag. Default is latest.\nforce_refresh – Make the server fetch the information from the actual marketplace\nsource, rather than\nrely on cached information. Default is False.\nReturns\nMarketplaceItem.\nget_marketplace_source(source_name: str)[source]#\nRetrieve a marketplace source from the DB.\nParameters\nsource_name – Name of the marketplace source to retrieve.\nget_model_endpoint(project: str, endpoint_id: str, start: Optional[str] = None, end: Optional[str] = None, metrics: Optional[List[str]] = None, feature_analysis: bool = False, access_key: Optional[str] = None) → mlrun.api.schemas.model_endpoints.ModelEndpoint[source]#\nReturns a ModelEndpoint object with additional metrics and feature related data.\nParameters\nproject – The name of the project\nendpoint_id – The id of the model endpoint\nmetrics – A list of metrics to return for each endpoint, read more in ‘TimeMetric’\nstart – The start time of the metrics\nend – The end time of the metrics\nfeature_analysis – When True, the base feature statistics and current feature statistics will be added to\nthe output of the resulting object\naccess_key – V3IO access key, when None, will be look for in environ\nget_pipeline(run_id: str, namespace: Optional[str] = None, timeout: int = 10, format_: Union[str, mlrun.api.schemas.pipeline.PipelinesFormat] = PipelinesFormat.summary, project: Optional[str] = None)[source]#\nRetrieve details of a specific pipeline using its run ID (as provided when the pipeline was executed).\nget_project(name: str) → mlrun.projects.project.MlrunProject[source]#\nGet details for a specific project.\nget_project_background_task(project: str, name: str) → mlrun.api.schemas.background_task.BackgroundTask[source]#\nRetrieve updated information on a project background task being executed.\nget_runtime(kind: str, label_selector: Optional[str] = None) → Dict[source]#\nDeprecated use list_runtime_resources() (with kind filter) instead\nget_schedule(project: str, name: str, include_last_run: bool = False) → mlrun.api.schemas.schedule.ScheduleOutput[source]#\nRetrieve details of the schedule in question. Besides returning the details of the schedule object itself,\nthis function also returns the next scheduled run for this specific schedule, as well as potentially the\nresults of the last run executed through this schedule.\nParameters\nproject – Project name.\nname – Name of the schedule object to query.\ninclude_last_run – Whether to include the results of the schedule’s last run in the response.\ninvoke_schedule(project: str, name: str)[source]#\nExecute the object referenced by the schedule immediately.\nkind = 'http'#\nlist_artifact_tags(project=None, category: Optional[Union[str, mlrun.api.schemas.artifact.ArtifactCategories]] = None) → List[str][source]#\nReturn a list of all the tags assigned to artifacts in the scope of the given project.\nlist_artifacts(name=None, project=None, tag=None, labels=None, since=None, until=None, iter: Optional[int] = None, best_iteration: bool = False, kind: Optional[str] = None, category: Optional[Union[str, mlrun.api.schemas.artifact.ArtifactCategories]] = None) → mlrun.lists.ArtifactList[source]#\nList artifacts filtered by various parameters.\nExamples:\n# Show latest version of all artifacts in project\nlatest_artifacts = db.list_artifacts('', tag='latest', project='iris')\n# check different artifact versions for a specific artifact\nresult_versions = db.list_artifacts('results', tag='*', project='iris')\nParameters\nname – Name of artifacts to retrieve. Name is used as a like query, and is not case-sensitive. This means\nthat querying for name may return artifacts named my_Name_1 or surname.\nproject – Project name.\ntag – Return artifacts assigned this tag.\nlabels – Return artifacts that have these labels.\nsince – Not in use in HTTPRunDB.\nuntil – Not in use in HTTPRunDB.\niter – Return artifacts from a specific iteration (where iter=0 means the root iteration). If\nNone (default) return artifacts from all iterations.\nbest_iteration – Returns the artifact which belongs to the best iteration of a given run, in the case of\nartifacts generated from a hyper-param run. If only a single iteration exists, will return the artifact\nfrom that iteration. If using best_iter, the iter parameter must not be used.\nkind – Return artifacts of the requested kind.\ncategory – Return artifacts of the requested category.\nlist_entities(project: str, name: Optional[str] = None, tag: Optional[str] = None, labels: Optional[List[str]] = None) → List[dict][source]#\nRetrieve a list of entities and their mapping to the containing feature-sets. This function is similar\nto the list_features() function, and uses the same logic. However, the entities are matched\nagainst the name rather than the features.\nlist_feature_sets(project: str = '', name: Optional[str] = None, tag: Optional[str] = None, state: Optional[str] = None, entities: Optional[List[str]] = None, features: Optional[List[str]] = None, labels: Optional[List[str]] = None, partition_by: Optional[Union[mlrun.api.schemas.constants.FeatureStorePartitionByField, str]] = None, rows_per_partition: int = 1, partition_sort_by: Optional[Union[mlrun.api.schemas.constants.SortField, str]] = None, partition_order: Union[mlrun.api.schemas.constants.OrderType, str] = OrderType.desc) → List[mlrun.feature_store.feature_set.FeatureSet][source]#\nRetrieve a list of feature-sets matching the criteria provided.\nParameters\nproject – Project name.\nname – Name of feature-set to match. This is a like query, and is case-insensitive.\ntag – Match feature-sets with specific tag.\nstate – Match feature-sets with a specific state.\nentities – Match feature-sets which contain entities whose name is in this list.\nfeatures – Match feature-sets which contain features whose name is in this list.\nlabels – Match feature-sets which have these labels.\npartition_by – Field to group results by. Only allowed value is name. When partition_by is specified,\nthe partition_sort_by parameter must be provided as well.\nrows_per_partition – How many top rows (per sorting defined by partition_sort_by and partition_order)\nto return per group. Default value is 1.\npartition_sort_by – What field to sort the results by, within each partition defined by partition_by.\nCurrently the only allowed value are created and updated.\npartition_order – Order of sorting within partitions - asc or desc. Default is desc.\nReturns\nList of matching FeatureSet objects.\nlist_feature_vectors(project: str = '', name: Optional[str] = None, tag: Optional[str] = None, state: Optional[str] = None, labels: Optional[List[str]] = None, partition_by: Optional[Union[mlrun.api.schemas.constants.FeatureStorePartitionByField, str]] = None, rows_per_partition: int = 1, partition_sort_by: Optional[Union[mlrun.api.schemas.constants.SortField, str]] = None, partition_order: Union[mlrun.api.schemas.constants.OrderType, str] = OrderType.desc) → List[mlrun.feature_store.feature_vector.FeatureVector][source]#\nRetrieve a list of feature-vectors matching the criteria provided.\nParameters\nproject – Project name.\nname – Name of feature-vector to match. This is a like query, and is case-insensitive.\ntag – Match feature-vectors with specific tag.\nstate – Match feature-vectors with a specific state.\nlabels – Match feature-vectors which have these labels.\npartition_by – Field to group results by. Only allowed value is name. When partition_by is specified,\nthe partition_sort_by parameter must be provided as well.\nrows_per_partition – How many top rows (per sorting defined by partition_sort_by and partition_order)\nto return per group. Default value is 1.\npartition_sort_by – What field to sort the results by, within each partition defined by partition_by.\nCurrently the only allowed values are created and updated.\npartition_order – Order of sorting within partitions - asc or desc. Default is desc.\nReturns\nList of matching FeatureVector objects.\nlist_features(project: str, name: Optional[str] = None, tag: Optional[str] = None, entities: Optional[List[str]] = None, labels: Optional[List[str]] = None) → List[dict][source]#\nList feature-sets which contain specific features. This function may return multiple versions of the same\nfeature-set if a specific tag is not requested. Note that the various filters of this function actually\nrefer to the feature-set object containing the features, not to the features themselves.\nParameters\nproject – Project which contains these features.\nname – Name of the feature to look for. The name is used in a like query, and is not case-sensitive. For\nexample, looking for feat will return features which are named MyFeature as well as defeat.\ntag – Return feature-sets which contain the features looked for, and are tagged with the specific tag.\nentities – Return only feature-sets which contain an entity whose name is contained in this list.\nlabels – Return only feature-sets which are labeled as requested.\nReturns\nA list of mapping from feature to a digest of the feature-set, which contains the feature-set\nmeta-data. Multiple entries may be returned for any specific feature due to multiple tags or versions\nof the feature-set.\nlist_functions(name=None, project=None, tag=None, labels=None)[source]#\nRetrieve a list of functions, filtered by specific criteria.\nParameters\nname – Return only functions with a specific name.\nproject – Return functions belonging to this project. If not specified, the default project is used.\ntag – Return function versions with specific tags.\nlabels – Return functions that have specific labels assigned to them.\nReturns\nList of function objects (as dictionary).\nlist_marketplace_sources()[source]#\nList marketplace sources in the MLRun DB.\nlist_model_endpoints(project: str, model: Optional[str] = None, function: Optional[str] = None, labels: Optional[List[str]] = None, start: str = 'now-1h', end: str = 'now', metrics: Optional[List[str]] = None, access_key: Optional[str] = None, top_level: bool = False, uids: Optional[List[str]] = None) → mlrun.api.schemas.model_endpoints.ModelEndpointList[source]#\nReturns a list of ModelEndpointState objects. Each object represents the current state of a model endpoint.\nThis functions supports filtering by the following parameters:\n1) model\n2) function\n3) labels\nBy default, when no filters are applied, all available endpoints for the given project will be listed.\nIn addition, this functions provides a facade for listing endpoint related metrics. This facade is time-based\nand depends on the ‘start’ and ‘end’ parameters. By default, when the metrics parameter is None, no metrics are\nadded to the output of this function.\nParameters\nproject – The name of the project\nmodel – The name of the model to filter by\nfunction – The name of the function to filter by\nlabels – A list of labels to filter by. Label filters work by either filtering a specific value of a label\n(i.e. list(“key==value”)) or by looking for the existence of a given key (i.e. “key”)\nmetrics – A list of metrics to return for each endpoint, read more in ‘TimeMetric’\nstart – The start time of the metrics\nend – The end time of the metrics\naccess_key – V3IO access key, when None, will be look for in environ\ntop_level – if true will return only routers and endpoint that are NOT children of any router\nuids – if passed will return ModelEndpointList of endpoints with uid in uids\nlist_pipelines(project: str, namespace: Optional[str] = None, sort_by: str = '', page_token: str = '', filter_: str = '', format_: Union[str, mlrun.api.schemas.pipeline.PipelinesFormat] = PipelinesFormat.metadata_only, page_size: Optional[int] = None) → mlrun.api.schemas.pipeline.PipelinesOutput[source]#\nRetrieve a list of KFP pipelines. This function can be invoked to get all pipelines from all projects,\nby specifying project=*, in which case pagination can be used and the various sorting and pagination\nproperties can be applied. If a specific project is requested, then the pagination options cannot be\nused and pagination is not applied.\nParameters\nproject – Project name. Can be * for query across all projects.\nnamespace – Kubernetes namespace in which the pipelines are executing.\nsort_by – Field to sort the results by.\npage_token – Use for pagination, to retrieve next page.\nfilter – Kubernetes filter to apply to the query, can be used to filter on specific object fields.\nformat – Result format. Can be one of:\nfull - return the full objects.\nmetadata_only (default) - return just metadata of the pipelines objects.\nname_only - return just the names of the pipeline objects.\npage_size – Size of a single page when applying pagination.\nlist_project_secret_keys(project: str, provider: Union[str, mlrun.api.schemas.secret.SecretProviderName] = SecretProviderName.kubernetes, token: Optional[str] = None) → mlrun.api.schemas.secret.SecretKeysData[source]#\nRetrieve project-context secret keys from Vault or Kubernetes.\nNote\nThis method for Vault functionality is currently in technical preview, and requires a HashiCorp Vault\ninfrastructure properly set up and connected to the MLRun API server.\nParameters\nproject – The project name.\nprovider – The name of the secrets-provider to work with. Accepts a\nSecretProviderName enum.\ntoken – Vault token to use for retrieving secrets. Only in use if provider is vault.\nMust be a valid Vault token, with permissions to retrieve secrets of the project in question.\nlist_project_secrets(project: str, token: Optional[str] = None, provider: Union[str, mlrun.api.schemas.secret.SecretProviderName] = SecretProviderName.kubernetes, secrets: Optional[List[str]] = None) → mlrun.api.schemas.secret.SecretsData[source]#\nRetrieve project-context secrets from Vault.\nNote\nThis method for Vault functionality is currently in technical preview, and requires a HashiCorp Vault\ninfrastructure properly set up and connected to the MLRun API server.\nParameters\nproject – The project name.\ntoken – Vault token to use for retrieving secrets.\nMust be a valid Vault token, with permissions to retrieve secrets of the project in question.\nprovider – The name of the secrets-provider to work with. Currently only vault is accepted.\nsecrets – A list of secret names to retrieve. An empty list [] will retrieve all secrets assigned\nto this specific project. kubernetes provider only supports an empty list.\nlist_projects(owner: Optional[str] = None, format_: Union[str, mlrun.api.schemas.project.ProjectsFormat] = ProjectsFormat.full, labels: Optional[List[str]] = None, state: Optional[Union[str, mlrun.api.schemas.project.ProjectState]] = None) → List[Union[mlrun.projects.project.MlrunProject, str]][source]#\nReturn a list of the existing projects, potentially filtered by specific criteria.\nParameters\nowner – List only projects belonging to this specific owner.\nformat – Format of the results. Possible values are:\nfull (default value) - Return full project objects.\nname_only - Return just the names of the projects.\nlabels – Filter by labels attached to the project.\nstate – Filter by project’s state. Can be either online or archived.\nlist_runs(name=None, uid=None, project=None, labels=None, state=None, sort=True, last=0, iter=False, start_time_from: Optional[datetime.datetime] = None, start_time_to: Optional[datetime.datetime] = None, last_update_time_from: Optional[datetime.datetime] = None, last_update_time_to: Optional[datetime.datetime] = None, partition_by: Optional[Union[mlrun.api.schemas.constants.RunPartitionByField, str]] = None, rows_per_partition: int = 1, partition_sort_by: Optional[Union[mlrun.api.schemas.constants.SortField, str]] = None, partition_order: Union[mlrun.api.schemas.constants.OrderType, str] = OrderType.desc, max_partitions: int = 0) → mlrun.lists.RunList[source]#\nRetrieve a list of runs, filtered by various options.\nExample:\nruns = db.list_runs(name='download', project='iris', labels='owner=admin')\n# If running in Jupyter, can use the .show() function to display the results\ndb.list_runs(name='', project=project_name).show()\nParameters\nname – Name of the run to retrieve.\nuid – Unique ID of the run.\nproject – Project that the runs belongs to.\nlabels – List runs that have a specific label assigned. Currently only a single label filter can be\napplied, otherwise result will be empty.\nstate – List only runs whose state is specified.\nsort – Whether to sort the result according to their start time. Otherwise, results will be\nreturned by their internal order in the DB (order will not be guaranteed).\nlast – Deprecated - currently not used.\niter – If True return runs from all iterations. Otherwise, return only runs whose iter is 0.\nstart_time_from – Filter by run start time in [start_time_from, start_time_to].\nstart_time_to – Filter by run start time in [start_time_from, start_time_to].\nlast_update_time_from – Filter by run last update time in (last_update_time_from,\nlast_update_time_to).\nlast_update_time_to – Filter by run last update time in (last_update_time_from, last_update_time_to).\npartition_by – Field to group results by. Only allowed value is name. When partition_by is specified,\nthe partition_sort_by parameter must be provided as well.\nrows_per_partition – How many top rows (per sorting defined by partition_sort_by and partition_order)\nto return per group. Default value is 1.\npartition_sort_by – What field to sort the results by, within each partition defined by partition_by.\nCurrently the only allowed values are created and updated.\npartition_order – Order of sorting within partitions - asc or desc. Default is desc.\nmax_partitions – Maximal number of partitions to include in the result. Default is 0 which means no\nlimit.\nlist_runtime_resources(project: Optional[str] = None, label_selector: Optional[str] = None, kind: Optional[str] = None, object_id: Optional[str] = None, group_by: Optional[mlrun.api.schemas.runtime_resource.ListRuntimeResourcesGroupByField] = None) → Union[List[mlrun.api.schemas.runtime_resource.KindRuntimeResources], Dict[str, Dict[str, mlrun.api.schemas.runtime_resource.RuntimeResources]]][source]#\nList current runtime resources, which are usually (but not limited to) Kubernetes pods or CRDs.\nFunction applies for runs of type [‘dask’, ‘job’, ‘spark’, ‘remote-spark’, ‘mpijob’], and will return per\nruntime kind a list of the runtime resources (which may have already completed their execution).\nParameters\nproject – Get only runtime resources of a specific project, by default None, which will return only the\nprojects you’re authorized to see.\nlabel_selector – A label filter that will be passed to Kubernetes for filtering the results according\nto their labels.\nkind – The kind of runtime to query. May be one of [‘dask’, ‘job’, ‘spark’, ‘remote-spark’, ‘mpijob’]\nobject_id – The identifier of the mlrun object to query its runtime resources. for most function runtimes,\nruntime resources are per Run, for which the identifier is the Run’s UID. For dask runtime, the runtime\nresources are per Function, for which the identifier is the Function’s name.\ngroup_by – Object to group results by. Allowed values are job and project.\nlist_runtimes(label_selector: Optional[str] = None) → List[source]#\nDeprecated use list_runtime_resources() instead\nlist_schedules(project: str, name: Optional[str] = None, kind: Optional[mlrun.api.schemas.schedule.ScheduleKinds] = None, include_last_run: bool = False) → mlrun.api.schemas.schedule.SchedulesOutput[source]#\nRetrieve list of schedules of specific name or kind.\nParameters\nproject – Project name.\nname – Name of schedule to retrieve. Can be omitted to list all schedules.\nkind – Kind of schedule objects to retrieve, can be either job or pipeline.\ninclude_last_run – Whether to return for each schedule returned also the results of the last run of\nthat schedule.\npatch_feature_set(name, feature_set_update: dict, project='', tag=None, uid=None, patch_mode: Union[str, mlrun.api.schemas.constants.PatchMode] = PatchMode.replace)[source]#\nModify (patch) an existing FeatureSet object.\nThe object is identified by its name (and project it belongs to), as well as optionally a tag or its\nuid (for versioned object). If both tag and uid are omitted then the object with tag latest\nis modified.\nParameters\nname – Name of the object to patch.\nfeature_set_update – The modifications needed in the object. This parameter only has the changes in it,\nnot a full object.\nExample:\nfeature_set_update = {\"status\": {\"processed\" : True}}\nWill apply the field status.processed to the existing object.\nproject – Project which contains the modified object.\ntag – The tag of the object to modify.\nuid – uid of the object to modify.\npatch_mode – The strategy for merging the changes with the existing object. Can be either replace\nor additive.\npatch_feature_vector(name, feature_vector_update: dict, project='', tag=None, uid=None, patch_mode: Union[str, mlrun.api.schemas.constants.PatchMode] = PatchMode.replace)[source]#\nModify (patch) an existing FeatureVector object.\nThe object is identified by its name (and project it belongs to), as well as optionally a tag or its\nuid (for versioned object). If both tag and uid are omitted then the object with tag latest\nis modified.\nParameters\nname – Name of the object to patch.\nfeature_vector_update – The modifications needed in the object. This parameter only has the changes in it,\nnot a full object.\nproject – Project which contains the modified object.\ntag – The tag of the object to modify.\nuid – uid of the object to modify.\npatch_mode – The strategy for merging the changes with the existing object. Can be either replace\nor additive.\npatch_project(name: str, project: dict, patch_mode: Union[str, mlrun.api.schemas.constants.PatchMode] = PatchMode.replace) → mlrun.projects.project.MlrunProject[source]#\nPatch an existing project object.\nParameters\nname – Name of project to patch.\nproject – The actual changes to the project object.\npatch_mode – The strategy for merging the changes with the existing object. Can be either replace\nor additive.\nread_artifact(key, tag=None, iter=None, project='')[source]#\nRead an artifact, identified by its key, tag and iteration.\nread_run(uid, project='', iter=0)[source]#\nRead the details of a stored run from the DB.\nParameters\nuid – The run’s unique ID.\nproject – Project name.\niter – Iteration within a specific execution.\nremote_builder(func, with_mlrun, mlrun_version_specifier=None, skip_deployed=False, builder_env=None)[source]#\nBuild the pod image for a function, for execution on a remote cluster. This is executed by the MLRun\nAPI server, and creates a Docker image out of the function provided and any specific build\ninstructions provided within. This is a pre-requisite for remotely executing a function, unless using\na pre-deployed image.\nParameters\nfunc – Function to build.\nwith_mlrun – Whether to add MLRun package to the built package. This is not required if using a base\nimage that already has MLRun in it.\nmlrun_version_specifier – Version of MLRun to include in the built image.\nskip_deployed – Skip the build if we already have an image for the function.\nbuilder_env – Kaniko builder pod env vars dict (for config/credentials)\nremote_start(func_url) → mlrun.api.schemas.background_task.BackgroundTask[source]#\nExecute a function remotely, Used for dask functions.\nParameters\nfunc_url – URL to the function to be executed.\nReturns\nA BackgroundTask object, with details on execution process and its status.\nremote_status(project, name, kind, selector)[source]#\nRetrieve status of a function being executed remotely (relevant to dask functions).\nParameters\nproject – The project of the function\nname – The name of the function\nkind – The kind of the function, currently dask is supported.\nselector – Selector clause to be applied to the Kubernetes status query to filter the results.\nstore_artifact(key, artifact, uid, iter=None, tag=None, project='')[source]#\nStore an artifact in the DB.\nParameters\nkey – Identifying key of the artifact.\nartifact – The actual artifact to store.\nuid – A unique ID for this specific version of the artifact.\niter – The task iteration which generated this artifact. If iter is not None the iteration will\nbe added to the key provided to generate a unique key for the artifact of the specific iteration.\ntag – Tag of the artifact.\nproject – Project that the artifact belongs to.\nstore_feature_set(feature_set: Union[dict, mlrun.api.schemas.feature_store.FeatureSet, mlrun.feature_store.feature_set.FeatureSet], name=None, project='', tag=None, uid=None, versioned=True) → dict[source]#\nSave a FeatureSet object in the mlrun DB. The\nfeature-set can be either a new object or a modification to existing object referenced by the params of\nthe function.\nParameters\nfeature_set – The FeatureSet to store.\nproject – Name of project this feature-set belongs to.\ntag – The tag of the object to replace in the DB, for example latest.\nuid – The uid of the object to replace in the DB. If using this parameter, the modified object\nmust have the same uid of the previously-existing object. This cannot be used for non-versioned objects.\nversioned – Whether to maintain versions for this feature-set. All versions of a versioned object\nwill be kept in the DB and can be retrieved until explicitly deleted.\nReturns\nThe FeatureSet object (as dict).\nstore_feature_vector(feature_vector: Union[dict, mlrun.api.schemas.feature_store.FeatureVector, mlrun.feature_store.feature_vector.FeatureVector], name=None, project='', tag=None, uid=None, versioned=True) → dict[source]#\nStore a FeatureVector object in the mlrun DB. The\nfeature-vector can be either a new object or a modification to existing object referenced by the params\nof the function.\nParameters\nfeature_vector – The FeatureVector to store.\nproject – Name of project this feature-vector belongs to.\ntag – The tag of the object to replace in the DB, for example latest.\nuid – The uid of the object to replace in the DB. If using this parameter, the modified object\nmust have the same uid of the previously-existing object. This cannot be used for non-versioned objects.\nversioned – Whether to maintain versions for this feature-vector. All versions of a versioned object\nwill be kept in the DB and can be retrieved until explicitly deleted.\nReturns\nThe FeatureVector object (as dict).\nstore_function(function, name, project='', tag=None, versioned=False)[source]#\nStore a function object. Function is identified by its name and tag, and can be versioned.\nstore_log(uid, project='', body=None, append=False)[source]#\nSave a log persistently.\nParameters\nuid – Log unique ID\nproject – Project name for which this log belongs\nbody – The actual log to store\nappend – Whether to append the log provided in body to an existing log with the same uid or to\ncreate a new log. If set to False, an existing log with same uid will be overwritten\nstore_marketplace_source(source_name: str, source: Union[dict, mlrun.api.schemas.marketplace.IndexedMarketplaceSource])[source]#\nCreate or replace a marketplace source.\nFor an example of the source format and explanation of the source order logic,\nplease see create_marketplace_source(). This method can be used to modify the source itself or its\norder in the list of sources.\nParameters\nsource_name – Name of the source object to modify/create. It must match the source.metadata.name\nparameter in the source itself.\nsource – Source object to store in the database.\nReturns\nThe source object as stored in the DB.\nstore_project(name: str, project: Union[dict, mlrun.projects.project.MlrunProject, mlrun.api.schemas.project.Project]) → mlrun.projects.project.MlrunProject[source]#\nStore a project in the DB. This operation will overwrite existing project of the same name if exists.\nstore_run(struct, uid, project='', iter=0)[source]#\nStore run details in the DB. This method is usually called from within other mlrun flows\nand not called directly by the user.\nsubmit_job(runspec, schedule: Optional[Union[str, mlrun.api.schemas.schedule.ScheduleCronTrigger]] = None)[source]#\nSubmit a job for remote execution.\nParameters\nrunspec – The runtime object spec (Task) to execute.\nschedule – Whether to schedule this job using a Cron trigger. If not specified, the job will be submitted\nimmediately.\nsubmit_pipeline(project, pipeline, arguments=None, experiment=None, run=None, namespace=None, artifact_path=None, ops=None, ttl=None)[source]#\nSubmit a KFP pipeline for execution.\nParameters\nproject – The project of the pipeline\npipeline – Pipeline function or path to .yaml/.zip pipeline file.\narguments – A dictionary of arguments to pass to the pipeline.\nexperiment – A name to assign for the specific experiment.\nrun – A name for this specific run.\nnamespace – Kubernetes namespace to execute the pipeline in.\nartifact_path – A path to artifacts used by this pipeline.\nops – Transformers to apply on all ops in the pipeline.\nttl – Set the TTL for the pipeline after its completion.\ntag_artifacts(artifacts: Union[List[mlrun.artifacts.base.Artifact], List[dict], mlrun.artifacts.base.Artifact, dict], project: str, tag_name: str, replace: bool = False)[source]#\nTag a list of artifacts.\nParameters\nartifacts – The artifacts to tag. Can be a list of Artifact objects or\ndictionaries, or a single object.\nproject – Project which contains the artifacts.\ntag_name – The tag to set on the artifacts.\nreplace – If True, replace existing tags, otherwise append to existing tags.\ntag_objects(project: str, tag_name: str, objects: Union[mlrun.api.schemas.tag.TagObjects, dict], replace: bool = False)[source]#\nTag a list of objects.\nParameters\nproject – Project which contains the objects.\ntag_name – The tag to set on the objects.\nobjects – The objects to tag.\nreplace – Whether to replace the existing tags of the objects or to add the new tag to them.\ntrigger_migrations() → Optional[mlrun.api.schemas.background_task.BackgroundTask][source]#\nTrigger migrations (will do nothing if no migrations are needed) and wait for them to finish if actually\ntriggered\n:returns: BackgroundTask.\nupdate_run(updates: dict, uid, project='', iter=0)[source]#\nUpdate the details of a stored run in the DB.\nupdate_schedule(project: str, name: str, schedule: mlrun.api.schemas.schedule.ScheduleUpdate)[source]#\nUpdate an existing schedule, replace it with the details contained in the schedule object.\nverify_authorization(authorization_verification_input: mlrun.api.schemas.auth.AuthorizationVerificationInput)[source]#\nVerifies authorization for the provided action on the provided resource.\nParameters\nauthorization_verification_input – Instance of\nAuthorizationVerificationInput that includes all the needed parameters for\nthe auth verification\nwatch_log(uid, project='', watch=True, offset=0)[source]#\nRetrieve logs of a running process, and watch the progress of the execution until it completes. This\nmethod will print out the logs and continue to periodically poll for, and print, new logs as long as the\nstate of the runtime which generates this log is either pending or running.\nParameters\nuid – The uid of the log object to watch.\nproject – Project that the log belongs to.\nwatch – If set to True will continue tracking the log as described above. Otherwise this function\nis practically equivalent to the get_log() function.\noffset – Minimal offset in the log to watch.\nReturns\nThe final state of the log being watched.\nclass mlrun.api.schemas.secret.SecretProviderName(value)[source]#\nBases: str, enum.Enum\nEnum containing names of valid providers for secrets.\nkubernetes = 'kubernetes'#\nvault = 'vault'#"}
{"text": "mlrun.execution#\nclass mlrun.execution.MLClientCtx(autocommit=False, tmp='', log_stream=None)[source]#\nBases: object\nML Execution Client Context\nthe context is generated and injected to the function using the function.run()\nor manually using the get_or_create_ctx() call\nand provides an interface to use run params, metadata, inputs, and outputs\nbase metadata include: uid, name, project, and iteration (for hyper params)\nusers can set labels and annotations using set_label(), set_annotation()\naccess parameters and secrets using get_param(), get_secret()\naccess input data objects using get_input()\nstore results, artifacts, and real-time metrics using the log_result(),\nlog_artifact(), log_dataset() and log_model() methods\nsee doc for the individual params and methods\nproperty annotations#\ndictionary with annotations (read-only)\nartifact_subpath(*subpaths)[source]#\nsubpaths under output path artifacts path\nexample:\ndata_path=context.artifact_subpath('data')\nproperty artifacts#\ndictionary of artifacts (read-only)\ncommit(message: str = '', completed=True)[source]#\nsave run state and optionally add a commit message\nParameters\nmessage – commit message to save in the run\ncompleted – mark run as completed\nclassmethod from_dict(attrs: dict, rundb='', autocommit=False, tmp='', host=None, log_stream=None, is_api=False)[source]#\ncreate execution context from dict\nget_cached_artifact(key)[source]#\nreturn an logged artifact from cache (for potential updates)\nget_child_context(with_parent_params=False, **params)[source]#\nget child context (iteration)\nallow sub experiments (epochs, hyper-param, ..) under a parent\nwill create a new iteration, log_xx will update the child only\nuse commit_children() to save all the children and specify the best run\nexample:\ndef handler(context: mlrun.MLClientCtx, data: mlrun.DataItem):\ndf = data.as_df()\nbest_accuracy = accuracy_sum = 0\nfor param in param_list:\nwith context.get_child_context(myparam=param) as child:\naccuracy = child_handler(child, df, **child.parameters)\naccuracy_sum += accuracy\nchild.log_result('accuracy', accuracy)\nif accuracy > best_accuracy:\nchild.mark_as_best()\nbest_accuracy = accuracy\ncontext.log_result('avg_accuracy', accuracy_sum / len(param_list))\nParameters\nparams – extra (or override) params to parent context\nwith_parent_params – child will copy the parent parameters and add to them\nReturns\nchild context\nget_dataitem(url)[source]#\nget mlrun dataitem from url\nexample:\ndata = context.get_dataitem(\"s3://my-bucket/file.csv\").as_df()\nget_input(key: str, url: str = '')[source]#\nget an input DataItem object, data objects have methods such as\n.get(), .download(), .url, .. to access the actual data\nexample:\ndata = context.get_input(\"my_data\").get()\nget_meta() → dict[source]#\nReserved for internal use\nget_param(key: str, default=None)[source]#\nget a run parameter, or use the provided default if not set\nexample:\np1 = context.get_param(\"p1\", 0)\nget_project_param(key: str, default=None)[source]#\nget a parameter from the run’s project’s parameters\nget_secret(key: str)[source]#\nget a key based secret e.g. DB password from the context\nsecrets can be specified when invoking a run through vault, files, env, ..\nexample:\naccess_key = context.get_secret(\"ACCESS_KEY\")\nget_store_resource(url)[source]#\nget mlrun data resource (feature set/vector, artifact, item) from url\nexample:\nfeature_vector = context.get_store_resource(\"store://feature-vectors/default/myvec\")\ndataset = context.get_store_resource(\"store://artifacts/default/mydata\")\nParameters\nurl – store resource uri/path, store://<type>/<project>/<name>:<version>\ntypes: artifacts | feature-sets | feature-vectors\nproperty in_path#\ndefault input path for data objects\nproperty inputs#\ndictionary of input data items (read-only)\nproperty iteration#\nchild iteration index, for hyper parameters\nkind = 'run'#\nproperty labels#\ndictionary with labels (read-only)\nlog_artifact(item, body=None, local_path=None, artifact_path=None, tag='', viewer=None, target_path='', src_path=None, upload=None, labels=None, format=None, db_key=None, **kwargs)[source]#\nlog an output artifact and optionally upload it to datastore\nexample:\ncontext.log_artifact(\n\"some-data\",\nbody=b\"abc is 123\",\nlocal_path=\"model.txt\",\nlabels={\"framework\": \"xgboost\"},\n)\nParameters\nitem – artifact key or artifact class ()\nbody – will use the body as the artifact content\nlocal_path – path to the local file we upload, will also be use\nas the destination subpath (under “artifact_path”)\nartifact_path – target artifact path (when not using the default)\nto define a subpath under the default location use:\nartifact_path=context.artifact_subpath(‘data’)\ntag – version tag\nviewer – kubeflow viewer type\ntarget_path – absolute target path (instead of using artifact_path + local_path)\nsrc_path – deprecated, use local_path\nupload – upload to datastore (default is True)\nlabels – a set of key/value labels to tag the artifact with\nformat – optional, format to use (e.g. csv, parquet, ..)\ndb_key – the key to use in the artifact DB table, by default\nits run name + ‘_’ + key\ndb_key=False will not register it in the artifacts table\nReturns\nartifact object\nlog_dataset(key, df, tag='', local_path=None, artifact_path=None, upload=True, labels=None, format='', preview=None, stats=False, db_key=None, target_path='', extra_data=None, label_column: Optional[str] = None, **kwargs)[source]#\nlog a dataset artifact and optionally upload it to datastore\nexample:\nraw_data = {\n\"first_name\": [\"Jason\", \"Molly\", \"Tina\", \"Jake\", \"Amy\"],\n\"last_name\": [\"Miller\", \"Jacobson\", \"Ali\", \"Milner\", \"Cooze\"],\n\"age\": [42, 52, 36, 24, 73],\n\"testScore\": [25, 94, 57, 62, 70],\n}\ndf = pd.DataFrame(raw_data, columns=[\"first_name\", \"last_name\", \"age\", \"testScore\"])\ncontext.log_dataset(\"mydf\", df=df, stats=True)\nParameters\nkey – artifact key\ndf – dataframe object\nlabel_column – name of the label column (the one holding the target (y) values)\nlocal_path – path to the local file we upload, will also be use\nas the destination subpath (under “artifact_path”)\nartifact_path – target artifact path (when not using the default)\nto define a subpath under the default location use:\nartifact_path=context.artifact_subpath(‘data’)\ntag – version tag\nformat – optional, format to use (e.g. csv, parquet, ..)\ntarget_path – absolute target path (instead of using artifact_path + local_path)\npreview – number of lines to store as preview in the artifact metadata\nstats – calculate and store dataset stats in the artifact metadata\nextra_data – key/value list of extra files/charts to link with this dataset\nupload – upload to datastore (default is True)\nlabels – a set of key/value labels to tag the artifact with\ndb_key – the key to use in the artifact DB table, by default\nits run name + ‘_’ + key\ndb_key=False will not register it in the artifacts table\nReturns\nartifact object\nlog_iteration_results(best, summary: list, task: dict, commit=False)[source]#\nReserved for internal use\nproperty log_level#\nget the logging level, e.g. ‘debug’, ‘info’, ‘error’\nlog_metric(key: str, value, timestamp=None, labels=None)[source]#\nTBD, log a real-time time-series metric\nlog_metrics(keyvals: dict, timestamp=None, labels=None)[source]#\nTBD, log a set of real-time time-series metrics\nlog_model(key, body=None, framework='', tag='', model_dir=None, model_file=None, algorithm=None, metrics=None, parameters=None, artifact_path=None, upload=True, labels=None, inputs: Optional[List[mlrun.features.Feature]] = None, outputs: Optional[List[mlrun.features.Feature]] = None, feature_vector: Optional[str] = None, feature_weights: Optional[list] = None, training_set=None, label_column: Optional[Union[str, list]] = None, extra_data=None, db_key=None, **kwargs)[source]#\nlog a model artifact and optionally upload it to datastore\nexample:\ncontext.log_model(\"model\", body=dumps(model),\nmodel_file=\"model.pkl\",\nmetrics=context.results,\ntraining_set=training_df,\nlabel_column='label',\nfeature_vector=feature_vector_uri,\nlabels={\"app\": \"fraud\"})\nParameters\nkey – artifact key or artifact class ()\nbody – will use the body as the artifact content\nmodel_file – path to the local model file we upload (see also model_dir)\nor to a model file data url (e.g. http://host/path/model.pkl)\nmodel_dir – path to the local dir holding the model file and extra files\nartifact_path – target artifact path (when not using the default)\nto define a subpath under the default location use:\nartifact_path=context.artifact_subpath(‘data’)\nframework – name of the ML framework\nalgorithm – training algorithm name\ntag – version tag\nmetrics – key/value dict of model metrics\nparameters – key/value dict of model parameters\ninputs – ordered list of model input features (name, type, ..)\noutputs – ordered list of model output/result elements (name, type, ..)\nupload – upload to datastore (default is True)\nlabels – a set of key/value labels to tag the artifact with\nfeature_vector – feature store feature vector uri (store://feature-vectors/<project>/<name>[:tag])\nfeature_weights – list of feature weights, one per input column\ntraining_set – training set dataframe, used to infer inputs & outputs\nlabel_column – which columns in the training set are the label (target) columns\nextra_data – key/value list of extra files/charts to link with this dataset\nvalue can be absolute path | relative path (to model dir) | bytes | artifact object\ndb_key – the key to use in the artifact DB table, by default\nits run name + ‘_’ + key\ndb_key=False will not register it in the artifacts table\nReturns\nartifact object\nlog_result(key: str, value, commit=False)[source]#\nlog a scalar result value\nexample:\ncontext.log_result('accuracy', 0.85)\nParameters\nkey – result key\nvalue – result value\ncommit – commit (write to DB now vs wait for the end of the run)\nlog_results(results: dict, commit=False)[source]#\nlog a set of scalar result values\nexample:\ncontext.log_results({'accuracy': 0.85, 'loss': 0.2})\nParameters\nresults – key/value dict or results\ncommit – commit (write to DB now vs wait for the end of the run)\nproperty logger#\nbuilt-in logger interface\nexample:\ncontext.logger.info(\"started experiment..\", param=5)\nmark_as_best()[source]#\nmark a child as the best iteration result, see .get_child_context()\nproperty out_path#\ndefault output path for artifacts\nproperty parameters#\ndictionary of run parameters (read-only)\nproperty project#\nproject name, runs can be categorized by projects\nproperty results#\ndictionary of results (read-only)\nset_annotation(key: str, value, replace: bool = True)[source]#\nset/record a specific annotation\nexample:\ncontext.set_annotation(\"comment\", \"some text\")\nset_hostname(host: str)[source]#\nupdate the hostname, for internal use\nset_label(key: str, value, replace: bool = True)[source]#\nset/record a specific label\nexample:\ncontext.set_label(\"framework\", \"sklearn\")\nset_logger_stream(stream)[source]#\nset_state(state: Optional[str] = None, error: Optional[str] = None, commit=True)[source]#\nmodify and store the run state or mark an error\nParameters\nstate – set run state\nerror – error message (if exist will set the state to error)\ncommit – will immediately update the state in the DB\nproperty tag#\nrun tag (uid or workflow id if exists)\nto_dict()[source]#\nconvert the run context to a dictionary\nto_json()[source]#\nconvert the run context to a json buffer\nto_yaml()[source]#\nconvert the run context to a yaml buffer\nproperty uid#\nUnique run id\nupdate_artifact(artifact_object)[source]#\nupdate an artifact object in the cache and the DB\nupdate_child_iterations(best_run=0, commit_children=False, completed=True)[source]#\nupdate children results in the parent, and optionally mark the best\nParameters\nbest_run – marks the child iteration number (starts from 1)\ncommit_children – commit all child runs to the db\ncompleted – mark children as completed"}
{"text": "mlrun.feature_store#\nclass mlrun.feature_store.Entity(name: Optional[str] = None, value_type: Optional[mlrun.data_types.data_types.ValueType] = None, description: Optional[str] = None, labels: Optional[Dict[str, str]] = None)[source]#\nBases: mlrun.model.ModelObj\ndata entity (index)\ndata entity (index key)\nParameters\nname – entity name\nvalue_type – type of the entity, e.g. ValueType.STRING, ValueType.INT\ndescription – test description of the entity\nlabels – a set of key/value labels (tags)\nclass mlrun.feature_store.Feature(value_type: Optional[str] = None, dims: Optional[List[int]] = None, description: Optional[str] = None, aggregate: Optional[bool] = None, name: Optional[str] = None, validator=None, default: Optional[str] = None, labels: Optional[Dict[str, str]] = None)[source]#\nBases: mlrun.model.ModelObj\ndata feature\ndata feature\nFeatures can be specified manually or inferred automatically (during ingest/preview)\nParameters\nvalue_type – type of the feature. Use the ValueType constants library e.g. ValueType.STRING,\nValueType.INT\ndims – list of dimensions for vectors/tensors, e.g. [2, 2]\ndescription – text description of the feature\naggregate – is it an aggregated value\nname – name of the feature\nvalidator – feature validation policy\ndefault – default value\nlabels – a set of key/value labels (tags)\nproperty validator#\nclass mlrun.feature_store.FeatureSet(name: Optional[str] = None, description: Optional[str] = None, entities: Optional[List[Union[mlrun.features.Entity, str]]] = None, timestamp_key: Optional[str] = None, engine: Optional[str] = None, label_column: Optional[str] = None)[source]#\nBases: mlrun.model.ModelObj\nFeature set object, defines a set of features and their data pipeline\nFeature set object, defines a set of features and their data pipeline\nexample:\nimport mlrun.feature_store as fstore\nticks = fstore.FeatureSet(\"ticks\", entities=[\"stock\"], timestamp_key=\"timestamp\")\nfstore.ingest(ticks, df)\nParameters\nname – name of the feature set\ndescription – text description\nentities – list of entity (index key) names or Entity\ntimestamp_key – timestamp column name\nengine – name of the processing engine (storey, pandas, or spark), defaults to storey\nlabel_column – name of the label column (the one holding the target (y) values)\nadd_aggregation(column, operations, windows, period=None, name=None, step_name=None, after=None, before=None, state_name=None, emit_policy: Optional[storey.dtypes.EmitPolicy] = None)[source]#\nadd feature aggregation rule\nexample:\nmyset.add_aggregation(\"ask\", [\"sum\", \"max\"], \"1h\", \"10m\", name=\"asks\")\nParameters\ncolumn – name of column/field aggregate. Do not name columns starting with either _ or aggr_.\nThey are reserved for internal use, and the data does not ingest correctly.\nWhen using the pandas engine, do not use spaces (` ) or periods (.`) in the column names;\nthey cause errors in the ingestion.\noperations – aggregation operations, e.g. [‘sum’, ‘std’]\nwindows – time windows, can be a single window, e.g. ‘1h’, ‘1d’,\nor a list of same unit windows e.g. [‘1h’, ‘6h’]\nwindows are transformed to fixed windows or\nsliding windows depending whether period parameter\nprovided.\nSliding window is fixed-size overlapping windows\nthat slides with time.\nThe window size determines the size of the sliding window\nand the period determines the step size to slide.\nPeriod must be integral divisor of the window size.\nIf the period is not provided then fixed windows is used.\nFixed window is fixed-size, non-overlapping, gap-less window.\nThe window is referred to as a tumbling window.\nIn this case, each record on an in-application stream belongs\nto a specific window. It is processed only once\n(when the query processes the window to which the record belongs).\nperiod – optional, sliding window granularity, e.g. ’20s’ ‘10m’  ‘3h’ ‘7d’\nname – optional, aggregation name/prefix. Must be unique per feature set. If not passed,\nthe column will be used as name.\nstep_name – optional, graph step name\nstate_name – Deprecated - use step_name instead\nafter – optional, after which graph step it runs\nbefore – optional, comes before graph step\nemit_policy – optional, which emit policy to use when performing the aggregations. Use the derived\nclasses of storey.EmitPolicy. The default is to emit every period for Spark engine\nand emit every event for storey. Currently the only other supported option is to use\nemit_policy=storey.EmitEveryEvent() when using the Spark engine to emit every event\nadd_entity(name: str, value_type: Optional[mlrun.data_types.data_types.ValueType] = None, description: Optional[str] = None, labels: Optional[Dict[str, str]] = None)[source]#\nadd/set an entity (dataset index)\nexample:\nimport mlrun.feature_store as fstore\nticks = fstore.FeatureSet(\"ticks\",\nentities=[\"stock\"],\ntimestamp_key=\"timestamp\")\nticks.add_entity(\"country\",\nmlrun.data_types.ValueType.STRING,\ndescription=\"stock country\")\nticks.add_entity(\"year\", mlrun.data_types.ValueType.INT16)\nticks.save()\nParameters\nname – entity name\nvalue_type – type of the entity (default to ValueType.STRING)\ndescription – description of the entity\nlabels – label tags dict\nadd_feature(feature: mlrun.features.Feature, name=None)[source]#\nadd/set a feature\nexample:\nimport mlrun.feature_store as fstore\nfrom mlrun.features import Feature\nticks = fstore.FeatureSet(\"ticks\",\nentities=[\"stock\"],\ntimestamp_key=\"timestamp\")\nticks.add_feature(Feature(value_type=mlrun.data_types.ValueType.STRING,\ndescription=\"client consistency\"),\"ABC01\")\nticks.add_feature(Feature(value_type=mlrun.data_types.ValueType.FLOAT,\ndescription=\"client volatility\"),\"SAB\")\nticks.save()\nParameters\nfeature – setting of Feature\nname – feature name\nproperty fullname: str#\n{tag}]\nType\nfull name in the form {project}/{name}[\nget_stats_table()[source]#\nget feature statistics table (as dataframe)\nget_target_path(name=None)[source]#\nget the url/path for an offline or specified data target\nproperty graph#\nfeature set transformation graph/DAG\nhas_valid_source()[source]#\ncheck if object’s spec has a valid (non empty) source definition\nkind = 'FeatureSet'#\nlink_analysis(name, uri)[source]#\nadd a linked file/artifact (chart, data, ..)\nproperty metadata: mlrun.model.VersionedObjMetadata#\nplot(filename=None, format=None, with_targets=False, **kw)[source]#\ngenerate graphviz plot\npurge_targets(target_names: Optional[List[str]] = None, silent: bool = False)[source]#\nDelete data of specific targets\n:param target_names: List of names of targets to delete (default: delete all ingested targets)\n:param silent: Fail silently if target doesn’t exist in featureset status\nreload(update_spec=True)[source]#\nreload/sync the feature vector status and spec from the DB\nsave(tag='', versioned=False)[source]#\nsave to mlrun db\nset_targets(targets=None, with_defaults=True, default_final_step=None, default_final_state=None)[source]#\nset the desired target list or defaults\nParameters\ntargets – list of target type names (‘csv’, ‘nosql’, ..) or target objects\nCSVTarget(), ParquetTarget(), NoSqlTarget(), StreamTarget(), ..\nwith_defaults – add the default targets (as defined in the central config)\ndefault_final_step – the final graph step after which we add the\ntarget writers, used when the graph branches and\nthe end cant be determined automatically\ndefault_final_state – Deprecated - use default_final_step instead\nproperty spec: mlrun.feature_store.feature_set.FeatureSetSpec#\nproperty status: mlrun.feature_store.feature_set.FeatureSetStatus#\nto_dataframe(columns=None, df_module=None, target_name=None, start_time=None, end_time=None, time_column=None, **kwargs)[source]#\nreturn featureset (offline) data as dataframe\nParameters\ncolumns – list of columns to select (if not all)\ndf_module – py module used to create the DataFrame (pd for Pandas, dd for Dask, ..)\ntarget_name – select a specific target (material view)\nstart_time – filter by start time\nend_time – filter by end time\ntime_column – specify the time column name in the file\nkwargs – additional reader (csv, parquet, ..) args\nReturns\nDataFrame\nupdate_targets_for_ingest(targets: List[mlrun.model.DataTargetBase], overwrite: Optional[bool] = None)[source]#\nproperty uri#\nfully qualified feature set uri\nclass mlrun.feature_store.FeatureVector(name=None, features=None, label_feature=None, description=None, with_indexes=None)[source]#\nBases: mlrun.model.ModelObj\nFeature vector, specify selected features, their metadata and material views\nFeature vector, specify selected features, their metadata and material views\nexample:\nimport mlrun.feature_store as fstore\nfeatures = [\"quotes.bid\", \"quotes.asks_sum_5h as asks_5h\", \"stocks.*\"]\nvector = fstore.FeatureVector(\"my-vec\", features)\n# get the vector as a dataframe\ndf = fstore.get_offline_features(vector).to_dataframe()\n# return an online/real-time feature service\nsvc = fs.get_online_feature_service(vector, impute_policy={\"*\": \"$mean\"})\nresp = svc.get([{\"stock\": \"GOOG\"}])\nParameters\nname – List of names of targets to delete (default: delete all ingested targets)\nfeatures – list of feature to collect to this vector.\nFormat [<project>/]<feature_set>.<feature_name or *> [as <alias>]\nlabel_feature – feature name to be used as label data\ndescription – text description of the vector\nwith_indexes – whether to keep the entity and timestamp columns in the response\nget_feature_aliases()[source]#\nget_stats_table()[source]#\nget feature statistics table (as dataframe)\nget_target_path(name=None)[source]#\nkind = 'FeatureVector'#\nlink_analysis(name, uri)[source]#\nadd a linked file/artifact (chart, data, ..)\nproperty metadata: mlrun.model.VersionedObjMetadata#\nparse_features(offline=True, update_stats=False)[source]#\nparse and validate feature list (from vector) and add metadata from feature sets\n:returnsfeature_set_objects: cache of used feature set objects\nfeature_set_fields:  list of field (name, alias) per featureset\nreload(update_spec=True)[source]#\nreload/sync the feature set status and spec from the DB\nsave(tag='', versioned=False)[source]#\nsave to mlrun db\nproperty spec: mlrun.feature_store.feature_vector.FeatureVectorSpec#\nproperty status: mlrun.feature_store.feature_vector.FeatureVectorStatus#\nto_dataframe(df_module=None, target_name=None)[source]#\nreturn feature vector (offline) data as dataframe\nproperty uri#\nfully qualified feature vector uri\nclass mlrun.feature_store.FixedWindowType(value)[source]#\nBases: enum.Enum\nAn enumeration.\nCurrentOpenWindow = 1#\nLastClosedWindow = 2#\nto_qbk_fixed_window_type()[source]#\nclass mlrun.feature_store.OfflineVectorResponse(merger)[source]#\nBases: object\nget_offline_features response object\nproperty status#\nvector prep job status (ready, running, error)\nto_csv(target_path, **kw)[source]#\nreturn results as csv file\nto_dataframe(to_pandas=True)[source]#\nreturn result as dataframe\nto_parquet(target_path, **kw)[source]#\nreturn results as parquet file\nclass mlrun.feature_store.OnlineVectorService(vector, graph, index_columns, impute_policy: Optional[dict] = None)[source]#\nBases: object\nget_online_feature_service response object\nclose()[source]#\nterminate the async loop\nget(entity_rows: List[Union[dict, list]], as_list=False)[source]#\nget feature vector given the provided entity inputs\ntake a list of input vectors/rows and return a list of enriched feature vectors\neach input and/or output vector can be a list of values or a dictionary of field names and values,\nto return the vector as a list of values set the as_list to True.\nif the input is a list of list (vs a list of dict), the values in the list will correspond to the\nindex/entity values, i.e. [[“GOOG”], [“MSFT”]] means “GOOG” and “MSFT” are the index/entity fields.\nexample:\n# accept list of dict, return list of dict\nsvc = fs.get_online_feature_service(vector)\nresp = svc.get([{\"name\": \"joe\"}, {\"name\": \"mike\"}])\n# accept list of list, return list of list\nsvc = fs.get_online_feature_service(vector, as_list=True)\nresp = svc.get([[\"joe\"], [\"mike\"]])\nParameters\nentity_rows – list of list/dict with input entity data/rows\nas_list – return a list of list (list input is required by many ML frameworks)\ninitialize()[source]#\ninternal, init the feature service and prep the imputing logic\nproperty status#\nvector merger function status (ready, running, error)\nclass mlrun.feature_store.RunConfig(function: Optional[Union[str, mlrun.runtimes.function_reference.FunctionReference, mlrun.runtimes.base.BaseRuntime]] = None, local: Optional[bool] = None, image: Optional[str] = None, kind: Optional[str] = None, handler: Optional[str] = None, parameters: Optional[dict] = None, watch: Optional[bool] = None, owner=None, credentials: Optional[mlrun.model.Credentials] = None, code: Optional[str] = None, requirements: Optional[Union[str, List[str]]] = None, extra_spec: Optional[dict] = None, auth_info=None)[source]#\nBases: object\nclass for holding function and run specs for jobs and serving functions\nclass for holding function and run specs for jobs and serving functions\nwhen running feature ingestion or merging tasks we use the RunConfig class to pass\nthe desired function and job configuration.\nthe apply() method is used to set resources like volumes, the with_secret() method adds secrets\nMost attributes are optional, if not specified a proper default value will be set\nexamples:\n# config for local run emulation\nconfig = RunConfig(local=True)\n# config for using empty/default code\nconfig = RunConfig()\n# config for using .py/.ipynb file with image and extra package requirements\nconfig = RunConfig(\"mycode.py\", image=\"mlrun/mlrun\", requirements=[\"spacy\"])\n# config for using function object\nfunction = mlrun.import_function(\"hub://some_function\")\nconfig = RunConfig(function)\nParameters\nfunction – this can be function uri or function object or path to function code (.py/.ipynb)\nor a FunctionReference\nthe function define the code, dependencies, and resources\nlocal – use True to simulate local job run or mock service\nimage – function container image\nkind – function runtime kind (job, serving, spark, ..), required when function points to code\nhandler – the function handler to execute (for jobs or nuclio)\nparameters – job parameters\nwatch – in batch jobs will wait for the job completion and print job logs to the console\nowner – job owner\ncredentials – job credentials\ncode – function source code (as string)\nrequirements – python requirements file path or list of packages\nextra_spec – additional dict with function spec fields/values to add to the function\nauth_info – authentication info. For internal use when running on server\napply(modifier)[source]#\napply a modifier to add/set function resources like volumes\nexample:\nrun_config.apply(mlrun.platforms.auto_mount())\ncopy()[source]#\nproperty function#\nto_function(default_kind=None, default_image=None)[source]#\ninternal, generate function object\nwith_secret(kind, source)[source]#\nregister a secrets source (file, env or dict)\nread secrets from a source provider to be used in jobs, example:\nrun_config.with_secrets('file', 'file.txt')\nrun_config.with_secrets('inline', {'key': 'val'})\nrun_config.with_secrets('env', 'ENV1,ENV2')\nrun_config.with_secrets('vault', ['secret1', 'secret2'...])\nParameters\nkind – secret type (file, inline, env, vault)\nsource – secret data or link (see example)\nReturns\nThis (self) object\nmlrun.feature_store.delete_feature_set(name, project='', tag=None, uid=None, force=False)[source]#\nDelete a FeatureSet object from the DB.\n:param name: Name of the object to delete\n:param project: Name of the object’s project\n:param tag: Specific object’s version tag\n:param uid: Specific object’s uid\n:param force: Delete feature set without purging its targets\nIf tag or uid are specified, then just the version referenced by them will be deleted. Using bothis not allowed.\nIf none are specified, then all instances of the object whose name is name will be deleted.\nmlrun.feature_store.delete_feature_vector(name, project='', tag=None, uid=None)[source]#\nDelete a FeatureVector object from the DB.\n:param name: Name of the object to delete\n:param project: Name of the object’s project\n:param tag: Specific object’s version tag\n:param uid: Specific object’s uid\nIf tag or uid are specified, then just the version referenced by them will be deleted. Using bothis not allowed.\nIf none are specified, then all instances of the object whose name is name will be deleted.\nmlrun.feature_store.deploy_ingestion_service(featureset: Union[mlrun.feature_store.feature_set.FeatureSet, str], source: Optional[mlrun.model.DataSource] = None, targets: Optional[List[mlrun.model.DataTargetBase]] = None, name: Optional[str] = None, run_config: Optional[mlrun.feature_store.common.RunConfig] = None, verbose=False)[source]#\nStart real-time ingestion service using nuclio function\nDeploy a real-time function implementing feature ingestion pipeline\nthe source maps to Nuclio event triggers (http, kafka, v3io stream, etc.)\nthe run_config parameter allow specifying the function and job configuration,\nsee: RunConfig\nexample:\nsource = HTTPSource()\nfunc = mlrun.code_to_function(\"ingest\", kind=\"serving\").apply(mount_v3io())\nconfig = RunConfig(function=func)\nfs.deploy_ingestion_service(my_set, source, run_config=config)\nParameters\nfeatureset – feature set object or uri\nsource – data source object describing the online or offline source\ntargets – list of data target objects\nname – name for the job/function\nrun_config – service runtime configuration (function object/uri, resources, etc..)\nverbose – verbose log\nmlrun.feature_store.get_feature_set(uri, project=None)[source]#\nget feature set object from the db\nParameters\nuri – a feature set uri({project}/{name}[:version])\nproject – project name if not specified in uri or not using the current/default\nmlrun.feature_store.get_feature_vector(uri, project=None)[source]#\nget feature vector object from the db\nParameters\nuri – a feature vector uri({project}/{name}[:version])\nproject – project name if not specified in uri or not using the current/default\nmlrun.feature_store.get_offline_features(feature_vector: Union[str, mlrun.feature_store.feature_vector.FeatureVector], entity_rows=None, entity_timestamp_column: Optional[str] = None, target: Optional[mlrun.model.DataTargetBase] = None, run_config: Optional[mlrun.feature_store.common.RunConfig] = None, drop_columns: Optional[List[str]] = None, start_time: Optional[Union[str, pandas._libs.tslibs.timestamps.Timestamp]] = None, end_time: Optional[Union[str, pandas._libs.tslibs.timestamps.Timestamp]] = None, with_indexes: bool = False, update_stats: bool = False, engine: Optional[str] = None, engine_args: Optional[dict] = None, query: Optional[str] = None) → mlrun.feature_store.feature_vector.OfflineVectorResponse[source]#\nretrieve offline feature vector results\nspecify a feature vector object/uri and retrieve the desired features, their metadata\nand statistics. returns OfflineVectorResponse,\nresults can be returned as a dataframe or written to a target\nThe start_time and end_time attributes allow filtering the data to a given time range, they accept\nstring values or pandas Timestamp objects, string values can also be relative, for example:\n“now”, “now - 1d2h”, “now+5m”, where a valid pandas Timedelta string follows the verb “now”,\nfor time alignment you can use the verb “floor” e.g. “now -1d floor 1H” will align the time to the last hour\n(the floor string is passed to pandas.Timestamp.floor(), can use D, H, T, S for day, hour, min, sec alignment).\nAnother option to filter the data is by the query argument - can be seen in the example.\nexample:\nfeatures = [\n\"stock-quotes.bid\",\n\"stock-quotes.asks_sum_5h\",\n\"stock-quotes.ask as mycol\",\n\"stocks.*\",\n]\nvector = FeatureVector(features=features)\nresp = get_offline_features(\nvector, entity_rows=trades, entity_timestamp_column=\"time\", query=\"ticker in ['GOOG'] and bid>100\"\n)\nprint(resp.to_dataframe())\nprint(vector.get_stats_table())\nresp.to_parquet(\"./out.parquet\")\nParameters\nfeature_vector – feature vector uri or FeatureVector object. passing feature vector obj requires update\npermissions\nentity_rows – dataframe with entity rows to join with\ntarget – where to write the results to\ndrop_columns – list of columns to drop from the final result\nentity_timestamp_column – timestamp column name in the entity rows dataframe\nrun_config – function and/or run configuration\nsee RunConfig\nstart_time – datetime, low limit of time needed to be filtered. Optional.\nentity_timestamp_column must be passed when using time filtering.\nend_time – datetime, high limit of time needed to be filtered. Optional.\nentity_timestamp_column must be passed when using time filtering.\nwith_indexes – return vector with index columns and timestamp_key from the feature sets (default False)\nupdate_stats – update features statistics from the requested feature sets on the vector. Default is False.\nengine – processing engine kind (“local”, “dask”, or “spark”)\nengine_args – kwargs for the processing engine\nquery – The query string used to filter rows\nmlrun.feature_store.get_online_feature_service(feature_vector: Union[str, mlrun.feature_store.feature_vector.FeatureVector], run_config: Optional[mlrun.feature_store.common.RunConfig] = None, fixed_window_type: mlrun.feature_store.feature_vector.FixedWindowType = FixedWindowType.LastClosedWindow, impute_policy: Optional[dict] = None, update_stats: bool = False) → mlrun.feature_store.feature_vector.OnlineVectorService[source]#\ninitialize and return online feature vector service api,\nreturns OnlineVectorService\nUsage\nThere are two ways to use the function:\nAs context manager\nExample:\nwith get_online_feature_service(vector_uri) as svc:\nresp = svc.get([{\"ticker\": \"GOOG\"}, {\"ticker\": \"MSFT\"}])\nprint(resp)\nresp = svc.get([{\"ticker\": \"AAPL\"}], as_list=True)\nprint(resp)\nExample with imputing:\nwith get_online_feature_service(vector_uri, impute_policy={\"*\": \"$mean\", \"amount\": 0)) as svc:\nresp = svc.get([{\"id\": \"C123487\"}])\nas simple function, note that in that option you need to close the session.\nExample:\nsvc = get_online_feature_service(vector_uri)\ntry:\nresp = svc.get([{\"ticker\": \"GOOG\"}, {\"ticker\": \"MSFT\"}])\nprint(resp)\nresp = svc.get([{\"ticker\": \"AAPL\"}], as_list=True)\nprint(resp)\nfinally:\nsvc.close()\nExample with imputing:\nsvc = get_online_feature_service(vector_uri, impute_policy={\"*\": \"$mean\", \"amount\": 0))\ntry:\nresp = svc.get([{\"id\": \"C123487\"}])\nexcept Exception as e:\nhandling exception...\nfinally:\nsvc.close()\nParameters\nfeature_vector – feature vector uri or FeatureVector object. passing feature vector obj requires update\npermissions\nrun_config – function and/or run configuration for remote jobs/services\nimpute_policy – a dict with impute_policy per feature, the dict key is the feature name and the dict\nvalue indicate which value will be used in case the feature is NaN/empty, the replaced\nvalue can be fixed number for constants or $mean, $max, $min, $std, $count for statistical\nvalues. “*” is used to specify the default for all features, example: {“*”: “$mean”}\nfixed_window_type – determines how to query the fixed window values which were previously inserted by ingest\nupdate_stats – update features statistics from the requested feature sets on the vector. Default: False.\nmlrun.feature_store.ingest(featureset: Optional[Union[mlrun.feature_store.feature_set.FeatureSet, str]] = None, source=None, targets: Optional[List[mlrun.model.DataTargetBase]] = None, namespace=None, return_df: bool = True, infer_options: mlrun.data_types.data_types.InferOptions = 63, run_config: Optional[mlrun.feature_store.common.RunConfig] = None, mlrun_context=None, spark_context=None, overwrite=None) → Optional[pandas.core.frame.DataFrame][source]#\nRead local DataFrame, file, URL, or source into the feature store\nIngest reads from the source, run the graph transformations, infers  metadata and stats\nand writes the results to the default of specified targets\nwhen targets are not specified data is stored in the configured default targets\n(will usually be NoSQL for real-time and Parquet for offline).\nthe run_config parameter allow specifying the function and job configuration,\nsee: RunConfig\nexample:\nstocks_set = FeatureSet(\"stocks\", entities=[Entity(\"ticker\")])\nstocks = pd.read_csv(\"stocks.csv\")\ndf = ingest(stocks_set, stocks, infer_options=fstore.InferOptions.default())\n# for running as remote job\nconfig = RunConfig(image='mlrun/mlrun')\ndf = ingest(stocks_set, stocks, run_config=config)\n# specify source and targets\nsource = CSVSource(\"mycsv\", path=\"measurements.csv\")\ntargets = [CSVTarget(\"mycsv\", path=\"./mycsv.csv\")]\ningest(measurements, source, targets)\nParameters\nfeatureset – feature set object or featureset.uri. (uri must be of a feature set that is in the DB,\ncall .save() if it’s not)\nsource – source dataframe or other sources (e.g. parquet source see:\nParquetSource and other classes in mlrun.datastore with suffix\nSource)\ntargets – optional list of data target objects\nnamespace – namespace or module containing graph classes\nreturn_df – indicate if to return a dataframe with the graph results\ninfer_options – schema and stats infer options\nrun_config – function and/or run configuration for remote jobs,\nsee RunConfig\nmlrun_context – mlrun context (when running as a job), for internal use !\nspark_context – local spark session for spark ingestion, example for creating the spark context:\nspark = SparkSession.builder.appName(“Spark function”).getOrCreate()\nFor remote spark ingestion, this should contain the remote spark service name\noverwrite – delete the targets’ data prior to ingestion\n(default: True for non scheduled ingest - deletes the targets that are about to be ingested.\nFalse for scheduled ingest - does not delete the target)\nReturns\nif return_df is True, a dataframe will be returned based on the graph\nmlrun.feature_store.preview(featureset: mlrun.feature_store.feature_set.FeatureSet, source, entity_columns: Optional[list] = None, timestamp_key: Optional[str] = None, namespace=None, options: Optional[mlrun.data_types.data_types.InferOptions] = None, verbose: bool = False, sample_size: Optional[int] = None) → pandas.core.frame.DataFrame[source]#\nrun the ingestion pipeline with local DataFrame/file data and infer features schema and stats\nexample:\nquotes_set = FeatureSet(\"stock-quotes\", entities=[Entity(\"ticker\")])\nquotes_set.add_aggregation(\"ask\", [\"sum\", \"max\"], [\"1h\", \"5h\"], \"10m\")\nquotes_set.add_aggregation(\"bid\", [\"min\", \"max\"], [\"1h\"], \"10m\")\ndf = preview(\nquotes_set,\nquotes_df,\nentity_columns=[\"ticker\"],\ntimestamp_key=\"time\",\n)\nParameters\nfeatureset – feature set object or uri\nsource – source dataframe or csv/parquet file path\nentity_columns – list of entity (index) column names\ntimestamp_key – timestamp column name\nnamespace – namespace or module containing graph classes\noptions – schema and stats infer options (InferOptions)\nverbose – verbose log\nsample_size – num of rows to sample from the dataset (for large datasets)\nclass mlrun.feature_store.feature_set.FeatureSetSpec(owner=None, description=None, entities=None, features=None, partition_keys=None, timestamp_key=None, label_column=None, relations=None, source=None, targets=None, graph=None, function=None, analysis=None, engine=None, output_path=None)[source]#\nFeature set spec object, defines the feature-set’s configuration.\nWarning\nThis class should not be modified directly. It is managed by the parent feature-set object or using\nfeature-store APIs. Modifying the spec manually may result in unpredictable behaviour.\nParameters\ndescription – text description (copied from parent feature-set)\nentities – list of entity (index key) names or Entity\nfeatures – list of features - Feature\npartition_keys – list of fields to partition results by (other than the default timestamp key)\ntimestamp_key – timestamp column name\nlabel_column – name of the label column (the one holding the target (y) values)\ntargets – list of data targets\ngraph – the processing graph\nfunction – MLRun runtime to execute the feature-set in\nengine – name of the processing engine (storey, pandas, or spark), defaults to storey\noutput_path – default location where to store results (defaults to MLRun’s artifact path)\nclass mlrun.feature_store.feature_set.FeatureSetStatus(state=None, targets=None, stats=None, preview=None, function_uri=None, run_uri=None)[source]#\nFeature set status object, containing the current feature-set’s status.\nWarning\nThis class should not be modified directly. It is managed by the parent feature-set object or using\nfeature-store APIs. Modifying the status manually may result in unpredictable behaviour.\nParameters\nstate – object’s current state\ntargets – list of the data targets used in the last ingestion operation\nstats – feature statistics calculated in the last ingestion (if stats calculation was requested)\npreview – preview of the feature-set contents (if preview generation was requested)\nfunction_uri – function used to execute the feature-set graph\nrun_uri – last run used for ingestion\nclass mlrun.feature_store.steps.DateExtractor(parts: Union[Dict[str, str], List[str]], timestamp_col: Optional[str] = None, **kwargs)[source]#\nDate Extractor allows you to extract a date-time component\nDate Extractor extract a date-time component into new columns\nThe extracted date part will appear as <timestamp_col>_<date_part> feature.\nSupports part values:\nasm8:              Return numpy datetime64 format in nanoseconds.\nday_of_week:       Return day of the week.\nday_of_year:       Return the day of the year.\ndayofweek:         Return day of the week.\ndayofyear:         Return the day of the year.\ndays_in_month:     Return the number of days in the month.\ndaysinmonth:       Return the number of days in the month.\nfreqstr:           Return the total number of days in the month.\nis_leap_year:      Return True if year is a leap year.\nis_month_end:      Return True if date is last day of month.\nis_month_start:    Return True if date is first day of month.\nis_quarter_end:    Return True if date is last day of the quarter.\nis_quarter_start:  Return True if date is first day of the quarter.\nis_year_end:       Return True if date is last day of the year.\nis_year_start:     Return True if date is first day of the year.\nquarter:           Return the quarter of the year.\ntz:                Alias for tzinfo.\nweek:              Return the week number of the year.\nweekofyear:        Return the week number of the year.\nexample:\n# (taken from the fraud-detection end-to-end feature store demo)\n# Define the Transactions FeatureSet\ntransaction_set = fs.FeatureSet(\"transactions\",\nentities=[fs.Entity(\"source\")],\ntimestamp_key='timestamp',\ndescription=\"transactions feature set\")\n# Get FeatureSet computation graph\ntransaction_graph = transaction_set.graph\n# Add the custom `DateExtractor` step\n# to the computation graph\ntransaction_graph.to(\nclass_name='DateExtractor',\nname='Extract Dates',\nparts = ['hour', 'day_of_week'],\ntimestamp_col = 'timestamp',\n)\nParameters\nparts – list of pandas style date-time parts you want to extract.\ntimestamp_col – The name of the column containing the timestamps to extract from,\nby default “timestamp”\n__init__(parts: Union[Dict[str, str], List[str]], timestamp_col: Optional[str] = None, **kwargs)[source]#\nDate Extractor extract a date-time component into new columns\nThe extracted date part will appear as <timestamp_col>_<date_part> feature.\nSupports part values:\nasm8:              Return numpy datetime64 format in nanoseconds.\nday_of_week:       Return day of the week.\nday_of_year:       Return the day of the year.\ndayofweek:         Return day of the week.\ndayofyear:         Return the day of the year.\ndays_in_month:     Return the number of days in the month.\ndaysinmonth:       Return the number of days in the month.\nfreqstr:           Return the total number of days in the month.\nis_leap_year:      Return True if year is a leap year.\nis_month_end:      Return True if date is last day of month.\nis_month_start:    Return True if date is first day of month.\nis_quarter_end:    Return True if date is last day of the quarter.\nis_quarter_start:  Return True if date is first day of the quarter.\nis_year_end:       Return True if date is last day of the year.\nis_year_start:     Return True if date is first day of the year.\nquarter:           Return the quarter of the year.\ntz:                Alias for tzinfo.\nweek:              Return the week number of the year.\nweekofyear:        Return the week number of the year.\nexample:\n# (taken from the fraud-detection end-to-end feature store demo)\n# Define the Transactions FeatureSet\ntransaction_set = fs.FeatureSet(\"transactions\",\nentities=[fs.Entity(\"source\")],\ntimestamp_key='timestamp',\ndescription=\"transactions feature set\")\n# Get FeatureSet computation graph\ntransaction_graph = transaction_set.graph\n# Add the custom `DateExtractor` step\n# to the computation graph\ntransaction_graph.to(\nclass_name='DateExtractor',\nname='Extract Dates',\nparts = ['hour', 'day_of_week'],\ntimestamp_col = 'timestamp',\n)\nParameters\nparts – list of pandas style date-time parts you want to extract.\ntimestamp_col – The name of the column containing the timestamps to extract from,\nby default “timestamp”\nclass mlrun.feature_store.steps.DropFeatures(features: List[str], **kwargs)[source]#\nDrop all the features from feature list\nParameters\nfeatures – string list of the features names to drop\nexample:\nfeature_set = fs.FeatureSet(\"fs-new\",\nentities=[fs.Entity(\"id\")],\ndescription=\"feature set\",\nengine=\"pandas\",\n)\n# Pre-processing grpah steps\nfeature_set.graph.to(DropFeatures(features=[\"age\"]))\ndf_pandas = fs.ingest(feature_set, data)\n__init__(features: List[str], **kwargs)[source]#\nDrop all the features from feature list\nParameters\nfeatures – string list of the features names to drop\nexample:\nfeature_set = fs.FeatureSet(\"fs-new\",\nentities=[fs.Entity(\"id\")],\ndescription=\"feature set\",\nengine=\"pandas\",\n)\n# Pre-processing grpah steps\nfeature_set.graph.to(DropFeatures(features=[\"age\"]))\ndf_pandas = fs.ingest(feature_set, data)\nclass mlrun.feature_store.steps.FeaturesetValidator(featureset=None, columns=None, name=None, **kwargs)[source]#\nValidate feature values according to the feature set validation policy\nValidate feature values according to the feature set validation policy\nParameters\nfeatureset – feature set uri (or “.” for current feature set pipeline)\ncolumns – names of the columns/fields to validate\nname – step name\nkwargs – optional kwargs (for storey)\n__init__(featureset=None, columns=None, name=None, **kwargs)[source]#\nValidate feature values according to the feature set validation policy\nParameters\nfeatureset – feature set uri (or “.” for current feature set pipeline)\ncolumns – names of the columns/fields to validate\nname – step name\nkwargs – optional kwargs (for storey)\nclass mlrun.feature_store.steps.Imputer(method: str = 'avg', default_value=None, mapping: Optional[Dict[str, Any]] = None, **kwargs)[source]#\nReplace None values with default values\nParameters\nmethod – for future use\ndefault_value – default value if not specified per column\nmapping – a dict of per column default value\nkwargs – optional kwargs (for storey)\n__init__(method: str = 'avg', default_value=None, mapping: Optional[Dict[str, Any]] = None, **kwargs)[source]#\nReplace None values with default values\nParameters\nmethod – for future use\ndefault_value – default value if not specified per column\nmapping – a dict of per column default value\nkwargs – optional kwargs (for storey)\nclass mlrun.feature_store.steps.MLRunStep(**kwargs)[source]#\nAbstract class for mlrun step.\nCan be used in pandas/storey feature set ingestion\n__init__(**kwargs)[source]#\nAbstract class for mlrun step.\nCan be used in pandas/storey feature set ingestion\ndo(event)[source]#\nThis method defines the do method of this class according to the first event type.\nclass mlrun.feature_store.steps.MapValues(mapping: Dict[str, Dict[str, Any]], with_original_features: bool = False, suffix: str = 'mapped', **kwargs)[source]#\nMap column values to new values\nMap column values to new values\nexample:\n# replace the value \"U\" with '0' in the age column\ngraph.to(MapValues(mapping={'age': {'U': '0'}}, with_original_features=True))\n# replace integers, example\ngraph.to(MapValues(mapping={'not': {0: 1, 1: 0}}))\n# replace by range, use -inf and inf for extended range\ngraph.to(MapValues(mapping={'numbers': {'ranges': {'negative': [-inf, 0], 'positive': [0, inf]}}}))\nParameters\nmapping – a dict with entry per column and the associated old/new values map\nwith_original_features – set to True to keep the original features\nsuffix – the suffix added to the column name <column>_<suffix> (default is “mapped”)\nkwargs – optional kwargs (for storey)\n__init__(mapping: Dict[str, Dict[str, Any]], with_original_features: bool = False, suffix: str = 'mapped', **kwargs)[source]#\nMap column values to new values\nexample:\n# replace the value \"U\" with '0' in the age column\ngraph.to(MapValues(mapping={'age': {'U': '0'}}, with_original_features=True))\n# replace integers, example\ngraph.to(MapValues(mapping={'not': {0: 1, 1: 0}}))\n# replace by range, use -inf and inf for extended range\ngraph.to(MapValues(mapping={'numbers': {'ranges': {'negative': [-inf, 0], 'positive': [0, inf]}}}))\nParameters\nmapping – a dict with entry per column and the associated old/new values map\nwith_original_features – set to True to keep the original features\nsuffix – the suffix added to the column name <column>_<suffix> (default is “mapped”)\nkwargs – optional kwargs (for storey)\nclass mlrun.feature_store.steps.OneHotEncoder(mapping: Dict[str, List[Union[int, str]]], **kwargs)[source]#\nCreate new binary fields, one per category (one hot encoded)\nexample:\nmapping = {'category': ['food', 'health', 'transportation'],\n'gender': ['male', 'female']}\ngraph.to(OneHotEncoder(mapping=one_hot_encoder_mapping))\nParameters\nmapping – a dict of per column categories (to map to binary fields)\nkwargs – optional kwargs (for storey)\n__init__(mapping: Dict[str, List[Union[int, str]]], **kwargs)[source]#\nCreate new binary fields, one per category (one hot encoded)\nexample:\nmapping = {'category': ['food', 'health', 'transportation'],\n'gender': ['male', 'female']}\ngraph.to(OneHotEncoder(mapping=one_hot_encoder_mapping))\nParameters\nmapping – a dict of per column categories (to map to binary fields)\nkwargs – optional kwargs (for storey)\nclass mlrun.feature_store.steps.SetEventMetadata(id_path: Optional[str] = None, key_path: Optional[str] = None, time_path: Optional[str] = None, random_id: Optional[bool] = None, **kwargs)[source]#\nSet the event metadata (id, key, timestamp) from the event body\nSet the event metadata (id, key, timestamp) from the event body\nset the event metadata fields (id, key, and time) from the event body data structure\nthe xx_path attribute defines the key or path to the value in the body dict, “.” in the path string\nindicate the value is in a nested dict e.g. “x.y” means {“x”: {“y”: value}}\nexample:\nflow = function.set_topology(\"flow\")\n# build a graph and use the SetEventMetadata step to extract the id, key and path from the event body\n# (\"myid\", \"mykey\" and \"mytime\" fields), the metadata will be used for following data processing steps\n# (e.g. feature store ops, time/key aggregations, write to databases/streams, etc.)\nflow.to(SetEventMetadata(id_path=\"myid\", key_path=\"mykey\", time_path=\"mytime\"))\n.to(...)  # additional steps\nserver = function.to_mock_server()\nevent = {\"myid\": \"34\", \"mykey\": \"123\", \"mytime\": \"2022-01-18 15:01\"}\nresp = server.test(body=event)\nParameters\nid_path – path to the id value\nkey_path – path to the key value\ntime_path – path to the time value (value should be of type str or datetime)\nrandom_id – if True will set the event.id to a random value\n__init__(id_path: Optional[str] = None, key_path: Optional[str] = None, time_path: Optional[str] = None, random_id: Optional[bool] = None, **kwargs)[source]#\nSet the event metadata (id, key, timestamp) from the event body\nset the event metadata fields (id, key, and time) from the event body data structure\nthe xx_path attribute defines the key or path to the value in the body dict, “.” in the path string\nindicate the value is in a nested dict e.g. “x.y” means {“x”: {“y”: value}}\nexample:\nflow = function.set_topology(\"flow\")\n# build a graph and use the SetEventMetadata step to extract the id, key and path from the event body\n# (\"myid\", \"mykey\" and \"mytime\" fields), the metadata will be used for following data processing steps\n# (e.g. feature store ops, time/key aggregations, write to databases/streams, etc.)\nflow.to(SetEventMetadata(id_path=\"myid\", key_path=\"mykey\", time_path=\"mytime\"))\n.to(...)  # additional steps\nserver = function.to_mock_server()\nevent = {\"myid\": \"34\", \"mykey\": \"123\", \"mytime\": \"2022-01-18 15:01\"}\nresp = server.test(body=event)\nParameters\nid_path – path to the id value\nkey_path – path to the key value\ntime_path – path to the time value (value should be of type str or datetime)\nrandom_id – if True will set the event.id to a random value"}
{"text": "mlrun.frameworks#\nMLRun provides a quick and easy integration into your code with mlrun.frameworks: a collection of sub-modules for the most commonly used machine and deep learning frameworks, providing features such as automatic logging, model management, and distributed training.\nAutoMLRun\nTensorFlow.Keras\nPyTorch\nSciKit-Learn\nXGBoost\nLightGBM"}
{"text": "mlrun.frameworks.auto_mlrun#\nclass mlrun.frameworks.auto_mlrun.auto_mlrun.AutoMLRun[source]#\nBases: object\nA library of automatic functions for managing models using MLRun’s frameworks package.\nstatic apply_mlrun(model: Optional[mlrun.frameworks._common.utils.ModelType] = None, model_name: Optional[str] = None, tag: str = '', model_path: Optional[str] = None, modules_map: Optional[Union[Dict[str, Union[None, str, List[str]]], str]] = None, custom_objects_map: Optional[Union[Dict[str, Union[str, List[str]]], str]] = None, custom_objects_directory: Optional[str] = None, context: Optional[mlrun.execution.MLClientCtx] = None, framework: Optional[str] = None, auto_log: bool = True, **kwargs) → mlrun.frameworks._common.model_handler.ModelHandler[source]#\nUse MLRun’s ‘apply_mlrun’ of the detected given model’s framework to wrap the framework relevant methods and\ngain the framework’s features in MLRun. A ModelHandler initialized with the model will be returned.\nParameters\nmodel – The model to wrap. Can be loaded from the model path given as well.\nmodel_name – The model name to use for storing the model artifact. If not given will have a\ndefault name according to the framework.\ntag – The model’s tag to log with.\nmodel_path – The model’s store object path. Mandatory for evaluation (to know which model to\nupdate). If model is not provided, it will be loaded from this path.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a\npath to a module and its value is the object name to import from it. All the\nmodules will be imported globally. If multiple objects needed to be imported\nfrom the same module a list can be given. The map can be passed as a path to a\njson file as well. For example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key\nis a path to a python file and its value is the custom object name to import\nfrom it. If multiple objects needed to be imported from the same py file a list\ncan be given. The map can be passed as a path to a json file as well. For\nexample:\n{\n\"/.../custom_model.py\": \"MyModel\",\n\"/.../custom_objects.py\": [\"object1\", \"object2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’,\nmeaning each py file will be read from ‘custom_objects_directory/<MAP VALUE>’.\nIf the model path given is of a store object, the custom objects map will be\nread from the logged custom object map artifact of the model.\nNotice: The custom objects will be imported in the order they came in this\ndictionary (or json). If a custom object is depended on another, make sure to\nput it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom\nobjects. Can be passed as a zip file as well (will be extracted during the run\nbefore loading the model). If the model path given is of a store object, the\ncustom objects files will be read from the logged custom object artifact of the\nmodel.\ncontext – A MLRun context.\nauto_log – Whether to enable auto-logging capabilities of MLRun or not. Auto logging will\nadd default artifacts and metrics besides the one you can pass here.\nframework – The model’s framework. If None, AutoMLRun will try to figure out the framework.\nFrom the provided model or model path. Default: None.\nkwargs – Additional parameters for the specific framework’s ‘apply_mlrun’ function like\nmetrics, callbacks and more (read the docs of the required framework to know\nmore).\nReturns\nThe framework’s model handler initialized with the given model.\nstatic load_model(model_path: str, model_name: Optional[str] = None, context: Optional[mlrun.execution.MLClientCtx] = None, modules_map: Optional[Union[Dict[str, Union[None, str, List[str]]], str]] = None, custom_objects_map: Optional[Union[Dict[str, Union[str, List[str]]], str]] = None, custom_objects_directory: Optional[str] = None, framework: Optional[str] = None, **kwargs) → mlrun.frameworks._common.model_handler.ModelHandler[source]#\nLoad a model using MLRun’s ModelHandler. The loaded model can be accessed from the model handler returned\nvia model_handler.model. If the model is a store object uri (it is logged in MLRun) then the framework will be\nread automatically, otherwise (for local path and urls) it must be given. The other parameters will be\nautomatically read in case its a logged model in MLRun.\nParameters\nmodel_path – A store object path of a logged model object in MLRun.\nmodel_name – The model name to use for storing the model artifact. If not given will have a\ndefault name according to the framework.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a\npath to a module and its value is the object name to import from it. All the\nmodules will be imported globally. If multiple objects needed to be imported\nfrom the same module a list can be given. The map can be passed as a path to a\njson file as well. For example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key\nis a path to a python file and its value is the custom object name to import\nfrom it. If multiple objects needed to be imported from the same py file a list\ncan be given. The map can be passed as a path to a json file as well. For\nexample:\n{\n\"/.../custom_model.py\": \"MyModel\",\n\"/.../custom_objects.py\": [\"object1\", \"object2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’,\nmeaning each py file will be read from ‘custom_objects_directory/<MAP VALUE>’.\nIf the model path given is of a store object, the custom objects map will be\nread from the logged custom object map artifact of the model.\nNotice: The custom objects will be imported in the order they came in this\ndictionary (or json). If a custom object is depended on another, make sure to\nput it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom\nobjects. Can be passed as a zip file as well (will be extracted during the run\nbefore loading the model). If the model path given is of a store object, the\ncustom objects files will be read from the logged custom object artifact of the\nmodel.\ncontext – A MLRun context.\nframework – The model’s framework. It must be provided for local paths or urls. If None,\nAutoMLRun will assume the model path is of a store uri model artifact and try\nto get the framework from it. Default: None.\nkwargs – Additional parameters for the specific framework’s ModelHandler class.\nReturns\nThe model inside a MLRun model handler.\nRaises\nMLRunInvalidArgumentError – In case the framework is incorrect or missing.\nmlrun.frameworks.auto_mlrun.auto_mlrun.framework_to_apply_mlrun(framework: str) → Callable[[...], mlrun.frameworks._common.model_handler.ModelHandler][source]#\nGet the ‘apply_mlrun’ shortcut function of the given framework’s name.\nParameters\nframework – The framework’s name.\nReturns\nThe framework’s ‘apply_mlrun’ shortcut function.\nRaises\nMLRunInvalidArgumentError – If the given framework is not supported by AutoMLRun or if it does not have an\n‘apply_mlrun’ yet.\nmlrun.frameworks.auto_mlrun.auto_mlrun.framework_to_model_handler(framework: str) → Type[mlrun.frameworks._common.model_handler.ModelHandler][source]#\nGet the ModelHandler class of the given framework’s name.\nParameters\nframework – The framework’s name.\nReturns\nThe framework’s ModelHandler class.\nRaises\nMLRunInvalidArgumentError – If the given framework is not supported by AutoMLRun.\nmlrun.frameworks.auto_mlrun.auto_mlrun.get_framework_by_class_name(model: mlrun.frameworks._common.utils.ModelType) → str[source]#\nGet the framework name of the given model by its class name.\nParameters\nmodel – The model to get its framework.\nReturns\nThe model’s framework.\nRaises\nMLRunInvalidArgumentError – If the given model’s class name is not supported by AutoMLRun or not recognized.\nmlrun.frameworks.auto_mlrun.auto_mlrun.get_framework_by_instance(model: mlrun.frameworks._common.utils.ModelType) → str[source]#\nGet the framework name of the given model by its instance.\nParameters\nmodel – The model to get his framework.\nReturns\nThe model’s framework.\nRaises\nMLRunInvalidArgumentError – If the given model type is not supported by AutoMLRun or not recognized."}
{"text": "mlrun.frameworks.lgbm#\nmlrun.frameworks.lgbm.apply_mlrun(model: Union[lightgbm.LGBMModel, lightgbm.Booster] = None, model_name: str = 'model', tag: str = '', model_path: str = None, modules_map: Union[Dict[str, Union[None, str, List[str]]], str] = None, custom_objects_map: Union[Dict[str, Union[str, List[str]]], str] = None, custom_objects_directory: str = None, context: mlrun.execution.MLClientCtx = None, model_format: str = 'pkl', artifacts: Union[List[mlrun.frameworks._ml_common.plan.MLPlan], List[str], Dict[str, dict]] = None, metrics: Union[List[mlrun.frameworks.sklearn.metric.Metric], List[Union[Tuple[Union[Callable, str], dict], Callable, str]], Dict[str, Union[Tuple[Union[Callable, str], dict], Callable, str]]] = None, x_test: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix, lightgbm.Dataset] = None, y_test: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix, lightgbm.Dataset] = None, sample_set: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix, lightgbm.Dataset, mlrun.datastore.base.DataItem, str] = None, y_columns: Union[List[str], List[int]] = None, feature_vector: str = None, feature_weights: List[float] = None, labels: Dict[str, Union[str, int, float]] = None, parameters: Dict[str, Union[str, int, float]] = None, extra_data: Dict[str, Union[str, bytes, mlrun.artifacts.base.Artifact, mlrun.datastore.base.DataItem]] = None, auto_log: bool = True, mlrun_logging_callback_kwargs: Dict[str, Any] = None, **kwargs) → Optional[mlrun.frameworks.lgbm.model_handler.LGBMModelHandler][source]#\nApply MLRun’s interface on top of LightGBM by wrapping the module itself or the given model, providing both with\nMLRun’s quality of life features.\nParameters\nmodel – The model to wrap. Can be loaded from the model path given as well.\nmodel_name – The model name to use for storing the model artifact. Default: “model”.\ntag – The model’s tag to log with.\nmodel_path – The model’s store object path. Mandatory for evaluation (to know which model to\nupdate). If model is not provided, it will be loaded from this path.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a\npath to a module and its value is the object name to import from it. All the\nmodules will be imported globally. If multiple objects needed to be imported\nfrom the same module a list can be given. The map can be passed as a path to a\njson file as well. For example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key is\na path to a python file and its value is the custom object name to import from it.\nIf multiple objects needed to be imported from the same py file a list can be\ngiven. The map can be passed as a path to a json file as well. For example:\n{\n\"/.../custom_model.py\": \"MyModel\",\n\"/.../custom_objects.py\": [\"object1\", \"object2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’, meaning\neach py file will be read from ‘custom_objects_directory/<MAP VALUE>’. If the model\npath given is of a store object, the custom objects map will be read from the\nlogged custom object map artifact of the model.\nNotice: The custom objects will be imported in the order they came in this\ndictionary (or json). If a custom object is depended on another, make sure to\nput it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom objects.\nCan be passed as a zip file as well (will be extracted during the run before\nloading the model). If the model path given is of a store object, the custom\nobjects files will be read from the logged custom object artifact of the model.\ncontext – MLRun context to work with. If no context is given it will be retrieved via\n‘mlrun.get_or_create_ctx(None)’\nartifacts – A list of artifacts plans to produce during the run.\nmetrics – A list of metrics to calculate during the run.\nx_test – The validation data for producing and calculating artifacts and metrics post\ntraining. Without this, validation will not be performed.\ny_test – The test data ground truth for producing and calculating artifacts and metrics post\ntraining or post predict / predict_proba.\nsample_set – A sample set of inputs for the model for logging its stats along the model in\nfavour of model monitoring.\ny_columns – List of names of all the columns in the ground truth labels in case its a\npd.DataFrame or a list of integers in case the dataset is a np.ndarray. If not\ngiven but ‘y_train’ / ‘y_test’ is given then the labels / indices in it will be\nused by default.\nfeature_vector – Feature store feature vector uri (store://feature-vectors/<project>/<name>[:tag])\nfeature_weights – List of feature weights, one per input column.\nlabels – Labels to log with the model.\nparameters – Parameters to log with the model.\nextra_data – Extra data to log with the model.\nauto_log – Whether to apply MLRun’s auto logging on the model. Auto logging will add the\ndefault artifacts and metrics to the lists of artifacts and metrics. Default:\nTrue.\nmlrun_logging_callback_kwargs – Key word arguments for the MLRun callback. For further information see the\ndocumentation of the class ‘MLRunLoggingCallback’. Note that ‘context’ is already\ngiven here.\nReturns\nIf a model was provided via model or model_path the model handler initialized with the provided model\nwill be returned. Otherwise, None."}
{"text": "mlrun.frameworks.pytorch#\nmlrun.frameworks.pytorch.evaluate(model_path: str, dataset: torch.utils.data.DataLoader, model: Optional[torch.nn.Module] = None, loss_function: Optional[torch.nn.Module] = None, metric_functions: Optional[List[Union[Callable[[torch.Tensor, torch.Tensor], Union[int, float, numpy.ndarray, torch.Tensor]], torch.nn.Module]]] = None, iterations: Optional[int] = None, callbacks_list: Optional[List[mlrun.frameworks.pytorch.callbacks.callback.Callback]] = None, use_cuda: bool = True, use_horovod: bool = False, auto_log: bool = True, model_name: Optional[str] = None, modules_map: Optional[Union[Dict[str, Union[None, str, List[str]]], str]] = None, custom_objects_map: Optional[Union[Dict[str, Union[str, List[str]]], str]] = None, custom_objects_directory: Optional[str] = None, mlrun_callback_kwargs: Optional[Dict[str, Any]] = None, context: Optional[mlrun.execution.MLClientCtx] = None) → Tuple[mlrun.frameworks.pytorch.model_handler.PyTorchModelHandler, List[Union[int, float, numpy.ndarray, torch.Tensor]]][source]#\nUse MLRun’s PyTorch interface to evaluate the model with the given parameters. For more information and further\noptions regarding the auto logging, see ‘PyTorchMLRunInterface’ documentation. Notice for auto-logging: In order to\nlog the model to MLRun, its class (torch.Module) must be in the custom objects map or the modules map.\nParameters\nmodel_path – The model’s store object path. Mandatory for evaluation (to know which model to\nupdate).\ndataset – A data loader for the validation process.\nmodel – The model to evaluate. IF None, the model will be loaded from the given store model\npath.\nloss_function – The loss function to use during training.\nmetric_functions – The metrics to use on training and validation.\niterations – Amount of iterations (batches) to perform on the dataset. If ‘None’ the entire\ndataset will be used.\ncallbacks_list – The callbacks to use on this run.\nuse_cuda – Whether or not to use cuda. Only relevant if cuda is available. Default: True.\nuse_horovod – Whether or not to use horovod - a distributed training framework. Default:\nFalse.\nauto_log – Whether or not to apply auto-logging to MLRun. Default: True.\nmodel_name – The model name to use for storing the model artifact. If not given, the model’s\nclass name will be used.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a path\nto a module and its value is the object name to import from it. All the modules\nwill be imported globally. If multiple objects needed to be imported from the same\nmodule a list can be given. The map can be passed as a path to a json file as well.\nFor example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key is\na path to a python file and its value is the custom object name to import from it.\nIf multiple objects needed to be imported from the same py file a list can be\ngiven. The map can be passed as a path to a json file as well. For example:\n{\n\"/.../custom_optimizer.py\": \"optimizer\",\n\"/.../custom_layers.py\": [\"layer1\", \"layer2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’, meaning\neach py file will be read from ‘custom_objects_directory/<MAP VALUE>’. If the model\npath given is of a store object, the custom objects map will be read from the\nlogged custom object map artifact of the model. Notice: The custom objects will be\nimported in the order they came in this dictionary (or json). If a custom object is\ndepended on another, make sure to put it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom objects.\nCan be passed as a zip file as well (will be extracted during the run before\nloading the model). If the model path given is of a store object, the custom\nobjects files will be read from the logged custom object artifact of the model.\nmlrun_callback_kwargs – Key word arguments for the MLRun callback. For further information see the\ndocumentation of the class ‘MLRunLoggingCallback’. Note that both ‘context’,\n‘custom_objects’ and ‘auto_log’ parameters are already given here.\ncontext – The context to use for the logs.\nReturns\nA tuple of:\n[0] = Initialized model handler with the evaluated model.\n[1] = The evaluation metrics results list.\nmlrun.frameworks.pytorch.train(model: torch.nn.Module, training_set: torch.utils.data.DataLoader, loss_function: torch.nn.Module, optimizer: torch.optim.Optimizer, validation_set: Optional[torch.utils.data.DataLoader] = None, metric_functions: Optional[List[Union[Callable[[torch.Tensor, torch.Tensor], Union[int, float, numpy.ndarray, torch.Tensor]], torch.nn.Module]]] = None, scheduler=None, scheduler_step_frequency: Union[int, float, str] = 'epoch', epochs: int = 1, training_iterations: Optional[int] = None, validation_iterations: Optional[int] = None, callbacks_list: Optional[List[mlrun.frameworks.pytorch.callbacks.callback.Callback]] = None, use_cuda: bool = True, use_horovod: Optional[bool] = None, auto_log: bool = True, model_name: Optional[str] = None, modules_map: Optional[Union[Dict[str, Union[None, str, List[str]]], str]] = None, custom_objects_map: Optional[Union[Dict[str, Union[str, List[str]]], str]] = None, custom_objects_directory: Optional[str] = None, tensorboard_directory: Optional[str] = None, mlrun_callback_kwargs: Optional[Dict[str, Any]] = None, tensorboard_callback_kwargs: Optional[Dict[str, Any]] = None, context: Optional[mlrun.execution.MLClientCtx] = None) → mlrun.frameworks.pytorch.model_handler.PyTorchModelHandler[source]#\nUse MLRun’s PyTorch interface to train the model with the given parameters. For more information and further options\nregarding the auto logging, see ‘PyTorchMLRunInterface’ documentation. Notice for auto-logging: In order to log the\nmodel to MLRun, its class (torch.Module) must be in the custom objects map or the modules map.\nParameters\nmodel – The model to train.\ntraining_set – A data loader for the training process.\nloss_function – The loss function to use during training.\noptimizer – The optimizer to use during the training.\nvalidation_set – A data loader for the validation process.\nmetric_functions – The metrics to use on training and validation.\nscheduler – Scheduler to use on the optimizer at the end of each epoch. The scheduler must\nhave a ‘step’ method with no input.\nscheduler_step_frequency – The frequency in which to step the given scheduler. Can be equal to one of the\nstrings ‘epoch’ (for at the end of every epoch) and ‘batch’ (for at the end of\nevery batch), or an integer that specify per how many iterations to step or a\nfloat percentage (0.0 < x < 1.0) for per x / iterations to step. Default:\n‘epoch’.\nepochs – Amount of epochs to perform. Default: a single epoch.\ntraining_iterations – Amount of iterations (batches) to perform on each epoch’s training. If ‘None’\nthe entire training set will be used.\nvalidation_iterations – Amount of iterations (batches) to perform on each epoch’s validation. If ‘None’\nthe entire validation set will be used.\ncallbacks_list – The callbacks to use on this run.\nuse_cuda – Whether or not to use cuda. Only relevant if cuda is available. Default:\nTrue.\nuse_horovod – Whether or not to use horovod - a distributed training framework. Default:\nFalse.\nauto_log – Whether or not to apply auto-logging (to both MLRun and Tensorboard). Default:\nTrue. IF True, the custom objects are not optional.\nmodel_name – The model name to use for storing the model artifact. If not given, the model’s\nclass name will be used.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a\npath to a module and its value is the object name to import from it. All the\nmodules will be imported globally. If multiple objects needed to be imported\nfrom the same module a list can be given. The map can be passed as a path to a\njson file as well. For example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key\nis a path to a python file and its value is the custom object name to import\nfrom it. If multiple objects needed to be imported from the same py file a list\ncan be given. The map can be passed as a path to a json file as well. For\nexample:\n{\n\"/.../custom_optimizer.py\": \"optimizer\",\n\"/.../custom_layers.py\": [\"layer1\", \"layer2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’,\nmeaning each py file will be read from ‘custom_objects_directory/<MAP VALUE>’.\nIf the model path given is of a store object, the custom objects map will be\nread from the logged custom object map artifact of the model.\nNotice: The custom objects will be imported in the order they came in this\ndictionary (or json). If a custom object is depended on another, make sure to\nput it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom objects.\nCan be passed as a zip file as well (will be extracted during the run before\nloading the model). If the model path given is of a store object, the custom\nobjects files will be read from the logged custom object artifact of the model.\ntensorboard_directory – If context is not given, or if wished to set the directory even with context,\nthis will be the output for the event logs of tensorboard. If not given, the\n‘tensorboard_dir’ parameter will be tried to be taken from the provided context.\nIf not found in the context, the default tensorboard output directory will be:\n/User/.tensorboard/<PROJECT_NAME> or if working on local, the set artifacts\npath.\nmlrun_callback_kwargs – Key word arguments for the MLRun callback. For further information see the\ndocumentation of the class ‘MLRunLoggingCallback’. Note that both ‘context’,\n‘custom_objects’ and ‘auto_log’ parameters are already given here.\ntensorboard_callback_kwargs – Key word arguments for the tensorboard callback. For further information see\nthe documentation of the class ‘TensorboardLoggingCallback’. Note that both\n‘context’ and ‘auto_log’ parameters are already given here.\ncontext – The context to use for the logs.\nReturns\nA model handler with the provided model and parameters.\nRaises\nValueError – If ‘auto_log’ is set to True and one all of the custom objects or modules parameters given is\nNone."}
{"text": "mlrun.frameworks.sklearn#\nmlrun.frameworks.sklearn.apply_mlrun(model: Union[sklearn.base.BaseEstimator, sklearn.base.BiclusterMixin, sklearn.base.ClassifierMixin, sklearn.base.ClusterMixin, sklearn.base.DensityMixin, sklearn.base.RegressorMixin, sklearn.base.TransformerMixin] = None, model_name: str = 'model', tag: str = '', model_path: str = None, modules_map: Union[Dict[str, Union[None, str, List[str]]], str] = None, custom_objects_map: Union[Dict[str, Union[str, List[str]]], str] = None, custom_objects_directory: str = None, context: mlrun.execution.MLClientCtx = None, artifacts: Union[List[mlrun.frameworks._ml_common.plan.MLPlan], List[str], Dict[str, dict]] = None, metrics: Union[List[mlrun.frameworks.sklearn.metric.Metric], List[Union[Tuple[Union[Callable, str], dict], Callable, str]], Dict[str, Union[Tuple[Union[Callable, str], dict], Callable, str]]] = None, x_test: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix] = None, y_test: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix] = None, sample_set: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix, mlrun.datastore.base.DataItem, str] = None, y_columns: Union[List[str], List[int]] = None, feature_vector: str = None, feature_weights: List[float] = None, labels: Dict[str, Union[str, int, float]] = None, parameters: Dict[str, Union[str, int, float]] = None, extra_data: Dict[str, Union[str, bytes, mlrun.artifacts.base.Artifact, mlrun.datastore.base.DataItem]] = None, auto_log: bool = True, **kwargs) → mlrun.frameworks.sklearn.model_handler.SKLearnModelHandler[source]#\nWrap the given model with MLRun’s interface providing it with mlrun’s additional features.\nParameters\nmodel – The model to wrap. Can be loaded from the model path given as well.\nmodel_name – The model name to use for storing the model artifact. Default: “model”.\ntag – The model’s tag to log with.\nmodel_path – The model’s store object path. Mandatory for evaluation (to know which model to\nupdate). If model is not provided, it will be loaded from this path.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a\npath to a module and its value is the object name to import from it. All the\nmodules will be imported globally. If multiple objects needed to be imported\nfrom the same module a list can be given. The map can be passed as a path to a\njson file as well. For example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key is\na path to a python file and its value is the custom object name to import from it.\nIf multiple objects needed to be imported from the same py file a list can be\ngiven. The map can be passed as a path to a json file as well. For example:\n{\n\"/.../custom_model.py\": \"MyModel\",\n\"/.../custom_objects.py\": [\"object1\", \"object2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’, meaning\neach py file will be read from ‘custom_objects_directory/<MAP VALUE>’. If the model\npath given is of a store object, the custom objects map will be read from the\nlogged custom object map artifact of the model.\nNotice: The custom objects will be imported in the order they came in this\ndictionary (or json). If a custom object is depended on another, make sure to\nput it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom objects.\nCan be passed as a zip file as well (will be extracted during the run before\nloading the model). If the model path given is of a store object, the custom\nobjects files will be read from the logged custom object artifact of the model.\ncontext – MLRun context to work with. If no context is given it will be retrieved via\n‘mlrun.get_or_create_ctx(None)’\nartifacts – A list of artifacts plans to produce during the run.\nmetrics – A list of metrics to calculate during the run.\nx_test – The validation data for producing and calculating artifacts and metrics post\ntraining. Without this, validation will not be performed.\ny_test – The test data ground truth for producing and calculating artifacts and metrics post\ntraining or post predict / predict_proba.\nsample_set – A sample set of inputs for the model for logging its stats along the model in\nfavour of model monitoring. If not given the ‘x_train’ will be used by default.\ny_columns – List of names of all the columns in the ground truth labels in case its a\npd.DataFrame or a list of integers in case the dataset is a np.ndarray. If not\ngiven ‘y_train’ is given then the labels / indices in it will be used by default.\nfeature_vector – Feature store feature vector uri (store://feature-vectors/<project>/<name>[:tag])\nfeature_weights – List of feature weights, one per input column.\nlabels – Labels to log with the model.\nparameters – Parameters to log with the model.\nextra_data – Extra data to log with the model.\nauto_log – Whether to apply MLRun’s auto logging on the model. Auto logging will add the\ndefault artifacts and metrics to the lists of artifacts and metrics. Default:\nTrue.\nReturns\nThe model handler initialized with the provided model."}
{"text": "mlrun.frameworks.tf_keras#\nmlrun.frameworks.tf_keras.apply_mlrun(model: Optional[tensorflow.keras.Model] = None, model_name: Optional[str] = None, tag: str = '', model_path: Optional[str] = None, model_format: str = 'SavedModel', save_traces: bool = False, modules_map: Optional[Union[Dict[str, Union[None, str, List[str]]], str]] = None, custom_objects_map: Optional[Union[Dict[str, Union[str, List[str]]], str]] = None, custom_objects_directory: Optional[str] = None, context: Optional[mlrun.execution.MLClientCtx] = None, auto_log: bool = True, tensorboard_directory: Optional[str] = None, mlrun_callback_kwargs: Optional[Dict[str, Any]] = None, tensorboard_callback_kwargs: Optional[Dict[str, Any]] = None, use_horovod: Optional[bool] = None, **kwargs) → mlrun.frameworks.tf_keras.model_handler.TFKerasModelHandler[source]#\nWrap the given model with MLRun’s interface providing it with mlrun’s additional features.\nParameters\nmodel – The model to wrap. Can be loaded from the model path given as well.\nmodel_name – The model name to use for storing the model artifact. If not given, the\ntf.keras.Model.name will be used.\ntag – The model’s tag to log with.\nmodel_path – The model’s store object path. Mandatory for evaluation (to know which model to\nupdate). If model is not provided, it will be loaded from this path.\nmodel_format – The format to use for saving and loading the model. Should be passed as a\nmember of the class ‘ModelFormats’. Default: ‘ModelFormats.SAVED_MODEL’.\nsave_traces – Whether or not to use functions saving (only available for the ‘SavedModel’\nformat) for loading the model later without the custom objects dictionary. Only\nfrom tensorflow version >= 2.4.0. Using this setting will increase the model\nsaving size.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a\npath to a module and its value is the object name to import from it. All the\nmodules will be imported globally. If multiple objects needed to be imported\nfrom the same module a list can be given. The map can be passed as a path to a\njson file as well. For example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key\nis a path to a python file and its value is the custom object name to import\nfrom it. If multiple objects needed to be imported from the same py file a list\ncan be given. The map can be passed as a path to a json file as well. For\nexample:\n{\n\"/.../custom_optimizer.py\": \"optimizer\",\n\"/.../custom_layers.py\": [\"layer1\", \"layer2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’,\nmeaning each py file will be read from ‘custom_objects_directory/<MAP VALUE>’.\nIf the model path given is of a store object, the custom objects map will be\nread from the logged custom object map artifact of the model.\nNotice: The custom objects will be imported in the order they came in this\ndictionary (or json). If a custom object is depended on another, make sure to\nput it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom objects.\nCan be passed as a zip file as well (will be extracted during the run before\nloading the model). If the model path given is of a store object, the custom\nobjects files will be read from the logged custom object artifact of the model.\ncontext – MLRun context to work with. If no context is given it will be retrieved via\n‘mlrun.get_or_create_ctx(None)’\nauto_log – Whether or not to apply MLRun’s auto logging on the model. Default: True.\ntensorboard_directory – If context is not given, or if wished to set the directory even with context,\nthis will be the output for the event logs of tensorboard. If not given, the\n‘tensorboard_dir’ parameter will be tried to be taken from the provided context.\nIf not found in the context, the default tensorboard output directory will be:\n/User/.tensorboard/<PROJECT_NAME> or if working on local, the set artifacts\npath.\nmlrun_callback_kwargs – Key word arguments for the MLRun callback. For further information see the\ndocumentation of the class ‘MLRunLoggingCallback’. Note that both ‘context’\nand ‘auto_log’ parameters are already given here.\ntensorboard_callback_kwargs – Key word arguments for the tensorboard callback. For further information see\nthe documentation of the class ‘TensorboardLoggingCallback’. Note that both\n‘context’ and ‘auto_log’ parameters are already given here.\nuse_horovod – Whether or not to use horovod - a distributed training framework. Default:\nNone, meaning it will be read from context if available and if not - False.\nReturns\nThe model with MLRun’s interface."}
{"text": "mlrun.frameworks.xgboost#\nmlrun.frameworks.xgboost.apply_mlrun(model: xgboost.XGBModel = None, model_name: str = 'model', tag: str = '', model_path: str = None, modules_map: Union[Dict[str, Union[None, str, List[str]]], str] = None, custom_objects_map: Union[Dict[str, Union[str, List[str]]], str] = None, custom_objects_directory: str = None, context: mlrun.execution.MLClientCtx = None, artifacts: Union[List[mlrun.frameworks._ml_common.plan.MLPlan], List[str], Dict[str, dict]] = None, metrics: Union[List[mlrun.frameworks.sklearn.metric.Metric], List[Union[Tuple[Union[Callable, str], dict], Callable, str]], Dict[str, Union[Tuple[Union[Callable, str], dict], Callable, str]]] = None, x_test: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix, xgboost.DMatrix] = None, y_test: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix, xgboost.DMatrix] = None, sample_set: Union[list, tuple, dict, numpy.ndarray, pandas.core.frame.DataFrame, pandas.core.series.Series, scipy.sparse.base.spmatrix, xgboost.DMatrix, mlrun.datastore.base.DataItem, str] = None, y_columns: Union[List[str], List[int]] = None, feature_vector: str = None, feature_weights: List[float] = None, labels: Dict[str, Union[str, int, float]] = None, parameters: Dict[str, Union[str, int, float]] = None, extra_data: Dict[str, Union[str, bytes, mlrun.artifacts.base.Artifact, mlrun.datastore.base.DataItem]] = None, auto_log: bool = True, **kwargs) → mlrun.frameworks.xgboost.model_handler.XGBoostModelHandler[source]#\nWrap the given model with MLRun’s interface providing it with mlrun’s additional features.\nParameters\nmodel – The model to wrap. Can be loaded from the model path given as well.\nmodel_name – The model name to use for storing the model artifact. Default: “model”.\ntag – The model’s tag to log with.\nmodel_path – The model’s store object path. Mandatory for evaluation (to know which model to\nupdate). If model is not provided, it will be loaded from this path.\nmodules_map – A dictionary of all the modules required for loading the model. Each key is a\npath to a module and its value is the object name to import from it. All the\nmodules will be imported globally. If multiple objects needed to be imported\nfrom the same module a list can be given. The map can be passed as a path to a\njson file as well. For example:\n{\n\"module1\": None,  # import module1\n\"module2\": [\"func1\", \"func2\"],  # from module2 import func1, func2\n\"module3.sub_module\": \"func3\",  # from module3.sub_module import func3\n}\nIf the model path given is of a store object, the modules map will be read from\nthe logged modules map artifact of the model.\ncustom_objects_map – A dictionary of all the custom objects required for loading the model. Each key is\na path to a python file and its value is the custom object name to import from it.\nIf multiple objects needed to be imported from the same py file a list can be\ngiven. The map can be passed as a path to a json file as well. For example:\n{\n\"/.../custom_model.py\": \"MyModel\",\n\"/.../custom_objects.py\": [\"object1\", \"object2\"]\n}\nAll the paths will be accessed from the given ‘custom_objects_directory’, meaning\neach py file will be read from ‘custom_objects_directory/<MAP VALUE>’. If the model\npath given is of a store object, the custom objects map will be read from the\nlogged custom object map artifact of the model.\nNotice: The custom objects will be imported in the order they came in this\ndictionary (or json). If a custom object is depended on another, make sure to\nput it below the one it relies on.\ncustom_objects_directory – Path to the directory with all the python files required for the custom objects.\nCan be passed as a zip file as well (will be extracted during the run before\nloading the model). If the model path given is of a store object, the custom\nobjects files will be read from the logged custom object artifact of the model.\ncontext – MLRun context to work with. If no context is given it will be retrieved via\n‘mlrun.get_or_create_ctx(None)’\nartifacts – A list of artifacts plans to produce during the run.\nmetrics – A list of metrics to calculate during the run.\nx_test – The validation data for producing and calculating artifacts and metrics post\ntraining. Without this, validation will not be performed.\ny_test – The test data ground truth for producing and calculating artifacts and metrics post\ntraining or post predict / predict_proba.\nsample_set – A sample set of inputs for the model for logging its stats along the model in\nfavour of model monitoring.\ny_columns – List of names of all the columns in the ground truth labels in case its a\npd.DataFrame or a list of integers in case the dataset is a np.ndarray. If not\ngiven but ‘y_train’ / ‘y_test’ is given then the labels / indices in it will be\nused by default.\nfeature_vector – Feature store feature vector uri (store://feature-vectors/<project>/<name>[:tag])\nfeature_weights – List of feature weights, one per input column.\nlabels – Labels to log with the model.\nparameters – Parameters to log with the model.\nextra_data – Extra data to log with the model.\nauto_log – Whether to apply MLRun’s auto logging on the model. Auto logging will add the\ndefault artifacts and metrics to the lists of artifacts and metrics. Default:\nTrue.\nReturns\nThe model handler initialized with the provided model."}
{"text": "mlrun.model#\nclass mlrun.model.DataSource(name: Optional[str] = None, path: Optional[str] = None, attributes: Optional[Dict[str, str]] = None, key_field: Optional[str] = None, time_field: Optional[str] = None, schedule: Optional[str] = None, start_time: Optional[Union[datetime.datetime, str]] = None, end_time: Optional[Union[datetime.datetime, str]] = None)[source]#\nBases: mlrun.model.ModelObj\nonline or offline data source spec\nclass mlrun.model.DataTarget(kind: Optional[str] = None, name: str = '', path=None, online=None)[source]#\nBases: mlrun.model.DataTargetBase\ndata target with extra status information (used in the feature-set/vector status)\nclass mlrun.model.DataTargetBase(kind: Optional[str] = None, name: str = '', path=None, attributes: Optional[Dict[str, str]] = None, after_step=None, partitioned: bool = False, key_bucketing_number: Optional[int] = None, partition_cols: Optional[List[str]] = None, time_partitioning_granularity: Optional[str] = None, max_events: Optional[int] = None, flush_after_seconds: Optional[int] = None, after_state=None, storage_options: Optional[Dict[str, str]] = None)[source]#\nBases: mlrun.model.ModelObj\ndata target spec, specify a destination for the feature set data\nclassmethod from_dict(struct=None, fields=None)[source]#\ncreate an object from a python dictionary\nclass mlrun.model.FeatureSetProducer(kind=None, name=None, uri=None, owner=None, sources=None)[source]#\nBases: mlrun.model.ModelObj\ninformation about the task/job which produced the feature set data\nclass mlrun.model.HyperParamOptions(param_file=None, strategy=None, selector: Optional[mlrun.model.HyperParamStrategies] = None, stop_condition=None, parallel_runs=None, dask_cluster_uri=None, max_iterations=None, max_errors=None, teardown_dask=None)[source]#\nBases: mlrun.model.ModelObj\nHyper Parameter Options\nParameters\nparam_file (str) – hyper params input file path/url, instead of inline\nstrategy (str) – hyper param strategy - grid, list or random\nselector (str) – selection criteria for best result ([min|max.]<result>), e.g. max.accuracy\nstop_condition (str) – early stop condition e.g. “accuracy > 0.9”\nparallel_runs (int) – number of param combinations to run in parallel (over Dask)\ndask_cluster_uri (str) – db uri for a deployed dask cluster function, e.g. db://myproject/dask\nmax_iterations (int) – max number of runs (in random strategy)\nmax_errors (int) – max number of child runs errors for the overall job to fail\nteardown_dask (bool) – kill the dask cluster pods after the runs\nmlrun.model.NewTask(name=None, project=None, handler=None, params=None, hyper_params=None, param_file=None, selector=None, strategy=None, inputs=None, outputs=None, in_path=None, out_path=None, artifact_path=None, secrets=None, base=None)[source]#\nCreates a new task - see new_task\nclass mlrun.model.RunMetadata(uid=None, name=None, project=None, labels=None, annotations=None, iteration=None)[source]#\nBases: mlrun.model.ModelObj\nRun metadata\nclass mlrun.model.RunObject(spec: Optional[mlrun.model.RunSpec] = None, metadata: Optional[mlrun.model.RunMetadata] = None, status: Optional[mlrun.model.RunStatus] = None)[source]#\nBases: mlrun.model.RunTemplate\nA run\nartifact(key) → mlrun.datastore.base.DataItem[source]#\nreturn artifact DataItem by key\nlogs(watch=True, db=None, offset=0)[source]#\nreturn or watch on the run logs\noutput(key)[source]#\nreturn the value of a specific result or artifact by key\nproperty outputs#\nreturn a dict of outputs, result values and artifact uris\nrefresh()[source]#\nrefresh run state from the db\nshow()[source]#\nshow the current status widget, in jupyter notebook\nstate()[source]#\ncurrent run state\nproperty ui_url: str#\nUI URL (for relevant runtimes)\nuid()[source]#\nrun unique id\nwait_for_completion(sleep=3, timeout=0, raise_on_failure=True, show_logs=None, logs_interval=None)[source]#\nWait for remote run to complete.\nDefault behavior is to wait until reached terminal state or timeout passed, if timeout is 0 then wait forever\nIt pulls the run status from the db every sleep seconds.\nIf show_logs is not False and logs_interval is not None, it will print the logs when run reached terminal state\nIf show_logs is not False and logs_interval is defined, it will print the logs every logs_interval seconds\nclass mlrun.model.RunSpec(parameters=None, hyperparams=None, param_file=None, selector=None, handler=None, inputs=None, outputs=None, input_path=None, output_path=None, function=None, secret_sources=None, data_stores=None, strategy=None, verbose=None, scrape_metrics=None, hyper_param_options=None, allow_empty_resources=None)[source]#\nBases: mlrun.model.ModelObj\nRun specification\nto_dict(fields=None, exclude=None)[source]#\nconvert the object to a python dictionary\nclass mlrun.model.RunStatus(state=None, error=None, host=None, commit=None, status_text=None, results=None, artifacts=None, start_time=None, last_update=None, iterations=None, ui_url=None)[source]#\nBases: mlrun.model.ModelObj\nRun status\nclass mlrun.model.RunTemplate(spec: Optional[mlrun.model.RunSpec] = None, metadata: Optional[mlrun.model.RunMetadata] = None)[source]#\nBases: mlrun.model.ModelObj\nRun template\nset_label(key, value)[source]#\nset a key/value label for the task\nwith_hyper_params(hyperparams, selector=None, strategy: Optional[mlrun.model.HyperParamStrategies] = None, **options)[source]#\nset hyper param values and configurations,\nsee parameters in: HyperParamOptions\nexample:\ngrid_params = {\"p1\": [2,4,1], \"p2\": [10,20]}\ntask = mlrun.new_task(\"grid-search\")\ntask.with_hyper_params(grid_params, selector=\"max.accuracy\")\nwith_input(key, path)[source]#\nset task data input, path is an Mlrun global DataItem uri\nexamples:\ntask.with_input(\"data\", \"/file-dir/path/to/file\")\ntask.with_input(\"data\", \"s3://<bucket>/path/to/file\")\ntask.with_input(\"data\", \"v3io://[<remote-host>]/<data-container>/path/to/file\")\nwith_param_file(param_file, selector=None, strategy: Optional[mlrun.model.HyperParamStrategies] = None, **options)[source]#\nset hyper param values (from a file url) and configurations,\nsee parameters in: HyperParamOptions\nexample:\ngrid_params = \"s3://<my-bucket>/path/to/params.json\"\ntask = mlrun.new_task(\"grid-search\")\ntask.with_param_file(grid_params, selector=\"max.accuracy\")\nwith_params(**kwargs)[source]#\nset task parameters using key=value, key2=value2, ..\nwith_secrets(kind, source)[source]#\nregister a secrets source (file, env or dict)\nread secrets from a source provider to be used in workflows, example:\ntask.with_secrets('file', 'file.txt')\ntask.with_secrets('inline', {'key': 'val'})\ntask.with_secrets('env', 'ENV1,ENV2')\ntask.with_secrets('vault', ['secret1', 'secret2'...])\n# If using with k8s secrets, the k8s secret is managed by MLRun, through the project-secrets\n# mechanism. The secrets will be attached to the running pod as environment variables.\ntask.with_secrets('kubernetes', ['secret1', 'secret2'])\n# If using an empty secrets list [] then all accessible secrets will be available.\ntask.with_secrets('vault', [])\n# To use with Azure key vault, a k8s secret must be created with the following keys:\n# kubectl -n <namespace> create secret generic azure-key-vault-secret \\\n#     --from-literal=tenant_id=<service principal tenant ID> \\\n#     --from-literal=client_id=<service principal client ID> \\\n#     --from-literal=secret=<service principal secret key>\ntask.with_secrets('azure_vault', {\n'name': 'my-vault-name',\n'k8s_secret': 'azure-key-vault-secret',\n# An empty secrets list may be passed ('secrets': []) to access all vault secrets.\n'secrets': ['secret1', 'secret2'...]\n})\nParameters\nkind – secret type (file, inline, env)\nsource – secret data or link (see example)\nReturns\nThe RunTemplate object\nclass mlrun.model.TargetPathObject(base_path=None, run_id=None, is_single_file=False)[source]#\nBases: object\nClass configuring the target path\nThis class will take consideration of a few parameters to create the correct end result path:\nrun_id\nif run_id is provided target will be considered as run_id mode which require to\ncontain a {run_id} place holder in the path.\nis_single_file\nif true then run_id must be the directory containing the output file\nor generated before the file name (run_id/output.file).\nbase_path\nif contains the place holder for run_id, run_id must not be None.\nif run_id passed and place holder doesn’t exist the place holder will\nbe generated in the correct place.\nmlrun.model.new_task(name=None, project=None, handler=None, params=None, hyper_params=None, param_file=None, selector=None, hyper_param_options=None, inputs=None, outputs=None, in_path=None, out_path=None, artifact_path=None, secrets=None, base=None) → mlrun.model.RunTemplate[source]#\nCreates a new task\nParameters\nname – task name\nproject – task project\nhandler – code entry-point/handler name\nparams – input parameters (dict)\nhyper_params – dictionary of hyper parameters and list values, each\nhyper param holds a list of values, the run will be\nexecuted for every parameter combination (GridSearch)\nparam_file – a csv file with parameter combinations, first row hold\nthe parameter names, following rows hold param values\nselector – selection criteria for hyper params e.g. “max.accuracy”\nhyper_param_options – hyper parameter options, see: HyperParamOptions\ninputs – dictionary of input objects + optional paths (if path is\nomitted the path will be the in_path/key)\noutputs – dictionary of input objects + optional paths (if path is\nomitted the path will be the out_path/key)\nin_path – default input path/url (prefix) for inputs\nout_path – default output path/url (prefix) for artifacts\nartifact_path – default artifact output path\nsecrets – extra secrets specs, will be injected into the runtime\ne.g. [‘file=<filename>’, ‘env=ENV_KEY1,ENV_KEY2’]\nbase – task instance to use as a base instead of a fresh new task instance"}
{"text": "mlrun.platforms#\nmlrun.platforms.VolumeMount#\nalias of mlrun.platforms.iguazio.Mount\nmlrun.platforms.auto_mount(pvc_name='', volume_mount_path='', volume_name=None)[source]#\nchoose the mount based on env variables and params\nvolume will be selected by the following order:\n- k8s PVC volume when both pvc_name and volume_mount_path are set\n- k8s PVC volume when env var is set: MLRUN_PVC_MOUNT=<pvc-name>:<mount-path>\n- k8s PVC volume if it’s configured as the auto mount type\n- iguazio v3io volume when V3IO_ACCESS_KEY and V3IO_USERNAME env vars are set\nmlrun.platforms.mount_configmap(configmap_name, mount_path, volume_name='configmap', items=None)[source]#\nModifier function to mount kubernetes configmap as files(s)\nParameters\nconfigmap_name – k8s configmap name\nmount_path – path to mount inside the container\nvolume_name – unique volume name\nitems – If unspecified, each key-value pair in the Data field\nof the referenced Configmap will be projected into the\nvolume as a file whose name is the key and content is\nthe value.\nIf specified, the listed keys will be projected into\nthe specified paths, and unlisted keys will not be\npresent.\nmlrun.platforms.mount_hostpath(host_path, mount_path, volume_name='hostpath')[source]#\nModifier function to mount kubernetes configmap as files(s)\nParameters\nhost_path – host path\nmount_path – path to mount inside the container\nvolume_name – unique volume name\nmlrun.platforms.mount_pvc(pvc_name=None, volume_name='pipeline', volume_mount_path='/mnt/pipeline')[source]#\nModifier function to apply to a Container Op to simplify volume, volume mount addition and\nenable better reuse of volumes, volume claims across container ops.\nUsage:\ntrain = train_op(...)\ntrain.apply(mount_pvc('claim-name', 'pipeline', '/mnt/pipeline'))\nmlrun.platforms.mount_s3(secret_name=None, aws_access_key='', aws_secret_key='', endpoint_url=None, prefix='', aws_region=None, non_anonymous=False)[source]#\nModifier function to add s3 env vars or secrets to container\nParameters\nsecret_name – kubernetes secret name (storing the access/secret keys)\naws_access_key – AWS_ACCESS_KEY_ID value\naws_secret_key – AWS_SECRET_ACCESS_KEY value\nendpoint_url – s3 endpoint address (for non AWS s3)\nprefix – string prefix to add before the env var name (for working with multiple s3 data stores)\naws_region – amazon region\nnon_anonymous – force the S3 API to use non-anonymous connection, even if no credentials are provided\n(for authenticating externally, such as through IAM instance-roles)\nReturns\nmlrun.platforms.mount_secret(secret_name, mount_path, volume_name='secret', items=None)[source]#\nModifier function to mount kubernetes secret as files(s)\nParameters\nsecret_name – k8s secret name\nmount_path – path to mount inside the container\nvolume_name – unique volume name\nitems – If unspecified, each key-value pair in the Data field\nof the referenced Secret will be projected into the\nvolume as a file whose name is the key and content is\nthe value.\nIf specified, the listed keys will be projected into\nthe specified paths, and unlisted keys will not be\npresent.\nmlrun.platforms.mount_v3io(name='v3io', remote='', mount_path='', access_key='', user='', secret=None, volume_mounts=None)[source]#\nModifier function to apply to a Container Op to volume mount a v3io path\nParameters\nname – the volume name\nremote – the v3io path to use for the volume. ~/ prefix will be replaced with /users/<username>/\nmount_path – the volume mount path (deprecated, exists for backwards compatibility, prefer to\nuse mounts instead)\naccess_key – the access key used to auth against v3io. if not given V3IO_ACCESS_KEY env var will be used\nuser – the username used to auth against v3io. if not given V3IO_USERNAME env var will be used\nsecret – k8s secret name which would be used to get the username and access key to auth against v3io.\nvolume_mounts – list of VolumeMount. empty volume mounts & remote will default to mount /v3io & /User.\nmlrun.platforms.mount_v3io_extended(name='v3io', remote='', mounts=None, access_key='', user='', secret=None)[source]#\nModifier function to apply to a Container Op to volume mount a v3io path\nParameters\nname – the volume name\nremote – the v3io path to use for the volume. ~/ prefix will be replaced with /users/<username>/\nmounts – list of mount & volume sub paths (type Mount). empty mounts & remote mount /v3io & /User\naccess_key – the access key used to auth against v3io. if not given V3IO_ACCESS_KEY env var will be used\nuser – the username used to auth against v3io. if not given V3IO_USERNAME env var will be used\nsecret – k8s secret name which would be used to get the username and access key to auth against v3io.\nmlrun.platforms.mount_v3io_legacy(name='v3io', remote='~/', mount_path='/User', access_key='', user='', secret=None)[source]#\nModifier function to apply to a Container Op to volume mount a v3io path\n:param name:            the volume name\n:param remote:          the v3io path to use for the volume. ~/ prefix will be replaced with /users/<username>/\n:param mount_path:      the volume mount path\n:param access_key:      the access key used to auth against v3io. if not given V3IO_ACCESS_KEY env var will be used\n:param user:            the username used to auth against v3io. if not given V3IO_USERNAME env var will be used\n:param secret:          k8s secret name which would be used to get the username and access key to auth against v3io.\nmlrun.platforms.pprint(object, stream=None, indent=1, width=80, depth=None, *, compact=False)[source]#\nPretty-print a Python object to a stream [default is sys.stdout].\nmlrun.platforms.set_env_variables(env_vars_dict: Optional[Dict[str, str]] = None, **kwargs)[source]#\nModifier function to apply a set of environment variables to a runtime. Variables may be passed\nas either a dictionary of name-value pairs, or as arguments to the function.\nSee KubeResource.apply for more information on modifiers.\nUsage:\nfunction.apply(set_env_variables({\"ENV1\": \"value1\", \"ENV2\": \"value2\"}))\nor\nfunction.apply(set_env_variables(ENV1=value1, ENV2=value2))\nParameters\nenv_vars_dict – dictionary of env. variables\nkwargs – environment variables passed as args\nmlrun.platforms.sleep(seconds)#\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision.\nmlrun.platforms.v3io_cred(api='', user='', access_key='')[source]#\nModifier function to copy local v3io env vars to container\nUsage:\ntrain = train_op(...)\ntrain.apply(use_v3io_cred())\nmlrun.platforms.watch_stream(url, shard_ids: Optional[list] = None, seek_to: Optional[str] = None, interval=None, is_json=False, **kwargs)[source]#\nwatch on a v3io stream and print data every interval\nexample:\nwatch_stream('v3io:///users/admin/mystream')\nParameters\nurl – stream url\nshard_ids – range or list of shard IDs\nseek_to – where to start/seek (‘EARLIEST’, ‘LATEST’, ‘TIME’, ‘SEQUENCE’)\ninterval – watch interval time in seconds, 0 to run once and return\nis_json – indicate the payload is json (will be deserialized)"}
{"text": "mlrun.projects#\nclass mlrun.projects.MlrunProject(name=None, description=None, params=None, functions=None, workflows=None, artifacts=None, artifact_path=None, conda=None, metadata=None, spec=None, default_requirements: Optional[Union[str, List[str]]] = None)[source]#\nBases: mlrun.model.ModelObj\nproperty artifact_path: str#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\nproperty artifacts: list#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\nbuild_function(function: Union[str, mlrun.runtimes.base.BaseRuntime], with_mlrun: Optional[bool] = None, skip_deployed: bool = False, image=None, base_image=None, commands: Optional[list] = None, secret_name='', requirements: Optional[Union[str, List[str]]] = None, mlrun_version_specifier=None, builder_env: Optional[dict] = None, overwrite_build_params: bool = False) → Union[mlrun.projects.operations.BuildStatus, kfp.dsl._container_op.ContainerOp][source]#\ndeploy ML function, build container with its dependencies\nParameters\nfunction – name of the function (in the project) or function object\nwith_mlrun – add the current mlrun package to the container build\nskip_deployed – skip the build if we already have an image for the function\nimage – target image name/path\nbase_image – base image name/path (commands and source code will be added to it)\ncommands – list of docker build (RUN) commands e.g. [‘pip install pandas’]\nsecret_name – k8s secret for accessing the docker registry\nrequirements – list of python packages or pip requirements file path, defaults to None\nmlrun_version_specifier – which mlrun package version to include (if not current)\nbuilder_env – Kaniko builder pod env vars dict (for config/credentials)\ne.g. builder_env={“GIT_TOKEN”: token}, does not work yet in KFP\noverwrite_build_params – overwrite the function build parameters with the provided ones, or attempt to\nadd to existing parameters\nclear_context()[source]#\ndelete all files and clear the context dir\nproperty context: str#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\ncreate_remote(url, name='origin', branch=None)[source]#\ncreate remote for the project git\nParameters\nurl – remote git url\nname – name for the remote (default is ‘origin’)\nbranch – Git branch to use as source\ncreate_vault_secrets(secrets)[source]#\ndeploy_function(function: Union[str, mlrun.runtimes.base.BaseRuntime], dashboard: str = '', models: Optional[list] = None, env: Optional[dict] = None, tag: Optional[str] = None, verbose: Optional[bool] = None, builder_env: Optional[dict] = None, mock: Optional[bool] = None) → Union[mlrun.projects.operations.DeployStatus, kfp.dsl._container_op.ContainerOp][source]#\ndeploy real-time (nuclio based) functions\nParameters\nfunction – name of the function (in the project) or function object\ndashboard – url of the remote Nuclio dashboard (when not local)\nmodels – list of model items\nenv – dict of extra environment variables\ntag – extra version tag\nverbose – add verbose prints/logs\nbuilder_env – env vars dict for source archive config/credentials e.g. builder_env={“GIT_TOKEN”: token}\nmock – deploy mock server vs a real Nuclio function (for local simulations)\nproperty description: str#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\nexport(filepath=None, include_files: Optional[str] = None)[source]#\nsave the project object into a yaml file or zip archive (default to project.yaml)\nBy default the project object is exported to a yaml file, when the filepath suffix is ‘.zip’\nthe project context dir (code files) are also copied into the zip, the archive path can include\nDataItem urls (for remote object storage, e.g. s3://<bucket>/<path>).\nParameters\nfilepath – path to store project .yaml or .zip (with the project dir content)\ninclude_files – glob filter string for selecting files to include in the zip archive\nfunc(key, sync=False) → mlrun.runtimes.base.BaseRuntime[source]#\nget function object by name\nParameters\nsync – will reload/reinit the function\nReturns\nfunction object\nproperty functions: list#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\nget_artifact(key, tag=None, iter=None)[source]#\nReturn an artifact object\nParameters\nkey – artifact key\ntag – version tag\niter – iteration number (for hyper-param tasks)\nReturns\nArtifact object\nget_artifact_uri(key: str, category: str = 'artifact', tag: Optional[str] = None) → str[source]#\nreturn the project artifact uri (store://..) from the artifact key\nexample:\nuri = project.get_artifact_uri(\"my_model\", category=\"model\", tag=\"prod\")\nParameters\nkey – artifact key/name\ncategory – artifact category (artifact, model, feature-vector, ..)\ntag – artifact version tag, default to latest version\nget_function(key, sync=True, enrich=False, ignore_cache=False, copy_function=True) → mlrun.runtimes.base.BaseRuntime[source]#\nget function object by name\nParameters\nkey – name of key for search\nsync – will reload/reinit the function\nenrich – add project info/config/source info to the function object\nignore_cache – read the function object from the DB (ignore the local cache)\ncopy_function – return a copy of the function object\nReturns\nfunction object\nget_function_objects() → Dict[str, mlrun.runtimes.base.BaseRuntime][source]#\n“get a virtual dict with all the project functions ready for use in a pipeline\nget_param(key: str, default=None)[source]#\nget project param by key\nget_run_status(run, timeout=None, expected_statuses=None, notifiers: Optional[mlrun.utils.notifications.notification_pusher.CustomNotificationPusher] = None)[source]#\nget_secret(key: str)[source]#\nget a key based secret e.g. DB password from the context\nsecrets can be specified when invoking a run through files, env, ..\nget_store_resource(uri)[source]#\nget store resource object by uri\nget_vault_secrets(secrets=None, local=False)[source]#\nimport_artifact(item_path: str, new_key=None, artifact_path=None, tag=None)[source]#\nImport an artifact object/package from .yaml, .json, or .zip file\nParameters\nitem_path – dataitem url  or file path to the file/package\nnew_key – overwrite the artifact key/name\nartifact_path – target artifact path (when not using the default)\ntag – artifact tag to set\nReturns\nartifact object\nkind = 'project'#\nlist_artifacts(name=None, tag=None, labels=None, since=None, until=None, iter: Optional[int] = None, best_iteration: bool = False, kind: Optional[str] = None, category: Optional[Union[str, mlrun.api.schemas.artifact.ArtifactCategories]] = None) → mlrun.lists.ArtifactList[source]#\nList artifacts filtered by various parameters.\nThe returned result is an ArtifactList (list of dict), use .to_objects() to convert it to a list of\nRunObjects, .show() to view graphically in Jupyter, and .to_df() to convert to a DataFrame.\nExamples:\n# Get latest version of all artifacts in project\nlatest_artifacts = project.list_artifacts('', tag='latest')\n# check different artifact versions for a specific artifact, return as objects list\nresult_versions = project.list_artifacts('results', tag='*').to_objects()\nParameters\nname – Name of artifacts to retrieve. Name is used as a like query, and is not case-sensitive. This means\nthat querying for name may return artifacts named my_Name_1 or surname.\ntag – Return artifacts assigned this tag.\nlabels – Return artifacts that have these labels.\nsince – Not in use in HTTPRunDB.\nuntil – Not in use in HTTPRunDB.\niter – Return artifacts from a specific iteration (where iter=0 means the root iteration). If\nNone (default) return artifacts from all iterations.\nbest_iteration – Returns the artifact which belongs to the best iteration of a given run, in the case of\nartifacts generated from a hyper-param run. If only a single iteration exists, will return the artifact\nfrom that iteration. If using best_iter, the iter parameter must not be used.\nkind – Return artifacts of the requested kind.\ncategory – Return artifacts of the requested category.\nlist_functions(name=None, tag=None, labels=None)[source]#\nRetrieve a list of functions, filtered by specific criteria.\nexample:\nfunctions = project.list_functions(tag=\"latest\")\nParameters\nname – Return only functions with a specific name.\ntag – Return function versions with specific tags.\nlabels – Return functions that have specific labels assigned to them.\nReturns\nList of function objects.\nlist_models(name=None, tag=None, labels=None, since=None, until=None, iter: Optional[int] = None, best_iteration: bool = False)[source]#\nList models in project, filtered by various parameters.\nExamples:\n# Get latest version of all models in project\nlatest_models = project.list_models('', tag='latest')\nParameters\nname – Name of artifacts to retrieve. Name is used as a like query, and is not case-sensitive. This means\nthat querying for name may return artifacts named my_Name_1 or surname.\ntag – Return artifacts assigned this tag.\nlabels – Return artifacts that have these labels.\nsince – Not in use in HTTPRunDB.\nuntil – Not in use in HTTPRunDB.\niter – Return artifacts from a specific iteration (where iter=0 means the root iteration). If\nNone (default) return artifacts from all iterations.\nbest_iteration – Returns the artifact which belongs to the best iteration of a given run, in the case of\nartifacts generated from a hyper-param run. If only a single iteration exists, will return the artifact\nfrom that iteration. If using best_iter, the iter parameter must not be used.\nlist_runs(name=None, uid=None, labels=None, state=None, sort=True, last=0, iter=False, start_time_from: Optional[datetime.datetime] = None, start_time_to: Optional[datetime.datetime] = None, last_update_time_from: Optional[datetime.datetime] = None, last_update_time_to: Optional[datetime.datetime] = None, **kwargs) → mlrun.lists.RunList[source]#\nRetrieve a list of runs, filtered by various options.\nThe returned result is a `` (list of dict), use .to_objects() to convert it to a list of RunObjects,\n.show() to view graphically in Jupyter, .to_df() to convert to a DataFrame, and compare() to\ngenerate comparison table and PCP plot.\nExample:\n# return a list of runs matching the name and label and compare\nruns = project.list_runs(name='download', labels='owner=admin')\nruns.compare()\n# If running in Jupyter, can use the .show() function to display the results\nproject.list_runs(name='').show()\nParameters\nname – Name of the run to retrieve.\nuid – Unique ID of the run.\nproject – Project that the runs belongs to.\nlabels – List runs that have a specific label assigned. Currently only a single label filter can be\napplied, otherwise result will be empty.\nstate – List only runs whose state is specified.\nsort – Whether to sort the result according to their start time. Otherwise, results will be\nreturned by their internal order in the DB (order will not be guaranteed).\nlast – Deprecated - currently not used.\niter – If True return runs from all iterations. Otherwise, return only runs whose iter is 0.\nstart_time_from – Filter by run start time in [start_time_from, start_time_to].\nstart_time_to – Filter by run start time in [start_time_from, start_time_to].\nlast_update_time_from – Filter by run last update time in (last_update_time_from,\nlast_update_time_to).\nlast_update_time_to – Filter by run last update time in (last_update_time_from, last_update_time_to).\nlog_artifact(item, body=None, tag='', local_path='', artifact_path=None, format=None, upload=None, labels=None, target_path=None, **kwargs)[source]#\nlog an output artifact and optionally upload it to datastore\nexample:\nproject.log_artifact(\n\"some-data\",\nbody=b\"abc is 123\",\nlocal_path=\"model.txt\",\nlabels={\"framework\": \"xgboost\"},\n)\nParameters\nitem – artifact key or artifact class ()\nbody – will use the body as the artifact content\nlocal_path – path to the local file we upload, will also be use\nas the destination subpath (under “artifact_path”)\nartifact_path – target artifact path (when not using the default)\nto define a subpath under the default location use:\nartifact_path=context.artifact_subpath(‘data’)\nformat – artifact file format: csv, png, ..\ntag – version tag\ntarget_path – absolute target path (instead of using artifact_path + local_path)\nupload – upload to datastore (default is True)\nlabels – a set of key/value labels to tag the artifact with\nReturns\nartifact object\nlog_dataset(key, df, tag='', local_path=None, artifact_path=None, upload=None, labels=None, format='', preview=None, stats=False, target_path='', extra_data=None, label_column: Optional[str] = None, **kwargs) → mlrun.artifacts.dataset.DatasetArtifact[source]#\nlog a dataset artifact and optionally upload it to datastore\nexample:\nraw_data = {\n\"first_name\": [\"Jason\", \"Molly\", \"Tina\", \"Jake\", \"Amy\"],\n\"last_name\": [\"Miller\", \"Jacobson\", \"Ali\", \"Milner\", \"Cooze\"],\n\"age\": [42, 52, 36, 24, 73],\n\"testScore\": [25, 94, 57, 62, 70],\n}\ndf = pd.DataFrame(raw_data, columns=[\"first_name\", \"last_name\", \"age\", \"testScore\"])\nproject.log_dataset(\"mydf\", df=df, stats=True)\nParameters\nkey – artifact key\ndf – dataframe object\nlabel_column – name of the label column (the one holding the target (y) values)\nlocal_path – path to the local file we upload, will also be use\nas the destination subpath (under “artifact_path”)\nartifact_path – target artifact path (when not using the default)\nto define a subpath under the default location use:\nartifact_path=context.artifact_subpath(‘data’)\ntag – version tag\nformat – optional, format to use (e.g. csv, parquet, ..)\ntarget_path – absolute target path (instead of using artifact_path + local_path)\npreview – number of lines to store as preview in the artifact metadata\nstats – calculate and store dataset stats in the artifact metadata\nextra_data – key/value list of extra files/charts to link with this dataset\nupload – upload to datastore (default is True)\nlabels – a set of key/value labels to tag the artifact with\nReturns\nartifact object\nlog_model(key, body=None, framework='', tag='', model_dir=None, model_file=None, algorithm=None, metrics=None, parameters=None, artifact_path=None, upload=None, labels=None, inputs: Optional[List[mlrun.features.Feature]] = None, outputs: Optional[List[mlrun.features.Feature]] = None, feature_vector: Optional[str] = None, feature_weights: Optional[list] = None, training_set=None, label_column=None, extra_data=None, **kwargs)[source]#\nlog a model artifact and optionally upload it to datastore\nexample:\nproject.log_model(\"model\", body=dumps(model),\nmodel_file=\"model.pkl\",\nmetrics=context.results,\ntraining_set=training_df,\nlabel_column='label',\nfeature_vector=feature_vector_uri,\nlabels={\"app\": \"fraud\"})\nParameters\nkey – artifact key or artifact class ()\nbody – will use the body as the artifact content\nmodel_file – path to the local model file we upload (see also model_dir)\nor to a model file data url (e.g. http://host/path/model.pkl)\nmodel_dir – path to the local dir holding the model file and extra files\nartifact_path – target artifact path (when not using the default)\nto define a subpath under the default location use:\nartifact_path=context.artifact_subpath(‘data’)\nframework – name of the ML framework\nalgorithm – training algorithm name\ntag – version tag\nmetrics – key/value dict of model metrics\nparameters – key/value dict of model parameters\ninputs – ordered list of model input features (name, type, ..)\noutputs – ordered list of model output/result elements (name, type, ..)\nupload – upload to datastore (default is True)\nlabels – a set of key/value labels to tag the artifact with\nfeature_vector – feature store feature vector uri (store://feature-vectors/<project>/<name>[:tag])\nfeature_weights – list of feature weights, one per input column\ntraining_set – training set dataframe, used to infer inputs & outputs\nlabel_column – which columns in the training set are the label (target) columns\nextra_data – key/value list of extra files/charts to link with this dataset\nvalue can be absolute path | relative path (to model dir) | bytes | artifact object\nReturns\nartifact object\nproperty metadata: mlrun.projects.project.ProjectMetadata#\nproperty mountdir: str#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\nproperty name: str#\nProject name, this is a property of the project metadata\nproperty notifiers#\nproperty params: str#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\npull(branch=None, remote=None)[source]#\npull/update sources from git or tar into the context dir\nParameters\nbranch – git branch, if not the current one\nremote – git remote, if other than origin\npush(branch, message=None, update=True, remote=None, add: Optional[list] = None)[source]#\nupdate spec and push updates to remote git repo\nParameters\nbranch – target git branch\nmessage – git commit message\nupdate – update files (git add update=True)\nremote – git remote, default to origin\nadd – list of files to add\nregister_artifacts()[source]#\nregister the artifacts in the MLRun DB (under this project)\nreload(sync=False, context=None) → mlrun.projects.project.MlrunProject[source]#\nreload the project and function objects from the project yaml/specs\nParameters\nsync – set to True to load functions objects\ncontext – context directory (where the yaml and code exist)\nReturns\nproject object\nremove_function(name)[source]#\nremove a function from a project\nParameters\nname – name of the function (under the project)\nrun(name: Optional[str] = None, workflow_path: Optional[str] = None, arguments: Optional[Dict[str, Any]] = None, artifact_path: Optional[str] = None, workflow_handler: Optional[Union[str, Callable]] = None, namespace: Optional[str] = None, sync: bool = False, watch: bool = False, dirty: bool = False, ttl: Optional[int] = None, engine: Optional[str] = None, local: Optional[bool] = None, schedule: Optional[Union[str, mlrun.api.schemas.schedule.ScheduleCronTrigger, bool]] = None, timeout: Optional[int] = None) → mlrun.projects.pipelines._PipelineRunStatus[source]#\nrun a workflow using kubeflow pipelines\nParameters\nname – name of the workflow\nworkflow_path – url to a workflow file, if not a project workflow\narguments – kubeflow pipelines arguments (parameters)\nartifact_path – target path/url for workflow artifacts, the string\n‘{{workflow.uid}}’ will be replaced by workflow id\nworkflow_handler – workflow function handler (for running workflow function directly)\nnamespace – kubernetes namespace if other than default\nsync – force functions sync before run\nwatch – wait for pipeline completion\ndirty – allow running the workflow when the git repo is dirty\nttl – pipeline ttl in secs (after that the pods will be removed)\nengine – workflow engine running the workflow.\nsupported values are ‘kfp’ (default), ‘local’ or ‘remote’.\nfor setting engine for remote running use ‘remote:local’ or ‘remote:kfp’.\nlocal – run local pipeline with local functions (set local=True in function.run())\nschedule – ScheduleCronTrigger class instance or a standard crontab expression string\n(which will be converted to the class using its from_crontab constructor),\nsee this link for help:\nhttps://apscheduler.readthedocs.io/en/3.x/modules/triggers/cron.html#module-apscheduler.triggers.cron\nfor using the pre-defined workflow’s schedule, set schedule=True\ntimeout – timeout in seconds to wait for pipeline completion (used when watch=True)\nReturns\nrun id\nrun_function(function: Union[str, mlrun.runtimes.base.BaseRuntime], handler: Optional[str] = None, name: str = '', params: Optional[dict] = None, hyperparams: Optional[dict] = None, hyper_param_options: Optional[mlrun.model.HyperParamOptions] = None, inputs: Optional[dict] = None, outputs: Optional[List[str]] = None, workdir: str = '', labels: Optional[dict] = None, base_task: Optional[mlrun.model.RunTemplate] = None, watch: bool = True, local: Optional[bool] = None, verbose: Optional[bool] = None, selector: Optional[str] = None, auto_build: Optional[bool] = None, schedule: Optional[Union[str, mlrun.api.schemas.schedule.ScheduleCronTrigger]] = None, artifact_path: Optional[str] = None) → Union[mlrun.model.RunObject, kfp.dsl._container_op.ContainerOp][source]#\nRun a local or remote task as part of a local/kubeflow pipeline\nexample (use with project):\n# create a project with two functions (local and from marketplace)\nproject = mlrun.new_project(project_name, \"./proj\")\nproject.set_function(\"mycode.py\", \"myfunc\", image=\"mlrun/mlrun\")\nproject.set_function(\"hub://sklearn_classifier\", \"train\")\n# run functions (refer to them by name)\nrun1 = project.run_function(\"myfunc\", params={\"x\": 7})\nrun2 = project.run_function(\"train\", params={\"data\": run1.outputs[\"data\"]})\nParameters\nfunction – name of the function (in the project) or function object\nhandler – name of the function handler\nname – execution name\nparams – input parameters (dict)\nhyperparams – hyper parameters\nselector – selection criteria for hyper params e.g. “max.accuracy”\nhyper_param_options – hyper param options (selector, early stop, strategy, ..)\nsee: HyperParamOptions\ninputs – input objects (dict of key: path)\noutputs – list of outputs which can pass in the workflow\nworkdir – default input artifacts path\nlabels – labels to tag the job/run with ({key:val, ..})\nbase_task – task object to use as base\nwatch – watch/follow run log, True by default\nlocal – run the function locally vs on the runtime/cluster\nverbose – add verbose prints/logs\nauto_build – when set to True and the function require build it will be built on the first\nfunction run, use only if you dont plan on changing the build config between runs\nschedule – ScheduleCronTrigger class instance or a standard crontab expression string\n(which will be converted to the class using its from_crontab constructor),\nsee this link for help:\nhttps://apscheduler.readthedocs.io/en/v3.6.3/modules/triggers/cron.html#module-apscheduler.triggers.cron\nartifact_path – path to store artifacts, when running in a workflow this will be set automatically\nReturns\nMLRun RunObject or KubeFlow containerOp\nsave(filepath=None, store=True)[source]#\nexport project to yaml file and save project in database\nStore\nif True, allow updating in case project already exists\nsave_to_db(store=True)[source]#\nsave project to database\nStore\nif True, allow updating in case project already exists\nsave_workflow(name, target, artifact_path=None, ttl=None)[source]#\ncreate and save a workflow as a yaml or archive file\nParameters\nname – workflow name\ntarget – target file path (can end with .yaml or .zip)\nartifact_path – target path/url for workflow artifacts, the string\n‘{{workflow.uid}}’ will be replaced by workflow id\nttl – pipeline ttl (time to live) in secs (after that the pods will be removed)\nset_artifact(key, artifact: Optional[Union[str, dict, mlrun.artifacts.base.Artifact]] = None, target_path: Optional[str] = None, tag: Optional[str] = None)[source]#\nadd/set an artifact in the project spec (will be registered on load)\nexample:\n# register a simple file artifact\nproject.set_artifact('data', target_path=data_url)\n# register a model artifact\nproject.set_artifact('model', ModelArtifact(model_file=\"model.pkl\"), target_path=model_dir_url)\n# register a path to artifact package (will be imported on project load)\n# to generate such package use `artifact.export(target_path)`\nproject.set_artifact('model', 'https://mystuff.com/models/mymodel.zip')\nParameters\nkey – artifact key/name\nartifact – mlrun Artifact object/dict (or its subclasses) or path to artifact\nfile to import (yaml/json/zip), relative paths are relative to the context path\ntarget_path – absolute target path url (point to the artifact content location)\ntag – artifact tag\nset_function(func: Optional[Union[str, mlrun.runtimes.base.BaseRuntime]] = None, name: str = '', kind: str = '', image: Optional[str] = None, handler=None, with_repo: Optional[bool] = None, tag: Optional[str] = None, requirements: Optional[Union[str, List[str]]] = None) → mlrun.runtimes.base.BaseRuntime[source]#\nupdate or add a function object to the project\nfunction can be provided as an object (func) or a .py/.ipynb/.yaml url\nsupport url prefixes:\nobject (s3://, v3io://, ..)\nMLRun DB e.g. db://project/func:ver\nfunctions hub/market: e.g. hub://sklearn_classifier:master\nexamples:\nproj.set_function(func_object)\nproj.set_function('./src/mycode.py', 'ingest',\nimage='myrepo/ing:latest', with_repo=True)\nproj.set_function('http://.../mynb.ipynb', 'train')\nproj.set_function('./func.yaml')\nproj.set_function('hub://get_toy_data', 'getdata')\nParameters\nfunc – function object or spec/code url, None refers to current Notebook\nname – name of the function (under the project)\nkind – runtime kind e.g. job, nuclio, spark, dask, mpijob\ndefault: job\nimage – docker image to be used, can also be specified in\nthe function object/yaml\nhandler – default function handler to invoke (can only be set with .py/.ipynb files)\nwith_repo – add (clone) the current repo to the build source\nrequirements – list of python packages or pip requirements file path\nTag\nfunction version tag (none for ‘latest’, can only be set with .py/.ipynb files)\nReturns\nproject object\nset_model_monitoring_credentials(access_key: str)[source]#\nSet the credentials that will be used by the project’s model monitoring\ninfrastructure functions.\nThe supplied credentials must have data access\nParameters\naccess_key – Model Monitoring access key for managing user permissions.\nset_secrets(secrets: Optional[dict] = None, file_path: Optional[str] = None, provider: Optional[Union[str, mlrun.api.schemas.secret.SecretProviderName]] = None)[source]#\nset project secrets from dict or secrets env file\nwhen using a secrets file it should have lines in the form KEY=VALUE, comment line start with “#”\nV3IO paths/credentials and MLrun service API address are dropped from the secrets\nexample secrets file:\n# this is an env file\nAWS_ACCESS_KEY_ID-XXXX\nAWS_SECRET_ACCESS_KEY=YYYY\nusage:\n# read env vars from dict or file and set as project secrets\nproject.set_secrets({\"SECRET1\": \"value\"})\nproject.set_secrets(file_path=\"secrets.env\")\nParameters\nsecrets – dict with secrets key/value\nfile_path – path to secrets file\nprovider – MLRun secrets provider\nset_source(source, pull_at_runtime=False, workdir=None)[source]#\nset the project source code path(can be git/tar/zip archive)\nParameters\nsource – valid path to git, zip, or tar file, (or None for current) e.g.\ngit://github.com/mlrun/something.git\nhttp://some/url/file.zip\npull_at_runtime – load the archive into the container at job runtime vs on build/deploy\nworkdir – the relative workdir path (under the context dir)\nset_workflow(name, workflow_path: str, embed=False, engine=None, args_schema: Optional[List[mlrun.model.EntrypointParam]] = None, handler=None, schedule: Optional[Union[str, mlrun.api.schemas.schedule.ScheduleCronTrigger]] = None, ttl=None, **args)[source]#\nadd or update a workflow, specify a name and the code path\nParameters\nname – name of the workflow\nworkflow_path – url/path for the workflow file\nembed – add the workflow code into the project.yaml\nengine – workflow processing engine (“kfp” or “local”)\nargs_schema – list of arg schema definitions (:py:class`~mlrun.model.EntrypointParam`)\nhandler – workflow function handler\nschedule – ScheduleCronTrigger class instance or a standard crontab expression string\n(which will be converted to the class using its from_crontab constructor),\nsee this link for help:\nhttps://apscheduler.readthedocs.io/en/3.x/modules/triggers/cron.html#module-apscheduler.triggers.cron\nttl – pipeline ttl in secs (after that the pods will be removed)\nargs – argument values (key=value, ..)\nproperty source: str#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\nproperty spec: mlrun.projects.project.ProjectSpec#\nproperty status: mlrun.projects.project.ProjectStatus#\nsync_functions(names: Optional[list] = None, always=True, save=False)[source]#\nreload function objects from specs and files\nwith_secrets(kind, source, prefix='')[source]#\nregister a secrets source (file, env or dict)\nread secrets from a source provider to be used in workflows, example:\nproj.with_secrets('file', 'file.txt')\nproj.with_secrets('inline', {'key': 'val'})\nproj.with_secrets('env', 'ENV1,ENV2', prefix='PFX_')\nVault secret source has several options:\nproj.with_secrets('vault', {'user': <user name>, 'secrets': ['secret1', 'secret2' ...]})\nproj.with_secrets('vault', {'project': <proj.name>, 'secrets': ['secret1', 'secret2' ...]})\nproj.with_secrets('vault', ['secret1', 'secret2' ...])\nThe 2nd option uses the current project name as context.\nCan also use empty secret list:\nproj.with_secrets('vault', [])\nThis will enable access to all secrets in vault registered to the current project.\nParameters\nkind – secret type (file, inline, env, vault)\nsource – secret data or link (see example)\nprefix – add a prefix to the keys in this source\nReturns\nproject object\nproperty workflows: list#\nThis is a property of the spec, look there for documentation\nleaving here for backwards compatibility with users code that used MlrunProjectLegacy\nclass mlrun.projects.ProjectMetadata(name=None, created=None, labels=None, annotations=None)[source]#\nBases: mlrun.model.ModelObj\nproperty name: str#\nProject name\nstatic validate_project_name(name: str, raise_on_failure: bool = True) → bool[source]#\nclass mlrun.projects.ProjectSpec(description=None, params=None, functions=None, workflows=None, artifacts=None, artifact_path=None, conda=None, source=None, subpath=None, origin_url=None, goals=None, load_source_on_run=None, default_requirements: Optional[Union[str, List[str]]] = None, desired_state='online', owner=None, disable_auto_mount=None, workdir=None)[source]#\nBases: mlrun.model.ModelObj\nproperty artifacts: list#\nlist of artifacts used in this project\nproperty functions: list#\nlist of function object/specs used in this project\nget_code_path()[source]#\nGet the path to the code root/workdir\nproperty mountdir: str#\nspecify to mount the context dir inside the function container\nuse ‘.’ to use the same path as in the client e.g. Jupyter\nremove_artifact(key)[source]#\nremove_function(name)[source]#\nremove_workflow(name)[source]#\nset_artifact(key, artifact)[source]#\nset_function(name, function_object, function_dict)[source]#\nset_workflow(name, workflow)[source]#\nproperty source: str#\nsource url or git repo\nproperty workflows: List[dict]#\nlist of workflows specs dicts used in this project\nType\nreturns\nclass mlrun.projects.ProjectStatus(state=None)[source]#\nBases: mlrun.model.ModelObj\nmlrun.projects.build_function(function: Union[str, mlrun.runtimes.base.BaseRuntime], with_mlrun: Optional[bool] = None, skip_deployed: bool = False, image=None, base_image=None, commands: Optional[list] = None, secret_name='', requirements: Optional[Union[str, List[str]]] = None, mlrun_version_specifier=None, builder_env: Optional[dict] = None, project_object=None, overwrite_build_params: bool = False) → Union[mlrun.projects.operations.BuildStatus, kfp.dsl._container_op.ContainerOp][source]#\ndeploy ML function, build container with its dependencies\nParameters\nfunction – name of the function (in the project) or function object\nwith_mlrun – add the current mlrun package to the container build\nskip_deployed – skip the build if we already have an image for the function\nimage – target image name/path\nbase_image – base image name/path (commands and source code will be added to it)\ncommands – list of docker build (RUN) commands e.g. [‘pip install pandas’]\nsecret_name – k8s secret for accessing the docker registry\nrequirements – list of python packages or pip requirements file path, defaults to None\nmlrun_version_specifier – which mlrun package version to include (if not current)\nbuilder_env – Kaniko builder pod env vars dict (for config/credentials)\ne.g. builder_env={“GIT_TOKEN”: token}, does not work yet in KFP\nproject_object – override the project object to use, will default to the project set in the runtime context.\nbuilder_env – Kaniko builder pod env vars dict (for config/credentials)\ne.g. builder_env={“GIT_TOKEN”: token}, does not work yet in KFP\noverwrite_build_params – overwrite the function build parameters with the provided ones, or attempt to add\nto existing parameters\nmlrun.projects.deploy_function(function: Union[str, mlrun.runtimes.base.BaseRuntime], dashboard: str = '', models: Optional[list] = None, env: Optional[dict] = None, tag: Optional[str] = None, verbose: Optional[bool] = None, builder_env: Optional[dict] = None, project_object=None, mock: Optional[bool] = None) → Union[mlrun.projects.operations.DeployStatus, kfp.dsl._container_op.ContainerOp][source]#\ndeploy real-time (nuclio based) functions\nParameters\nfunction – name of the function (in the project) or function object\ndashboard – url of the remote Nuclio dashboard (when not local)\nmodels – list of model items\nenv – dict of extra environment variables\ntag – extra version tag\nverbose – add verbose prints/logs\nbuilder_env – env vars dict for source archive config/credentials e.g. builder_env={“GIT_TOKEN”: token}\nmock – deploy mock server vs a real Nuclio function (for local simulations)\nproject_object – override the project object to use, will default to the project set in the runtime context.\nmlrun.projects.get_or_create_project(name: str, context: str, url: Optional[str] = None, secrets: Optional[dict] = None, init_git=False, subpath: Optional[str] = None, clone: bool = False, user_project: bool = False, from_template: Optional[str] = None, save: bool = True) → mlrun.projects.project.MlrunProject[source]#\nLoad a project from MLRun DB, or create/import if doesnt exist\nexample:\n# load project from the DB (if exist) or the source repo\nproject = get_or_create_project(\"myproj\", \"./\", \"git://github.com/mlrun/demo-xgb-project.git\")\nproject.pull(\"development\")  # pull the latest code from git\nproject.run(\"main\", arguments={'data': data_url})  # run the workflow \"main\"\nParameters\nname – project name\ncontext – project local directory path\nurl – name (in DB) or git or tar.gz or .zip sources archive path e.g.:\ngit://github.com/mlrun/demo-xgb-project.git\nhttp://mysite/archived-project.zip\nsecrets – key:secret dict or SecretsStore used to download sources\ninit_git – if True, will git init the context dir\nsubpath – project subpath (within the archive/context)\nclone – if True, always clone (delete any existing content)\nuser_project – add the current user name to the project name (for db:// prefixes)\nfrom_template – path to project YAML file that will be used as from_template (for new projects)\nsave – whether to save the created project in the DB\nReturns\nproject object\nmlrun.projects.load_project(context: str, url: Optional[str] = None, name: Optional[str] = None, secrets: Optional[dict] = None, init_git: bool = False, subpath: Optional[str] = None, clone: bool = False, user_project: bool = False, save: bool = True) → mlrun.projects.project.MlrunProject[source]#\nLoad an MLRun project from git or tar or dir\nexample:\n# Load the project and run the 'main' workflow.\n# When using git as the url source the context directory must be an empty or\n# non-existent folder as the git repo will be cloned there\nproject = load_project(\"./demo_proj\", \"git://github.com/mlrun/project-demo.git\")\nproject.run(\"main\", arguments={'data': data_url})\nParameters\ncontext – project local directory path\nurl – name (in DB) or git or tar.gz or .zip sources archive path e.g.:\ngit://github.com/mlrun/demo-xgb-project.git\nhttp://mysite/archived-project.zip\n<project-name>\nThe git project should include the project yaml file.\nIf the project yaml file is in a sub-directory, must specify the sub-directory.\nname – project name\nsecrets – key:secret dict or SecretsStore used to download sources\ninit_git – if True, will git init the context dir\nsubpath – project subpath (within the archive)\nclone – if True, always clone (delete any existing content)\nuser_project – add the current user name to the project name (for db:// prefixes)\nsave – whether to save the created project and artifact in the DB\nReturns\nproject object\nmlrun.projects.new_project(name, context=None, init_git: bool = False, user_project: bool = False, remote: Optional[str] = None, from_template: Optional[str] = None, secrets: Optional[dict] = None, description: Optional[str] = None, subpath: Optional[str] = None, save: bool = True, overwrite: bool = False) → mlrun.projects.project.MlrunProject[source]#\nCreate a new MLRun project, optionally load it from a yaml/zip/git template\nexample:\n# create a project with local and marketplace functions, a workflow, and an artifact\nproject = mlrun.new_project(\"myproj\", \"./\", init_git=True, description=\"my new project\")\nproject.set_function('prep_data.py', 'prep-data', image='mlrun/mlrun', handler='prep_data')\nproject.set_function('hub://sklearn_classifier', 'train')\nproject.set_artifact('data', Artifact(target_path=data_url))\nproject.set_workflow('main', \"./myflow.py\")\nproject.save()\n# run the \"main\" workflow (watch=True to wait for run completion)\nproject.run(\"main\", watch=True)\nexample (load from template):\n# create a new project from a zip template (can also use yaml/git templates)\n# initialize a local git, and register the git remote path\nproject = mlrun.new_project(\"myproj\", \"./\", init_git=True,\nremote=\"git://github.com/mlrun/project-demo.git\",\nfrom_template=\"http://mysite/proj.zip\")\nproject.run(\"main\", watch=True)\nParameters\nname – project name\ncontext – project local directory path\ninit_git – if True, will git init the context dir\nuser_project – add the current user name to the provided project name (making it unique per user)\nremote – remote Git url\nfrom_template – path to project YAML/zip file that will be used as a template\nsecrets – key:secret dict or SecretsStore used to download sources\ndescription – text describing the project\nsubpath – project subpath (relative to the context dir)\nsave – whether to save the created project in the DB\noverwrite – overwrite project using ‘cascade’ deletion strategy (deletes project resources)\nif project with name exists\nReturns\nproject object\nmlrun.projects.run_function(function: Union[str, mlrun.runtimes.base.BaseRuntime], handler: Optional[str] = None, name: str = '', params: Optional[dict] = None, hyperparams: Optional[dict] = None, hyper_param_options: Optional[mlrun.model.HyperParamOptions] = None, inputs: Optional[dict] = None, outputs: Optional[List[str]] = None, workdir: str = '', labels: Optional[dict] = None, base_task: Optional[mlrun.model.RunTemplate] = None, watch: bool = True, local: Optional[bool] = None, verbose: Optional[bool] = None, selector: Optional[str] = None, project_object=None, auto_build: Optional[bool] = None, schedule: Optional[Union[str, mlrun.api.schemas.schedule.ScheduleCronTrigger]] = None, artifact_path: Optional[str] = None) → Union[mlrun.model.RunObject, kfp.dsl._container_op.ContainerOp][source]#\nRun a local or remote task as part of a local/kubeflow pipeline\nrun_function() allow you to execute a function locally, on a remote cluster, or as part of an automated workflow\nfunction can be specified as an object or by name (str), when the function is specified by name it is looked up\nin the current project eliminating the need to redefine/edit functions.\nwhen functions run as part of a workflow/pipeline (project.run()) some attributes can be set at the run level,\ne.g. local=True will run all the functions locally, setting artifact_path will direct all outputs to the same path.\nproject runs provide additional notifications/reporting and exception handling.\ninside a Kubeflow pipeline (KFP) run_function() generates KFP “ContainerOps” which are used to form a DAG\nsome behavior may differ between regular runs and deferred KFP runs.\nexample (use with function object):\nfunction = mlrun.import_function(\"hub://sklearn_classifier\")\nrun1 = run_function(function, params={\"data\": url})\nexample (use with project):\n# create a project with two functions (local and from marketplace)\nproject = mlrun.new_project(project_name, \"./proj)\nproject.set_function(\"mycode.py\", \"myfunc\", image=\"mlrun/mlrun\")\nproject.set_function(\"hub://sklearn_classifier\", \"train\")\n# run functions (refer to them by name)\nrun1 = run_function(\"myfunc\", params={\"x\": 7})\nrun2 = run_function(\"train\", params={\"data\": run1.outputs[\"data\"]})\nexample (use in pipeline):\n@dsl.pipeline(name=\"test pipeline\", description=\"test\")\ndef my_pipe(url=\"\"):\nrun1 = run_function(\"loaddata\", params={\"url\": url})\nrun2 = run_function(\"train\", params={\"data\": run1.outputs[\"data\"]})\nproject.run(workflow_handler=my_pipe, arguments={\"param1\": 7})\nParameters\nfunction – name of the function (in the project) or function object\nhandler – name of the function handler\nname – execution name\nparams – input parameters (dict)\nhyperparams – hyper parameters\nselector – selection criteria for hyper params e.g. “max.accuracy”\nhyper_param_options – hyper param options (selector, early stop, strategy, ..)\nsee: HyperParamOptions\ninputs – input objects (dict of key: path)\noutputs – list of outputs which can pass in the workflow\nworkdir – default input artifacts path\nlabels – labels to tag the job/run with ({key:val, ..})\nbase_task – task object to use as base\nwatch – watch/follow run log, True by default\nlocal – run the function locally vs on the runtime/cluster\nverbose – add verbose prints/logs\nproject_object – override the project object to use, will default to the project set in the runtime context.\nauto_build – when set to True and the function require build it will be built on the first\nfunction run, use only if you dont plan on changing the build config between runs\nschedule – ScheduleCronTrigger class instance or a standard crontab expression string\n(which will be converted to the class using its from_crontab constructor),\nsee this link for help:\nhttps://apscheduler.readthedocs.io/en/v3.6.3/modules/triggers/cron.html#module-apscheduler.triggers.cron\nartifact_path – path to store artifacts, when running in a workflow this will be set automatically\nReturns\nMLRun RunObject or KubeFlow containerOp"}
{"text": "mlrun.run#\nclass mlrun.run.ArtifactType(value)[source]#\nBases: enum.Enum\nPossible artifact types to log using the MLRun context decorator.\nDATASET = 'dataset'#\nDEFAULT = 'result'#\nDIRECTORY = 'directory'#\nFILE = 'file'#\nOBJECT = 'object'#\nPLOT = 'plot'#\nRESULT = 'result'#\nclass mlrun.run.ContextHandler[source]#\nBases: object\nPrivate class for handling an MLRun context of a function that is wrapped in MLRun’s handler decorator.\nThe context handler have 3 duties:\nCheck if the user used MLRun to run the wrapped function and if so, get the MLRun context.\nParse the user’s inputs (MLRun DataItem) to the function.\nLog the function’s outputs to MLRun.\nThe context handler use dictionaries to map objects to their logging / parsing function. The maps can be edited\nusing the relevant update_X class method. If needed to add additional artifacts types, the ArtifactType class\ncan be inherited and replaced as well using the update_artifact_type_class class method.\nInitialize a context handler.\nis_context_available() → bool[source]#\nCheck if a context was found by the method look_for_context.\nReturns\nTrue if a context was found and False otherwise.\nlog_outputs(outputs: list, logging_instructions: List[Optional[Union[Tuple[str, mlrun.run.ArtifactType], Tuple[str, str], Tuple[str, mlrun.run.ArtifactType, Dict[str, Any]], Tuple[str, str, Dict[str, Any]], str]]])[source]#\nLog the given outputs as artifacts with the stored context.\nParameters\noutputs – List of outputs to log.\nlogging_instructions – List of logging instructions to use.\nlook_for_context(args: tuple, kwargs: dict)[source]#\nLook for an MLRun context (mlrun.MLClientCtx). The handler will look for a context in the given order:\nThe given arguments.\nThe given keyword arguments.\nIf an MLRun RunTime was used the context will be located via the mlrun.get_or_create_ctx method.\nParameters\nargs – The arguments tuple passed to the function.\nkwargs – The keyword arguments dictionary passed to the function.\nparse_inputs(args: tuple, kwargs: dict, expected_arguments_types: collections.OrderedDict) → tuple[source]#\nParse the given arguments and keyword arguments data items to the expected types.\nParameters\nargs – The arguments tuple passed to the function.\nkwargs – The keyword arguments dictionary passed to the function.\nexpected_arguments_types – An ordered dictionary of the expected types of arguments.\nReturns\nThe parsed args (kwargs are parsed inplace).\nset_labels(labels: Dict[str, str])[source]#\nSet the given labels with the stored context.\nParameters\nlabels – The labels to set.\nclassmethod update_artifact_type_class(artifact_type_class: Type[mlrun.run.ArtifactType])[source]#\nUpdate the artifact type enum class that the handler will use to specify new artifact types to log and parse.\nParameters\nartifact_type_class – An enum inheriting from the ArtifactType enum.\nclassmethod update_default_objects_artifact_types_map(updates: Dict[type, mlrun.run.ArtifactType])[source]#\nEnrich the default objects artifact types map with new objects types to support.\nParameters\nupdates – New objects types to artifact types to support.\nclassmethod update_inputs_parsing_map(updates: Dict[type, Callable[[mlrun.datastore.base.DataItem], Any]])[source]#\nEnrich the inputs parsing map with new objects to support. The inputs parsing map is a dictionary of object\ntypes as key, and a function that will handle the given input. The function must accept 1 keyword argument\n(data_item: mlrun.DataItem) and return the relevant parsed object.\nParameters\nupdates – New object types to support - a dictionary of artifact type enum as key, and a function that\nwill handle the given input to update the current map.\nclassmethod update_outputs_logging_map(updates: Dict[mlrun.run.ArtifactType, Callable[[mlrun.execution.MLClientCtx, Any, str, dict], None]])[source]#\nEnrich the outputs logging map with new artifact types to support. The outputs logging map is a dictionary of\nartifact type enum as key, and a function that will handle the given output. The function must accept 4 keyword\narguments\nctx: mlrun.MLClientCtx - The MLRun context to log with.\nobj: Any - The value / object to log.\nkey: str - The key of the artifact.\nlogging_kwargs: dict - Keyword arguments the user can pass in the instructions tuple.\nParameters\nupdates – New artifact types to support - a dictionary of artifact type enum as key, and a function that\nwill handle the given output to update the current map.\nclass mlrun.run.InputsParser[source]#\nBases: object\nA static class to hold all the common parsing functions - functions for parsing MLRun DataItem to the user desired\ntype.\nstatic parse_dict(data_item: mlrun.datastore.base.DataItem) → dict[source]#\nParse an MLRun DataItem to a dict.\nParameters\ndata_item – The DataItem to parse.\nReturns\nThe DataItem as a dict.\nstatic parse_list(data_item: mlrun.datastore.base.DataItem) → list[source]#\nParse an MLRun DataItem to a list.\nParameters\ndata_item – The DataItem to parse.\nReturns\nThe DataItem as a list.\nstatic parse_numpy_array(data_item: mlrun.datastore.base.DataItem) → numpy.ndarray[source]#\nParse an MLRun DataItem to a numpy.ndarray.\nParameters\ndata_item – The DataItem to parse.\nReturns\nThe DataItem as a numpy.ndarray.\nstatic parse_object(data_item: mlrun.datastore.base.DataItem) → object[source]#\nParse an MLRun DataItem to its unpickled object. The pickle file will be downloaded to a local temp\ndirectory and then loaded.\nParameters\ndata_item – The DataItem to parse.\nReturns\nThe DataItem as the original object that was pickled once it was logged.\nstatic parse_pandas_dataframe(data_item: mlrun.datastore.base.DataItem) → pandas.core.frame.DataFrame[source]#\nParse an MLRun DataItem to a pandas.DataFrame.\nParameters\ndata_item – The DataItem to parse.\nReturns\nThe DataItem as a pandas.DataFrame.\nclass mlrun.run.OutputsLogger[source]#\nBases: object\nA static class to hold all the common logging functions - functions for logging different objects by artifact type\nto MLRun.\nstatic log_dataset(ctx: mlrun.execution.MLClientCtx, obj: Union[pandas.core.frame.DataFrame, numpy.ndarray, pandas.core.series.Series, dict, list], key: str, logging_kwargs: dict)[source]#\nLog an object as a dataset. The dataset wil lbe cast to a pandas.DataFrame. Supporting casting from\npandas.Series, numpy.ndarray, dict and list.\nParameters\nctx – The MLRun context to log with.\nobj – The data to log.\nkey – The key of the artifact.\nlogging_kwargs – Additional keyword arguments to pass to the context.log_dataset\nRaises\nMLRunInvalidArgumentError – If the type is not supported for being cast to pandas.DataFrame.\nstatic log_directory(ctx: mlrun.execution.MLClientCtx, obj: Union[str, pathlib.Path], key: str, logging_kwargs: dict)[source]#\nLog a directory as a zip file. The zip file will be created at the current working directory. Once logged,\nit will be deleted.\nParameters\nctx – The MLRun context to log with.\nobj – The directory to zip path.\nkey – The key of the artifact.\nlogging_kwargs – Additional keyword arguments to pass to the context.log_artifact method.\nRaises\nMLRunInvalidArgumentError – In case the given path is not of a directory or do not exist.\nstatic log_file(ctx: mlrun.execution.MLClientCtx, obj: Union[str, pathlib.Path], key: str, logging_kwargs: dict)[source]#\nLog a file to MLRun.\nParameters\nctx – The MLRun context to log with.\nobj – The path of the file to log.\nkey – The key of the artifact.\nlogging_kwargs – Additional keyword arguments to pass to the context.log_artifact method.\nRaises\nMLRunInvalidArgumentError – In case the given path is not of a file or do not exist.\nstatic log_object(ctx: mlrun.execution.MLClientCtx, obj, key: str, logging_kwargs: dict)[source]#\nLog an object as a pickle.\nParameters\nctx – The MLRun context to log with.\nobj – The object to log.\nkey – The key of the artifact.\nlogging_kwargs – Additional keyword arguments to pass to the context.log_artifact method.\nstatic log_plot(ctx: mlrun.execution.MLClientCtx, obj, key: str, logging_kwargs: dict)[source]#\nLog an object as a plot. Currently, supporting plots produced by one the following modules: matplotlib,\nseaborn, plotly and bokeh.\nParameters\nctx – The MLRun context to log with.\nobj – The plot to log.\nkey – The key of the artifact.\nlogging_kwargs – Additional keyword arguments to pass to the context.log_artifact.\nRaises\nMLRunInvalidArgumentError – If the object type is not supported (meaning the plot was not produced by\none of the supported modules).\nstatic log_result(ctx: mlrun.execution.MLClientCtx, obj: Union[int, float, str, list, tuple, dict, numpy.ndarray], key: str, logging_kwargs: dict)[source]#\nLog an object as a result. The objects value will be cast to a serializable version of itself. Supporting:\nint, float, str, list, tuple, dict, numpy.ndarray\nParameters\nctx – The MLRun context to log with.\nobj – The value to log.\nkey – The key of the artifact.\nlogging_kwargs – Additional keyword arguments to pass to the context.log_result method.\nclass mlrun.run.RunStatuses[source]#\nBases: object\nstatic all()[source]#\nerror = 'Error'#\nfailed = 'Failed'#\nrunning = 'Running'#\nskipped = 'Skipped'#\nstatic stable_statuses()[source]#\nsucceeded = 'Succeeded'#\nstatic transient_statuses()[source]#\nmlrun.run.code_to_function(name: str = '', project: str = '', tag: str = '', filename: str = '', handler: str = '', kind: str = '', image: Optional[str] = None, code_output: str = '', embed_code: bool = True, description: str = '', requirements: Optional[Union[str, List[str]]] = None, categories: Optional[List[str]] = None, labels: Optional[Dict[str, str]] = None, with_doc: bool = True, ignored_tags=None) → Union[mlrun.runtimes.mpijob.v1alpha1.MpiRuntimeV1Alpha1, mlrun.runtimes.mpijob.v1.MpiRuntimeV1, mlrun.runtimes.function.RemoteRuntime, mlrun.runtimes.serving.ServingRuntime, mlrun.runtimes.daskjob.DaskCluster, mlrun.runtimes.kubejob.KubejobRuntime, mlrun.runtimes.local.LocalRuntime, mlrun.runtimes.sparkjob.spark2job.Spark2Runtime, mlrun.runtimes.sparkjob.spark3job.Spark3Runtime, mlrun.runtimes.remotesparkjob.RemoteSparkRuntime][source]#\nConvenience function to insert code and configure an mlrun runtime.\nEasiest way to construct a runtime type object. Provides the most often\nused configuration options for all runtimes as parameters.\nInstantiated runtimes are considered ‘functions’ in mlrun, but they are\nanything from nuclio functions to generic kubernetes pods to spark jobs.\nFunctions are meant to be focused, and as such limited in scope and size.\nTypically a function can be expressed in a single python module with\nadded support from custom docker images and commands for the environment.\nThe returned runtime object can be further configured if more\ncustomization is required.\nOne of the most important parameters is ‘kind’. This is what is used to\nspecify the chosen runtimes. The options are:\nlocal: execute a local python or shell script\njob: insert the code into a Kubernetes pod and execute it\nnuclio: insert the code into a real-time serverless nuclio function\nserving: insert code into orchestrated nuclio function(s) forming a DAG\ndask: run the specified python code / script as Dask Distributed job\nmpijob: run distributed Horovod jobs over the MPI job operator\nspark: run distributed Spark job using Spark Kubernetes Operator\nremote-spark: run distributed Spark job on remote Spark service\nLearn more about function runtimes here:\nhttps://docs.mlrun.org/en/latest/runtimes/functions.html#function-runtimes\nParameters\nname – function name, typically best to use hyphen-case\nproject – project used to namespace the function, defaults to ‘default’\ntag – function tag to track multiple versions of the same function, defaults to ‘latest’\nfilename – path to .py/.ipynb file, defaults to current jupyter notebook\nhandler – The default function handler to call for the job or nuclio function, in batch functions\n(job, mpijob, ..) the handler can also be specified in the .run() command, when not specified\nthe entire file will be executed (as main).\nfor nuclio functions the handler is in the form of module:function, defaults to ‘main:handler’\nkind – function runtime type string - nuclio, job, etc. (see docstring for all options)\nimage – base docker image to use for building the function container, defaults to None\ncode_output – specify ‘.’ to generate python module from the current jupyter notebook\nembed_code – indicates whether or not to inject the code directly into the function runtime spec,\ndefaults to True\ndescription – short function description, defaults to ‘’\nrequirements – list of python packages or pip requirements file path, defaults to None\ncategories – list of categories for mlrun Function Hub, defaults to None\nlabels – immutable name/value pairs to tag the function with useful metadata, defaults to None\nwith_doc – indicates whether to document the function parameters, defaults to True\nignored_tags – notebook cells to ignore when converting notebooks to py code (separated by ‘;’)\nReturns\npre-configured function object from a mlrun runtime class\nexample:\nimport mlrun\n# create job function object from notebook code and add doc/metadata\nfn = mlrun.code_to_function(\"file_utils\", kind=\"job\",\nhandler=\"open_archive\", image=\"mlrun/mlrun\",\ndescription = \"this function opens a zip archive into a local/mounted folder\",\ncategories = [\"fileutils\"],\nlabels = {\"author\": \"me\"})\nexample:\nimport mlrun\nfrom pathlib import Path\n# create file\nPath(\"mover.py\").touch()\n# create nuclio function object from python module call mover.py\nfn = mlrun.code_to_function(\"nuclio-mover\", kind=\"nuclio\",\nfilename=\"mover.py\", image=\"python:3.7\",\ndescription = \"this function moves files from one system to another\",\nrequirements = [\"pandas\"],\nlabels = {\"author\": \"me\"})\nmlrun.run.download_object(url, target, secrets=None)[source]#\ndownload mlrun dataitem (from path/url to target path)\nmlrun.run.function_to_module(code='', workdir=None, secrets=None, silent=False)[source]#\nLoad code, notebook or mlrun function as .py module\nthis function can import a local/remote py file or notebook\nor load an mlrun function object as a module, you can use this\nfrom your code, notebook, or another function (for common libs)\nNote: the function may have package requirements which must be satisfied\nexample:\nmod = mlrun.function_to_module('./examples/training.py')\ntask = mlrun.new_task(inputs={'infile.txt': '../examples/infile.txt'})\ncontext = mlrun.get_or_create_ctx('myfunc', spec=task)\nmod.my_job(context, p1=1, p2='x')\nprint(context.to_yaml())\nfn = mlrun.import_function('hub://open_archive')\nmod = mlrun.function_to_module(fn)\ndata = mlrun.run.get_dataitem(\"https://fpsignals-public.s3.amazonaws.com/catsndogs.tar.gz\")\ncontext = mlrun.get_or_create_ctx('myfunc')\nmod.open_archive(context, archive_url=data)\nprint(context.to_yaml())\nParameters\ncode – path/url to function (.py or .ipynb or .yaml)\nOR function object\nworkdir – code workdir\nsecrets – secrets needed to access the URL (e.g.s3, v3io, ..)\nsilent – do not raise on errors\nReturns\npython module\nmlrun.run.get_dataitem(url, secrets=None, db=None) → mlrun.datastore.base.DataItem[source]#\nget mlrun dataitem object (from path/url)\nmlrun.run.get_object(url, secrets=None, size=None, offset=0, db=None)[source]#\nget mlrun dataitem body (from path/url)\nmlrun.run.get_or_create_ctx(name: str, event=None, spec=None, with_env: bool = True, rundb: str = '', project: str = '', upload_artifacts=False)[source]#\ncalled from within the user program to obtain a run context\nthe run context is an interface for receiving parameters, data and logging\nrun results, the run context is read from the event, spec, or environment\n(in that order), user can also work without a context (local defaults mode)\nall results are automatically stored in the “rundb” or artifact store,\nthe path to the rundb can be specified in the call or obtained from env.\nParameters\nname – run name (will be overridden by context)\nevent – function (nuclio Event object)\nspec – dictionary holding run spec\nwith_env – look for context in environment vars, default True\nrundb – path/url to the metadata and artifact database\nproject – project to initiate the context in (by default mlrun.mlctx.default_project)\nupload_artifacts – when using local context (not as part of a job/run), upload artifacts to the\nsystem default artifact path location\nReturns\nexecution context\nExamples:\n# load MLRUN runtime context (will be set by the runtime framework e.g. KubeFlow)\ncontext = get_or_create_ctx('train')\n# get parameters from the runtime context (or use defaults)\np1 = context.get_param('p1', 1)\np2 = context.get_param('p2', 'a-string')\n# access input metadata, values, files, and secrets (passwords)\nprint(f'Run: {context.name} (uid={context.uid})')\nprint(f'Params: p1={p1}, p2={p2}')\nprint(f'accesskey = {context.get_secret(\"ACCESS_KEY\")}')\ninput_str = context.get_input('infile.txt').get()\nprint(f'file: {input_str}')\n# RUN some useful code e.g. ML training, data prep, etc.\n# log scalar result values (job result metrics)\ncontext.log_result('accuracy', p1 * 2)\ncontext.log_result('loss', p1 * 3)\ncontext.set_label('framework', 'sklearn')\n# log various types of artifacts (file, web page, table), will be versioned and visible in the UI\ncontext.log_artifact('model.txt', body=b'abc is 123', labels={'framework': 'xgboost'})\ncontext.log_artifact('results.html', body=b'<b> Some HTML <b>', viewer='web-app')\nmlrun.run.get_pipeline(run_id, namespace=None, format_: Union[str, mlrun.api.schemas.pipeline.PipelinesFormat] = PipelinesFormat.summary, project: Optional[str] = None, remote: bool = True)[source]#\nGet Pipeline status\nParameters\nrun_id – id of pipelines run\nnamespace – k8s namespace if not default\nformat – Format of the results. Possible values are:\n- summary (default value) - Return summary of the object data.\n- full - Return full pipeline object.\nproject – the project of the pipeline run\nremote – read kfp data from mlrun service (default=True)\nReturns\nkfp run dict\nmlrun.run.handler(labels: Optional[Dict[str, str]] = None, outputs: Optional[List[Optional[Union[Tuple[str, mlrun.run.ArtifactType], Tuple[str, str], Tuple[str, mlrun.run.ArtifactType, Dict[str, Any]], Tuple[str, str, Dict[str, Any]], str]]]] = None, inputs: Union[bool, Dict[str, Type]] = True)[source]#\nMLRun’s handler is a decorator to wrap a function and enable setting labels, automatic mlrun.DataItem parsing and\noutputs logging.\nParameters\nlabels – Labels to add to the run. Expecting a dictionary with the labels names as keys. Default: None.\noutputs – Logging configurations for the function’s returned values. Expecting a list of tuples and None\nvalues:\nstr - A string in the format of ‘{key}:{artifact_type}’. If a string was given without ‘:’ it willindicate the key and the artifact type will be according to the returned value\ntype.\ntuple - A tuple of:\n[0]: str - The key (name) of the artifact to use for the logged output.\n[1]: Union[ArtifactType, str] = “result” - An ArtifactType enum or an equivalent\nstring, that indicates how to log the returned value. The artifact types can be one of:\nDATASET = “dataset”\nDIRECTORY = “directory”\nFILE = “file”\nOBJECT = “object”\nPLOT = “plot”\nRESULT = “result”.\n[2]: Optional[Dict[str, Any]] - A keyword arguments dictionary with the properties to pass to\nthe relevant logging function (one of context.log_artifact, context.log_result,\ncontext.log_dataset).\nNone - Do not log the output.\nThe list length must be equal to the total amount of returned values from the function. Default is\nNone - meaning no outputs will be logged.\ninputs – Parsing configurations for the arguments passed as inputs via the run method of an MLRun function.\nCan be passed as a boolean value or a dictionary:\nTrue - Parse all found inputs to the assigned type hint in the function’s signature. If there is notype hint assigned, the value will remain an mlrun.DataItem.\nFalse - Do not parse inputs, leaving the inputs as mlrun.DataItem.\nDict[str, Type] - A dictionary with argument name as key and the expected type to parse themlrun.DataItem to.\nDefault: True.\nExample:\nimport mlrun\n@mlrun.handler(outputs=[\"my_array\", None, \"my_multiplier\"])\ndef my_handler(array: np.ndarray, m: int):\narray = array * m\nm += 1\nreturn array, \"I won't be logged\", m\n>>> mlrun_function = mlrun.code_to_function(\"my_code.py\", kind=\"job\")\n>>> run_object = mlrun_function.run(\n...     handler=\"my_handler\",\n...     inputs={\"array\": \"store://my_array_Artifact\"},\n...     params={\"m\": 2}\n... )\n>>> run_object.outputs\n{'my_multiplier': 3, 'my_array': 'store://...'}\nmlrun.run.import_function(url='', secrets=None, db='', project=None, new_name=None)[source]#\nCreate function object from DB or local/remote YAML file\nFunctions can be imported from function repositories (mlrun Function Hub (formerly Marketplace) or local db),\nor be read from a remote URL (http(s), s3, git, v3io, ..) containing the function YAML\nspecial URLs:\nfunction hub: hub://{name}[:{tag}]\nlocal mlrun db:       db://{project-name}/{name}[:{tag}]\nexamples:\nfunction = mlrun.import_function(\"hub://sklearn_classifier\")\nfunction = mlrun.import_function(\"./func.yaml\")\nfunction = mlrun.import_function(\"https://raw.githubusercontent.com/org/repo/func.yaml\")\nParameters\nurl – path/url to Function Hub, db or function YAML file\nsecrets – optional, credentials dict for DB or URL (s3, v3io, …)\ndb – optional, mlrun api/db path\nproject – optional, target project for the function\nnew_name – optional, override the imported function name\nReturns\nfunction object\nmlrun.run.import_function_to_dict(url, secrets=None)[source]#\nLoad function spec from local/remote YAML file\nmlrun.run.list_pipelines(full=False, page_token='', page_size=None, sort_by='', filter_='', namespace=None, project='*', format_: mlrun.api.schemas.pipeline.PipelinesFormat = PipelinesFormat.metadata_only) → Tuple[int, Optional[int], List[dict]][source]#\nList pipelines\nParameters\nfull – Deprecated, use format_ instead. if True will set format_ to full, otherwise format_ will\nbe used\npage_token – A page token to request the next page of results. The token is acquired from the nextPageToken\nfield of the response from the previous call or can be omitted when fetching the first page.\npage_size – The number of pipelines to be listed per page. If there are more pipelines than this number, the\nresponse message will contain a nextPageToken field you can use to fetch the next page.\nsort_by – Can be format of “field_name”, “field_name asc” or “field_name desc” (Example, “name asc”\nor “id desc”). Ascending by default.\nfilter – A url-encoded, JSON-serialized Filter protocol buffer, see:\n[filter.proto](https://github.com/kubeflow/pipelines/ blob/master/backend/api/filter.proto).\nnamespace – Kubernetes namespace if other than default\nproject – Can be used to retrieve only specific project pipelines. “*” for all projects. Note that\nfiltering by project can’t be used together with pagination, sorting, or custom filter.\nformat – Control what will be returned (full/metadata_only/name_only)\nmlrun.run.load_func_code(command='', workdir=None, secrets=None, name='name')[source]#\nmlrun.run.new_function(name: str = '', project: str = '', tag: str = '', kind: str = '', command: str = '', image: str = '', args: Optional[list] = None, runtime=None, mode=None, handler: Optional[str] = None, source: Optional[str] = None, requirements: Optional[Union[str, List[str]]] = None, kfp=None)[source]#\nCreate a new ML function from base properties\nexample:\n# define a container based function (the `training.py` must exist in the container workdir)\nf = new_function(command='training.py -x {x}', image='myrepo/image:latest', kind='job')\nf.run(params={\"x\": 5})\n# define a container based function which reads its source from a git archive\nf = new_function(command='training.py -x {x}', image='myrepo/image:latest', kind='job',\nsource='git://github.com/mlrun/something.git')\nf.run(params={\"x\": 5})\n# define a local handler function (execute a local function handler)\nf = new_function().run(task, handler=myfunction)\nParameters\nname – function name\nproject – function project (none for ‘default’)\ntag – function version tag (none for ‘latest’)\nkind – runtime type (local, job, nuclio, spark, mpijob, dask, ..)\ncommand – command/url + args (e.g.: training.py –verbose)\nimage – container image (start with ‘.’ for default registry)\nargs – command line arguments (override the ones in command)\nruntime – runtime (job, nuclio, spark, dask ..) object/dict\nstore runtime specific details and preferences\nmode –\nruntime mode, “args” mode will push params into command template, example:command=`mycode.py –x {xparam}` will substitute the {xparam} with the value of the xparam param\n”pass” mode will run the command as is in the container (not wrapped by mlrun), the command can use{} for parameters like in the “args” mode\nhandler – The default function handler to call for the job or nuclio function, in batch functions\n(job, mpijob, ..) the handler can also be specified in the .run() command, when not specified\nthe entire file will be executed (as main).\nfor nuclio functions the handler is in the form of module:function, defaults to “main:handler”\nsource – valid path to git, zip, or tar file, e.g. git://github.com/mlrun/something.git,\nhttp://some/url/file.zip\nrequirements – list of python packages or pip requirements file path, defaults to None\nkfp – reserved, flag indicating running within kubeflow pipeline\nReturns\nfunction object\nmlrun.run.run_local(task=None, command='', name: str = '', args: Optional[list] = None, workdir=None, project: str = '', tag: str = '', secrets=None, handler=None, params: Optional[dict] = None, inputs: Optional[dict] = None, artifact_path: str = '', mode: Optional[str] = None, allow_empty_resources=None)[source]#\nRun a task on function/code (.py, .ipynb or .yaml) locally,\nexample:\n# define a task\ntask = new_task(params={'p1': 8}, out_path=out_path)\n# run\nrun = run_local(spec, command='src/training.py', workdir='src')\nor specify base task parameters (handler, params, ..) in the call:\nrun = run_local(handler=my_function, params={'x': 5})\nParameters\ntask – task template object or dict (see RunTemplate)\ncommand – command/url/function\nname – ad hook function name\nargs – command line arguments (override the ones in command)\nworkdir – working dir to exec in\nproject – function project (none for ‘default’)\ntag – function version tag (none for ‘latest’)\nsecrets – secrets dict if the function source is remote (s3, v3io, ..)\nhandler – pointer or name of a function handler\nparams – input parameters (dict)\ninputs – input objects (dict of key: path)\nartifact_path – default artifact output path\nReturns\nrun object\nmlrun.run.run_pipeline(pipeline, arguments=None, project=None, experiment=None, run=None, namespace=None, artifact_path=None, ops=None, url=None, ttl=None, remote: bool = True)[source]#\nremote KubeFlow pipeline execution\nSubmit a workflow task to KFP via mlrun API service\nParameters\npipeline – KFP pipeline function or path to .yaml/.zip pipeline file\narguments – pipeline arguments\nproject – name of project\nexperiment – experiment name\nrun – optional, run name\nnamespace – Kubernetes namespace (if not using default)\nurl – optional, url to mlrun API service\nartifact_path – target location/url for mlrun artifacts\nops – additional operators (.apply() to all pipeline functions)\nttl – pipeline ttl in secs (after that the pods will be removed)\nremote – read kfp data from mlrun service (default=True)\nReturns\nkubeflow pipeline id\nmlrun.run.wait_for_pipeline_completion(run_id, timeout=3600, expected_statuses: Optional[List[str]] = None, namespace=None, remote=True, project: Optional[str] = None)[source]#\nWait for Pipeline status, timeout in sec\nParameters\nrun_id – id of pipelines run\ntimeout – wait timeout in sec\nexpected_statuses – list of expected statuses, one of [ Succeeded | Failed | Skipped | Error ], by default\n[ Succeeded ]\nnamespace – k8s namespace if not default\nremote – read kfp data from mlrun service (default=True)\nproject – the project of the pipeline\nReturns\nkfp run dict\nmlrun.run.wait_for_runs_completion(runs: list, sleep=3, timeout=0, silent=False)[source]#\nwait for multiple runs to complete\nNote: need to use watch=False in .run() so the run will not wait for completion\nexample:\n# run two training functions in parallel and wait for the results\ninputs = {'dataset': cleaned_data}\nrun1 = train.run(name='train_lr', inputs=inputs, watch=False,\nparams={'model_pkg_class': 'sklearn.linear_model.LogisticRegression',\n'label_column': 'label'})\nrun2 = train.run(name='train_lr', inputs=inputs, watch=False,\nparams={'model_pkg_class': 'sklearn.ensemble.RandomForestClassifier',\n'label_column': 'label'})\ncompleted = wait_for_runs_completion([run1, run2])\nParameters\nruns – list of run objects (the returned values of function.run())\nsleep – time to sleep between checks (in seconds)\ntimeout – maximum time to wait in seconds (0 for unlimited)\nsilent – set to True for silent exit on timeout\nReturns\nlist of completed runs"}
{"text": "mlrun.runtimes#\nclass mlrun.runtimes.BaseRuntime(metadata=None, spec=None)[source]#\nBases: mlrun.model.ModelObj\nas_step(runspec: Optional[mlrun.model.RunObject] = None, handler=None, name: str = '', project: str = '', params: Optional[dict] = None, hyperparams=None, selector='', hyper_param_options: Optional[mlrun.model.HyperParamOptions] = None, inputs: Optional[dict] = None, outputs: Optional[dict] = None, workdir: str = '', artifact_path: str = '', image: str = '', labels: Optional[dict] = None, use_db=True, verbose=None, scrape_metrics=False)[source]#\nRun a local or remote task.\nParameters\nrunspec – run template object or dict (see RunTemplate)\nhandler – name of the function handler\nname – execution name\nproject – project name\nparams – input parameters (dict)\nhyperparams – hyper parameters\nselector – selection criteria for hyper params\nhyper_param_options – hyper param options (selector, early stop, strategy, ..)\nsee: HyperParamOptions\ninputs – input objects (dict of key: path)\noutputs – list of outputs which can pass in the workflow\nartifact_path – default artifact output path (replace out_path)\nworkdir – default input artifacts path\nimage – container image to use\nlabels – labels to tag the job/run with ({key:val, ..})\nuse_db – save function spec in the db (vs the workflow file)\nverbose – add verbose prints/logs\nscrape_metrics – whether to add the mlrun/scrape-metrics label to this run’s resources\nReturns\nKubeFlow containerOp\nclean_build_params()[source]#\ndoc()[source]#\nexport(target='', format='.yaml', secrets=None, strip=True)[source]#\nsave function spec to a local/remote path (default to./function.yaml)\nParameters\ntarget – target path/url\nformat – .yaml (default) or .json\nsecrets – optional secrets dict/object for target path (e.g. s3)\nstrip – strip status data\nReturns\nself\nfill_credentials()[source]#\nfull_image_path(image=None, client_version: Optional[str] = None)[source]#\nis_deployed()[source]#\nkind = 'base'#\nproperty metadata: mlrun.model.BaseMetadata#\nrun(runspec: Optional[mlrun.model.RunObject] = None, handler=None, name: str = '', project: str = '', params: Optional[dict] = None, inputs: Optional[Dict[str, str]] = None, out_path: str = '', workdir: str = '', artifact_path: str = '', watch: bool = True, schedule: Optional[Union[str, mlrun.api.schemas.schedule.ScheduleCronTrigger]] = None, hyperparams: Optional[Dict[str, list]] = None, hyper_param_options: Optional[mlrun.model.HyperParamOptions] = None, verbose=None, scrape_metrics: Optional[bool] = None, local=False, local_code_path=None, auto_build=None) → mlrun.model.RunObject[source]#\nRun a local or remote task.\nParameters\nrunspec – run template object or dict (see RunTemplate)\nhandler – pointer or name of a function handler\nname – execution name\nproject – project name\nparams – input parameters (dict)\ninputs – input objects (dict of key: path)\nout_path – default artifact output path\nartifact_path – default artifact output path (will replace out_path)\nworkdir – default input artifacts path\nwatch – watch/follow run log\nschedule – ScheduleCronTrigger class instance or a standard crontab expression string\n(which will be converted to the class using its from_crontab constructor),\nsee this link for help:\nhttps://apscheduler.readthedocs.io/en/v3.6.3/modules/triggers/cron.html#module-apscheduler.triggers.cron\nhyperparams – dict of param name and list of values to be enumerated e.g. {“p1”: [1,2,3]}\nthe default strategy is grid search, can specify strategy (grid, list, random)\nand other options in the hyper_param_options parameter\nhyper_param_options – dict or HyperParamOptions struct of\nhyper parameter options\nverbose – add verbose prints/logs\nscrape_metrics – whether to add the mlrun/scrape-metrics label to this run’s resources\nlocal – run the function locally vs on the runtime/cluster\nlocal_code_path – path of the code for local runs & debug\nauto_build – when set to True and the function require build it will be built on the first\nfunction run, use only if you dont plan on changing the build config between runs\nReturns\nrun context object (RunObject) with run metadata, results and status\nsave(tag='', versioned=False, refresh=False) → str[source]#\nset_db_connection(conn, is_api=False)[source]#\nset_label(key, value)[source]#\nproperty spec: mlrun.runtimes.base.FunctionSpec#\nproperty status: mlrun.runtimes.base.FunctionStatus#\nstore_run(runobj: mlrun.model.RunObject)[source]#\nto_dict(fields=None, exclude=None, strip=False)[source]#\nconvert the object to a python dictionary\ntry_auto_mount_based_on_config()[source]#\nproperty uri#\nvalidate_and_enrich_service_account(allowed_service_account, default_service_account)[source]#\nverify_base_image()[source]#\nwith_code(from_file='', body=None, with_doc=True)[source]#\nUpdate the function code\nThis function eliminates the need to build container images every time we edit the code\nParameters\nfrom_file – blank for current notebook, or path to .py/.ipynb file\nbody – will use the body as the function code\nwith_doc – update the document of the function parameters\nReturns\nfunction object\nwith_commands(commands: List[str], overwrite: bool = False, verify_base_image: bool = True)[source]#\nadd commands to build spec.\nParameters\ncommands – list of commands to run during build\nReturns\nfunction object\nwith_requirements(requirements: Union[str, List[str]], overwrite: bool = False, verify_base_image: bool = True)[source]#\nadd package requirements from file or list to build spec.\nParameters\nrequirements – python requirements file path or list of packages\noverwrite – overwrite existing requirements\nverify_base_image – verify that the base image is configured\nReturns\nfunction object\nclass mlrun.runtimes.DaskCluster(spec=None, metadata=None)[source]#\nBases: mlrun.runtimes.kubejob.KubejobRuntime\nproperty client#\nclose(running=True)[source]#\ncluster()[source]#\ndeploy(watch=True, with_mlrun=None, skip_deployed=False, is_kfp=False, mlrun_version_specifier=None, show_on_failure: bool = False)[source]#\ndeploy function, build container with dependencies\nParameters\nwatch – wait for the deploy to complete (and print build logs)\nwith_mlrun – add the current mlrun package to the container build\nskip_deployed – skip the build if we already have an image for the function\nmlrun_version_specifier – which mlrun package version to include (if not current)\nbuilder_env – Kaniko builder pod env vars dict (for config/credentials)\ne.g. builder_env={“GIT_TOKEN”: token}\nshow_on_failure – show logs only in case of build failure\n:return True if the function is ready (deployed)\nget_status()[source]#\ngpus(gpus, gpu_type='nvidia.com/gpu')[source]#\nproperty initialized#\nis_deployed()[source]#\ncheck if the function is deployed (have a valid container)\nkind = 'dask'#\nproperty spec: mlrun.runtimes.daskjob.DaskSpec#\nproperty status: mlrun.runtimes.daskjob.DaskStatus#\nwith_limits(mem=None, cpu=None, gpus=None, gpu_type='nvidia.com/gpu')[source]#\nset pod cpu/memory/gpu limits\nby default it overrides the whole limits section, if you wish to patch specific resources use patch=True.\nwith_requests(mem=None, cpu=None)[source]#\nset requested (desired) pod cpu/memory resources\nby default it overrides the whole requests section, if you wish to patch specific resources use patch=True.\nwith_scheduler_limits(mem: Optional[str] = None, cpu: Optional[str] = None, gpus: Optional[int] = None, gpu_type: str = 'nvidia.com/gpu', patch: bool = False)[source]#\nset scheduler pod resources limits\nby default it overrides the whole limits section, if you wish to patch specific resources use patch=True.\nwith_scheduler_requests(mem: Optional[str] = None, cpu: Optional[str] = None, patch: bool = False)[source]#\nset scheduler pod resources requests\nby default it overrides the whole requests section, if you wish to patch specific resources use patch=True.\nwith_worker_limits(mem: Optional[str] = None, cpu: Optional[str] = None, gpus: Optional[int] = None, gpu_type: str = 'nvidia.com/gpu', patch: bool = False)[source]#\nset worker pod resources limits\nby default it overrides the whole limits section, if you wish to patch specific resources use patch=True.\nwith_worker_requests(mem: Optional[str] = None, cpu: Optional[str] = None, patch: bool = False)[source]#\nset worker pod resources requests\nby default it overrides the whole requests section, if you wish to patch specific resources use patch=True.\nclass mlrun.runtimes.HandlerRuntime(metadata=None, spec=None)[source]#\nBases: mlrun.runtimes.base.BaseRuntime, mlrun.runtimes.local.ParallelRunner\nkind = 'handler'#\nclass mlrun.runtimes.KubejobRuntime(spec=None, metadata=None)[source]#\nBases: mlrun.runtimes.pod.KubeResource\nbuild_config(image='', base_image=None, commands: Optional[list] = None, secret=None, source=None, extra=None, load_source_on_run=None, with_mlrun=None, auto_build=None, requirements=None, overwrite=False, verify_base_image=True)[source]#\nspecify builder configuration for the deploy operation\nParameters\nimage – target image name/path\nbase_image – base image name/path\ncommands – list of docker build (RUN) commands e.g. [‘pip install pandas’]\nsecret – k8s secret for accessing the docker registry\nsource – source git/tar archive to load code from in to the context/workdir\ne.g. git://github.com/mlrun/something.git#development\nextra – extra Dockerfile lines\nload_source_on_run – load the archive code into the container at runtime vs at build time\nwith_mlrun – add the current mlrun package to the container build\nauto_build – when set to True and the function require build it will be built on the first\nfunction run, use only if you dont plan on changing the build config between runs\nrequirements – requirements.txt file to install or list of packages to install\noverwrite – overwrite existing build configuration\nwhen False we merge the new params with the existing (currently merge is applied to requirements and commands)\nwhen True we replace the existing params with the new ones\nParameters\nverify_base_image – verify the base image is set\nbuilder_status(watch=True, logs=True)[source]#\ndeploy(watch=True, with_mlrun=None, skip_deployed=False, is_kfp=False, mlrun_version_specifier=None, builder_env: Optional[dict] = None, show_on_failure: bool = False) → bool[source]#\ndeploy function, build container with dependencies\nParameters\nwatch – wait for the deploy to complete (and print build logs)\nwith_mlrun – add the current mlrun package to the container build\nskip_deployed – skip the build if we already have an image for the function\nmlrun_version_specifier – which mlrun package version to include (if not current)\nbuilder_env – Kaniko builder pod env vars dict (for config/credentials)\ne.g. builder_env={“GIT_TOKEN”: token}\nshow_on_failure – show logs only in case of build failure\n:return True if the function is ready (deployed)\ndeploy_step(image=None, base_image=None, commands: Optional[list] = None, secret_name='', with_mlrun=True, skip_deployed=False)[source]#\nis_deployed()[source]#\ncheck if the function is deployed (have a valid container)\nkind = 'job'#\nwith_source_archive(source, workdir=None, handler=None, pull_at_runtime=True)[source]#\nload the code from git/tar/zip archive at runtime or build\nParameters\nsource – valid path to git, zip, or tar file, e.g.\ngit://github.com/mlrun/something.git\nhttp://some/url/file.zip\nhandler – default function handler\nworkdir – working dir relative to the archive root or absolute (e.g. ‘./subdir’)\npull_at_runtime – load the archive into the container at job runtime vs on build/deploy\nclass mlrun.runtimes.LocalRuntime(metadata=None, spec=None)[source]#\nBases: mlrun.runtimes.base.BaseRuntime, mlrun.runtimes.local.ParallelRunner\nis_deployed()[source]#\nkind = 'local'#\nproperty spec: mlrun.runtimes.local.LocalFunctionSpec#\nto_job(image='')[source]#\nwith_source_archive(source, workdir=None, handler=None, target_dir=None)[source]#\nload the code from git/tar/zip archive at runtime or build\nParameters\nsource – valid path to git, zip, or tar file, e.g.\ngit://github.com/mlrun/something.git\nhttp://some/url/file.zip\nhandler – default function handler\nworkdir – working dir relative to the archive root or absolute (e.g. ‘./subdir’)\ntarget_dir – local target dir for repo clone (by default its <current-dir>/code)\nclass mlrun.runtimes.RemoteRuntime(spec=None, metadata=None)[source]#\nBases: mlrun.runtimes.pod.KubeResource\nadd_secrets_config_to_spec()[source]#\nadd_trigger(name, spec)[source]#\nadd a nuclio trigger object/dict\nParameters\nname – trigger name\nspec – trigger object or dict\nadd_v3io_stream_trigger(stream_path, name='stream', group='serving', seek_to='earliest', shards=1, extra_attributes=None, ack_window_size=None, **kwargs)[source]#\nadd v3io stream trigger to the function\nParameters\nstream_path – v3io stream path (e.g. ‘v3io:///projects/myproj/stream1’)\nname – trigger name\ngroup – consumer group\nseek_to – start seek from: “earliest”, “latest”, “time”, “sequence”\nshards – number of shards (used to set number of replicas)\nextra_attributes – key/value dict with extra trigger attributes\nack_window_size – stream ack window size (the consumer group will be updated with the\nevent id - ack_window_size, on failure the events in the window will be retransmitted)\nkwargs – extra V3IOStreamTrigger class attributes\nadd_volume(local, remote, name='fs', access_key='', user='')[source]#\ndeploy(dashboard='', project='', tag='', verbose=False, auth_info: Optional[mlrun.api.schemas.auth.AuthInfo] = None, builder_env: Optional[dict] = None)[source]#\nDeploy the nuclio function to the cluster\nParameters\ndashboard – address of the nuclio dashboard service (keep blank for current cluster)\nproject – project name\ntag – function tag\nverbose – set True for verbose logging\nauth_info – service AuthInfo\nbuilder_env – env vars dict for source archive config/credentials e.g. builder_env={“GIT_TOKEN”: token}\ndeploy_step(dashboard='', project='', models=None, env=None, tag=None, verbose=None, use_function_from_db=None)[source]#\nreturn as a Kubeflow pipeline step (ContainerOp), recommended to use mlrun.deploy_function() instead\nfrom_image(image)[source]#\ninvoke(path: str, body: Optional[Union[str, bytes, dict]] = None, method: Optional[str] = None, headers: Optional[dict] = None, dashboard: str = '', force_external_address: bool = False, auth_info: Optional[mlrun.api.schemas.auth.AuthInfo] = None)[source]#\nInvoke the remote (live) function and return the results\nexample:\nfunction.invoke(\"/api\", body={\"inputs\": x})\nParameters\npath – request sub path (e.g. /images)\nbody – request body (str, bytes or a dict for json requests)\nmethod – HTTP method (GET, PUT, ..)\nheaders – key/value dict with http headers\ndashboard – nuclio dashboard address\nforce_external_address – use the external ingress URL\nauth_info – service AuthInfo\nkind = 'remote'#\nset_config(key, value)[source]#\nproperty spec: mlrun.runtimes.function.NuclioSpec#\nproperty status: mlrun.runtimes.function.NuclioStatus#\nwith_annotations(annotations: dict)[source]#\nset a key/value annotations for function\nwith_http(workers=8, port=0, host=None, paths=None, canary=None, secret=None, worker_timeout: Optional[int] = None, gateway_timeout: Optional[int] = None, trigger_name=None, annotations=None, extra_attributes=None)[source]#\nupdate/add nuclio HTTP trigger settings\nNote: gateway timeout is the maximum request time before an error is returned, while the worker timeout\nif the max time a request will wait for until it will start processing, gateway_timeout must be greater than\nthe worker_timeout.\nParameters\nworkers – number of worker processes (default=8)\nport – TCP port\nhost – hostname\npaths – list of sub paths\ncanary – k8s ingress canary (% traffic value between 0 to 100)\nsecret – k8s secret name for SSL certificate\nworker_timeout – worker wait timeout in sec (how long a message should wait in the worker queue\nbefore an error is returned)\ngateway_timeout – nginx ingress timeout in sec (request timeout, when will the gateway return an error)\ntrigger_name – alternative nuclio trigger name\nannotations – key/value dict of ingress annotations\nextra_attributes – key/value dict of extra nuclio trigger attributes\nReturns\nfunction object (self)\nwith_node_selection(**kwargs)[source]#\nEnables to control on which k8s node the job will run\nParameters\nnode_name – The name of the k8s node\nnode_selector – Label selector, only nodes with matching labels will be eligible to be picked\naffinity – Expands the types of constraints you can express - see\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\nfor details\ntolerations – Tolerations are applied to pods, and allow (but do not require) the pods to schedule\nonto nodes with matching taints - see\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration\nfor details\nwith_preemption_mode(**kwargs)[source]#\nPreemption mode controls whether pods can be scheduled on preemptible nodes.\nTolerations, node selector, and affinity are populated on preemptible nodes corresponding to the function spec.\nThe supported modes are:\nallow - The function can be scheduled on preemptible nodes\nconstrain - The function can only run on preemptible nodes\nprevent - The function cannot be scheduled on preemptible nodes\nnone - No preemptible configuration will be applied on the function\nThe default preemption mode is configurable in mlrun.mlconf.function_defaults.preemption_mode,\nby default it’s set to prevent\nParameters\nmode – allow | constrain | prevent | none defined in PreemptionModes\nwith_priority_class(**kwargs)[source]#\nEnables to control the priority of the pod\nIf not passed - will default to mlrun.mlconf.default_function_priority_class_name\nParameters\nname – The name of the priority class\nwith_source_archive(source, workdir=None, handler=None, runtime='')[source]#\nLoad nuclio function from remote source\nNote: remote source may require credentials, those can be stored in the project secrets or passed\nin the function.deploy() using the builder_env dict, see the required credentials per source:\nv3io - “V3IO_ACCESS_KEY”.\ngit - “GIT_USERNAME”, “GIT_PASSWORD”.\nAWS S3 - “AWS_ACCESS_KEY_ID”, “AWS_SECRET_ACCESS_KEY” or “AWS_SESSION_TOKEN”.\nParameters\nsource – a full path to the nuclio function source (code entry) to load the function from\nhandler – a path to the function’s handler, including path inside archive/git repo\nworkdir – working dir  relative to the archive root (e.g. ‘subdir’)\nruntime – (optional) the runtime of the function (defaults to python:3.7)\nExamples\ngit:\nfn.with_source_archive(\"git://github.com/org/repo#my-branch\",\nhandler=\"main:handler\",\nworkdir=\"path/inside/repo\")\ns3:\nfn.spec.nuclio_runtime = \"golang\"\nfn.with_source_archive(\"s3://my-bucket/path/in/bucket/my-functions-archive\",\nhandler=\"my_func:Handler\",\nworkdir=\"path/inside/functions/archive\",\nruntime=\"golang\")\nwith_v3io(local='', remote='')[source]#\nAdd v3io volume to the function\nParameters\nlocal – local path (mount path inside the function container)\nremote – v3io path\nclass mlrun.runtimes.RemoteSparkRuntime(spec=None, metadata=None)[source]#\nBases: mlrun.runtimes.kubejob.KubejobRuntime\ndefault_image = '.remote-spark-default-image'#\ndeploy(watch=True, with_mlrun=None, skip_deployed=False, is_kfp=False, mlrun_version_specifier=None, show_on_failure: bool = False)[source]#\ndeploy function, build container with dependencies\nParameters\nwatch – wait for the deploy to complete (and print build logs)\nwith_mlrun – add the current mlrun package to the container build\nskip_deployed – skip the build if we already have an image for the function\nmlrun_version_specifier – which mlrun package version to include (if not current)\nbuilder_env – Kaniko builder pod env vars dict (for config/credentials)\ne.g. builder_env={“GIT_TOKEN”: token}\nshow_on_failure – show logs only in case of build failure\n:return True if the function is ready (deployed)\nclassmethod deploy_default_image()[source]#\nis_deployed()[source]#\ncheck if the function is deployed (have a valid container)\nkind = 'remote-spark'#\nproperty spec: mlrun.runtimes.remotesparkjob.RemoteSparkSpec#\nwith_security_context(security_context: kubernetes.client.models.v1_security_context.V1SecurityContext)[source]#\nWith security context is not supported for spark runtime.\nDriver / Executor processes run with uid / gid 1000 as long as security context is not defined.\nIf in the future we want to support setting security context it will work only from spark version 3.2 onwards.\nwith_spark_service(spark_service, provider='iguazio')[source]#\nAttach spark service to function\nclass mlrun.runtimes.ServingRuntime(spec=None, metadata=None)[source]#\nBases: mlrun.runtimes.function.RemoteRuntime\nMLRun Serving Runtime\nadd_child_function(name, url=None, image=None, requirements=None, kind=None)[source]#\nin a multi-function pipeline add child function\nexample:\nfn.add_child_function('enrich', './enrich.ipynb', 'mlrun/mlrun')\nParameters\nname – child function name\nurl – function/code url, support .py, .ipynb, .yaml extensions\nimage – base docker image for the function\nrequirements – py package requirements file path OR list of packages\nkind – mlrun function/runtime kind\n:return function object\nadd_model(key: str, model_path: Optional[str] = None, class_name: Optional[str] = None, model_url: Optional[str] = None, handler: Optional[str] = None, router_step: Optional[str] = None, child_function: Optional[str] = None, **class_args)[source]#\nadd ml model and/or route to the function.\nExample, create a function (from the notebook), add a model class, and deploy:\nfn = code_to_function(kind='serving')\nfn.add_model('boost', model_path, model_class='MyClass', my_arg=5)\nfn.deploy()\nonly works with router topology, for nested topologies (model under router under flow)\nneed to add router to flow and use router.add_route()\nParameters\nkey – model api key (or name:version), will determine the relative url/path\nmodel_path – path to mlrun model artifact or model directory file/object path\nclass_name – V2 Model python class name or a model class instance\n(can also module.submodule.class and it will be imported automatically)\nmodel_url – url of a remote model serving endpoint (cannot be used with model_path)\nhandler – for advanced users!, override default class handler name (do_event)\nrouter_step – router step name (to determine which router we add the model to in graphs\nwith multiple router steps)\nchild_function – child function name, when the model runs in a child function\nclass_args – extra kwargs to pass to the model serving class __init__\n(can be read in the model using .get_param(key) method)\nadd_secrets_config_to_spec()[source]#\ndeploy(dashboard='', project='', tag='', verbose=False, auth_info: Optional[mlrun.api.schemas.auth.AuthInfo] = None, builder_env: Optional[dict] = None)[source]#\ndeploy model serving function to a local/remote cluster\nParameters\ndashboard – remote nuclio dashboard url (blank for local or auto detection)\nproject – optional, override function specified project name\ntag – specify unique function tag (a different function service is created for every tag)\nverbose – verbose logging\nauth_info – The auth info to use to communicate with the Nuclio dashboard, required only when providing\ndashboard\nbuilder_env – env vars dict for source archive config/credentials e.g. builder_env={“GIT_TOKEN”: token}\nkind = 'serving'#\nplot(filename=None, format=None, source=None, **kw)[source]#\nplot/save graph using graphviz\nexample:\nserving_fn = mlrun.new_function(\"serving\", image=\"mlrun/mlrun\", kind=\"serving\")\nserving_fn.add_model('my-classifier',model_path=model_path,\nclass_name='mlrun.frameworks.sklearn.SklearnModelServer')\nserving_fn.plot(rankdir=\"LR\")\nParameters\nfilename – target filepath for the image (None for the notebook)\nformat – The output format used for rendering ('pdf', 'png', etc.)\nsource – source step to add to the graph\nkw – kwargs passed to graphviz, e.g. rankdir=”LR” (see: https://graphviz.org/doc/info/attrs.html)\nReturns\ngraphviz graph object\nremove_states(keys: list)[source]#\nremove one, multiple, or all states/models from the spec (blank list for all)\nset_topology(topology=None, class_name=None, engine=None, exist_ok=False, **class_args) → Union[mlrun.serving.states.RootFlowStep, mlrun.serving.states.RouterStep][source]#\nset the serving graph topology (router/flow) and root class or params\nexamples:\n# simple model router topology\ngraph = fn.set_topology(\"router\")\nfn.add_model(name, class_name=\"ClassifierModel\", model_path=model_uri)\n# async flow topology\ngraph = fn.set_topology(\"flow\", engine=\"async\")\ngraph.to(\"MyClass\").to(name=\"to_json\", handler=\"json.dumps\").respond()\ntopology options are:\nrouter - root router + multiple child route states/models\nroute is usually determined by the path (route key/name)\ncan specify special router class and router arguments\nflow   - workflow (DAG) with a chain of states\nflow support \"sync\" and \"async\" engines, branches are not allowed in sync mode\nwhen using async mode calling state.respond() will mark the state as the\none which generates the (REST) call response\nParameters\ntopology –\ngraph topology, router or flow\nclass_name –\noptional for router, router class name/path or router object\nengine –\noptional for flow, sync or async engine (default to async)\nexist_ok –\nallow overriding existing topology\nclass_args –\noptional, router/flow class init args\n:return graph object (fn.spec.graph)\nset_tracking(stream_path: Optional[str] = None, batch: Optional[int] = None, sample: Optional[int] = None, stream_args: Optional[dict] = None, tracking_policy: Optional[Union[mlrun.utils.model_monitoring.TrackingPolicy, dict]] = None)[source]#\nset tracking parameters:\nParameters\nstream_path – Path/url of the tracking stream e.g. v3io:///users/mike/mystream\nyou can use the “dummy://” path for test/simulation.\nbatch – Micro batch size (send micro batches of N records at a time).\nsample – Sample size (send only one of N records).\nstream_args – Stream initialization parameters, e.g. shards, retention_in_hours, ..\ntracking_policy – Tracking policy object or a dictionary that will be converted into a tracking policy\nobject. By using TrackingPolicy, the user can apply his model monitoring requirements,\nsuch as setting the scheduling policy of the model monitoring batch job or changing\nthe image of the model monitoring stream.\nexample:\n# initialize a new serving function\nserving_fn = mlrun.import_function(\"hub://v2_model_server\", new_name=\"serving\")\n# apply model monitoring and set monitoring batch job to run every 3 hours\ntracking_policy = {'default_batch_intervals':\"0 */3 * * *\"}\nserving_fn.set_tracking(tracking_policy=tracking_policy)\nproperty spec: mlrun.runtimes.serving.ServingSpec#\nto_mock_server(namespace=None, current_function='*', track_models=False, workdir=None, **kwargs) → mlrun.serving.server.GraphServer[source]#\ncreate mock server object for local testing/emulation\nParameters\nnamespace – one or list of namespaces/modules to search the steps classes/functions in\nlog_level – log level (error | info | debug)\ncurrent_function – specify if you want to simulate a child function, * for all functions\ntrack_models – allow model tracking (disabled by default in the mock server)\nworkdir – working directory to locate the source code (if not the current one)\nwith_secrets(kind, source)[source]#\nregister a secrets source (file, env or dict)\nread secrets from a source provider to be used in workflows, example:\ntask.with_secrets('file', 'file.txt')\ntask.with_secrets('inline', {'key': 'val'})\ntask.with_secrets('env', 'ENV1,ENV2')\ntask.with_secrets('vault', ['secret1', 'secret2'...])\n# If using an empty secrets list [] then all accessible secrets will be available.\ntask.with_secrets('vault', [])\n# To use with Azure key vault, a k8s secret must be created with the following keys:\n# kubectl -n <namespace> create secret generic azure-key-vault-secret \\\n#     --from-literal=tenant_id=<service principal tenant ID> \\\n#     --from-literal=client_id=<service principal client ID> \\\n#     --from-literal=secret=<service principal secret key>\ntask.with_secrets('azure_vault', {\n'name': 'my-vault-name',\n'k8s_secret': 'azure-key-vault-secret',\n# An empty secrets list may be passed ('secrets': []) to access all vault secrets.\n'secrets': ['secret1', 'secret2'...]\n})\nParameters\nkind – secret type (file, inline, env)\nsource – secret data or link (see example)\nReturns\nThe Runtime (function) object"}
{"text": "mlrun.serving#\nclass mlrun.serving.GraphContext(level='info', logger=None, server=None, nuclio_context=None)[source]#\nBases: object\nGraph context object\nget_param(key: str, default=None)[source]#\nget_remote_endpoint(name, external=True)[source]#\nreturn the remote nuclio/serving function http(s) endpoint given its name\nParameters\nname – the function name/uri in the form [project/]function-name[:tag]\nexternal – return the external url (returns the external url by default)\nget_secret(key: str)[source]#\nproperty project#\ncurrent project name (for the current function)\npush_error(event, message, source=None, **kwargs)[source]#\nproperty server#\nclass mlrun.serving.GraphServer(graph=None, parameters=None, load_mode=None, function_uri=None, verbose=False, version=None, functions=None, graph_initializer=None, error_stream=None, track_models=None, tracking_policy=None, secret_sources=None, default_content_type=None)[source]#\nBases: mlrun.model.ModelObj\nproperty graph: Union[mlrun.serving.states.RootFlowStep, mlrun.serving.states.RouterStep]#\ninit_object(namespace)[source]#\ninit_states(context, namespace, resource_cache: Optional[mlrun.datastore.store_resources.ResourceCache] = None, logger=None, is_mock=False)[source]#\nfor internal use, initialize all steps (recursively)\nkind = 'server'#\nrun(event, context=None, get_body=False, extra_args=None)[source]#\nset_current_function(function)[source]#\nset which child function this server is currently running on\nset_error_stream(error_stream)[source]#\nset/initialize the error notification stream\ntest(path: str = '/', body: Optional[Union[str, bytes, dict]] = None, method: str = '', headers: Optional[str] = None, content_type: Optional[str] = None, silent: bool = False, get_body: bool = True, event_id: Optional[str] = None, trigger: Optional[mlrun.serving.server.MockTrigger] = None, offset=None, time=None)[source]#\ninvoke a test event into the server to simulate/test server behavior\nexample:\nserver = create_graph_server()\nserver.add_model(\"my\", class_name=MyModelClass, model_path=\"{path}\", z=100)\nprint(server.test(\"my/infer\", testdata))\nParameters\npath – api path, e.g. (/{router.url_prefix}/{model-name}/..) path\nbody – message body (dict or json str/bytes)\nmethod – optional, GET, POST, ..\nheaders – optional, request headers, ..\ncontent_type – optional, http mime type\nsilent – don’t raise on error responses (when not 20X)\nget_body – return the body as py object (vs serialize response into json)\nevent_id – specify the unique event ID (by default a random value will be generated)\ntrigger – nuclio trigger info or mlrun.serving.server.MockTrigger class (holds kind and name)\noffset – trigger offset (for streams)\ntime – event time Datetime or str, default to now()\nwait_for_completion()[source]#\nwait for async operation to complete\nclass mlrun.serving.QueueStep(name: Optional[str] = None, path: Optional[str] = None, after: Optional[list] = None, shards: Optional[int] = None, retention_in_hours: Optional[int] = None, trigger_args: Optional[dict] = None, **options)[source]#\nBases: mlrun.serving.states.BaseStep\nqueue step, implement an async queue or represent a stream\nproperty async_object#\ndefault_shape = 'cds'#\ninit_object(context, namespace, mode='sync', reset=False, **extra_kwargs)[source]#\ninit the step class\nkind = 'queue'#\nrun(event, *args, **kwargs)[source]#\nclass mlrun.serving.RouterStep(class_name: Optional[Union[str, type]] = None, class_args: Optional[dict] = None, handler: Optional[str] = None, routes: Optional[list] = None, name: Optional[str] = None, function: Optional[str] = None, input_path: Optional[str] = None, result_path: Optional[str] = None)[source]#\nBases: mlrun.serving.states.TaskStep\nrouter step, implement routing logic for running child routes\nadd_route(key, route=None, class_name=None, handler=None, function=None, **class_args)[source]#\nadd child route step or class to the router\nParameters\nkey – unique name (and route path) for the child step\nroute – child step object (Task, ..)\nclass_name – class name to build the route step from (when route is not provided)\nclass_args – class init arguments\nhandler – class handler to invoke on run/event\nfunction – function this step should run in\nclear_children(routes: list)[source]#\nclear child steps (routes)\ndefault_shape = 'doubleoctagon'#\nget_children()[source]#\nget child steps (routes)\ninit_object(context, namespace, mode='sync', reset=False, **extra_kwargs)[source]#\ninit the step class\nkind = 'router'#\nplot(filename=None, format=None, source=None, **kw)[source]#\nplot/save graph using graphviz\nParameters\nfilename – target filepath for the image (None for the notebook)\nformat – The output format used for rendering ('pdf', 'png', etc.)\nsource – source step to add to the graph\nkw – kwargs passed to graphviz, e.g. rankdir=”LR” (see: https://graphviz.org/doc/info/attrs.html)\nReturns\ngraphviz graph object\nproperty routes#\nchild routes/steps, traffic is routed to routes based on router logic\nclass mlrun.serving.TaskStep(class_name: Optional[Union[str, type]] = None, class_args: Optional[dict] = None, handler: Optional[str] = None, name: Optional[str] = None, after: Optional[list] = None, full_event: Optional[bool] = None, function: Optional[str] = None, responder: Optional[bool] = None, input_path: Optional[str] = None, result_path: Optional[str] = None)[source]#\nBases: mlrun.serving.states.BaseStep\ntask execution step, runs a class or handler\nproperty async_object#\nreturn the sync or async (storey) class instance\nclear_object()[source]#\ninit_object(context, namespace, mode='sync', reset=False, **extra_kwargs)[source]#\ninit the step class\nkind = 'task'#\nrespond()[source]#\nmark this step as the responder.\nstep output will be returned as the flow result, no other step can follow\nrun(event, *args, **kwargs)[source]#\nrun this step, in async flows the run is done through storey\nclass mlrun.serving.V2ModelServer(context=None, name: Optional[str] = None, model_path: Optional[str] = None, model=None, protocol=None, input_path: Optional[str] = None, result_path: Optional[str] = None, **kwargs)[source]#\nBases: mlrun.serving.utils.StepToDict\nbase model serving class (v2), using similar API to KFServing v2 and Triton\nbase model serving class (v2), using similar API to KFServing v2 and Triton\nThe class is initialized automatically by the model server and can run locally\nas part of a nuclio serverless function, or as part of a real-time pipeline\ndefault model url is: /v2/models/<model>[/versions/<ver>]/operation\nYou need to implement two mandatory methods:load()     - download the model file(s) and load the model into memory\npredict()  - accept request payload and return prediction/inference results\nyou can override additional methods : preprocess, validate, postprocess, explain\nyou can add custom api endpoint by adding method op_xx(event), will be invoked by\ncalling the <model-url>/xx (operation = xx)\nmodel server classes are subclassed (subclass implements the load() and predict() methods)\nthe subclass can be added to a serving graph or to a model router\ndefining a sub class:\nclass MyClass(V2ModelServer):\ndef load(self):\n# load and initialize the model and/or other elements\nmodel_file, extra_data = self.get_model(suffix='.pkl')\nself.model = load(open(model_file, \"rb\"))\ndef predict(self, request):\nevents = np.array(request['inputs'])\ndmatrix = xgb.DMatrix(events)\nresult: xgb.DMatrix = self.model.predict(dmatrix)\nreturn {\"outputs\": result.tolist()}\nusage example:\n# adding a model to a serving graph using the subclass MyClass\n# MyClass will be initialized with the name \"my\", the model_path, and an arg called my_param\ngraph = fn.set_topology(\"router\")\nfn.add_model(\"my\", class_name=\"MyClass\", model_path=\"<model-uri>>\", my_param=5)\nParameters\ncontext – for internal use (passed in init)\nname – step name\nmodel_path – model file/dir or artifact path\nmodel – model object (for local testing)\nprotocol – serving API protocol (default “v2”)\ninput_path – when specified selects the key/path in the event to use as body\nthis require that the event body will behave like a dict, example:\nevent: {“data”: {“a”: 5, “b”: 7}}, input_path=”data.b” means request body will be 7\nresult_path – selects the key/path in the event to write the results to\nthis require that the event body will behave like a dict, example:\nevent: {“x”: 5} , result_path=”resp” means the returned response will be written\nto event[“y”] resulting in {“x”: 5, “resp”: <result>}\nkwargs – extra arguments (can be accessed using self.get_param(key))\ndo_event(event, *args, **kwargs)[source]#\nmain model event handler method\nexplain(request: Dict) → Dict[source]#\nmodel explain operation\nget_model(suffix='')[source]#\nget the model file(s) and metadata from model store\nthe method returns a path to the model file and the extra data (dict of dataitem objects)\nit also loads the model metadata into the self.model_spec attribute, allowing direct access\nto all the model metadata attributes.\nget_model is usually used in the model .load() method to init the model\n.. rubric:: Examples\ndef load(self):\nmodel_file, extra_data = self.get_model(suffix='.pkl')\nself.model = load(open(model_file, \"rb\"))\ncategories = extra_data['categories'].as_df()\nParameters\nsuffix (str) – optional, model file suffix (when the model_path is a directory)\nReturns\nstr – (local) model file\ndict – extra dataitems dictionary\nget_param(key: str, default=None)[source]#\nget param by key (specified in the model or the function)\nload()[source]#\nmodel loading function, see also .get_model() method\nlogged_results(request: dict, response: dict, op: str)[source]#\nhook for controlling which results are tracked by the model monitoring\nthis hook allows controlling which input/output data is logged by the model monitoring\nallow filtering out columns or adding custom values, can also be used to monitor derived metrics\nfor example in image classification calculate and track the RGB values vs the image bitmap\nthe request[“inputs”] holds a list of input values/arrays, the response[“outputs”] holds a list of\ncorresponding output values/arrays (the schema of the input/output fields is stored in the model object),\nthis method should return lists of alternative inputs and outputs which will be monitored\nParameters\nrequest – predict/explain request, see model serving docs for details\nresponse – result from the model predict/explain (after postprocess())\nop – operation (predict/infer or explain)\nReturns\nthe input and output lists to track\npost_init(mode='sync')[source]#\nsync/async model loading, for internal use\npostprocess(request: Dict) → Dict[source]#\npostprocess, before returning response\npredict(request: Dict) → Dict[source]#\nmodel prediction operation\npreprocess(request: Dict, operation) → Dict[source]#\npreprocess the event body before validate and action\nset_metric(name: str, value)[source]#\nset real time metric (for model monitoring)\nvalidate(request, operation)[source]#\nvalidate the event body (after preprocess)\nclass mlrun.serving.VotingEnsemble(context=None, name: Optional[str] = None, routes=None, protocol: Optional[str] = None, url_prefix: Optional[str] = None, health_prefix: Optional[str] = None, vote_type=None, executor_type=None, prediction_col_name=None, **kwargs)[source]#\nBases: mlrun.serving.routers.BaseModelRouter\nVoting Ensemble\nThe VotingEnsemble class enables you to apply prediction logic on top of\nthe different added models.\nYou can use it by calling:\n<prefix>/<model>[/versions/<ver>]/operationSends the event to the specific <model>[/versions/<ver>]\n<prefix>/operationSends the event to all models and applies vote(self, event)\nThe VotingEnsemble applies the following logic:\nIncoming Event -> Router Preprocessing -> Send to model/s ->\nApply all model/s logic (Preprocessing -> Prediction -> Postprocessing) ->\nRouter Voting logic -> Router Postprocessing -> Response\nThis enables you to do the general preprocessing and postprocessing steps\nonce on the router level, with only model-specific adjustments at the\nmodel level.\nWhen enabling model tracking via set_tracking() the ensemble logic\npredictions will appear with model name as the given VotingEnsemble name\nor “VotingEnsemble” by default.\nExample:\n# Define a serving function\n# Note: You can point the function to a file containing you own Router or Classifier Model class\n#       this basic class supports sklearn based models (with `<model>.predict()` api)\nfn = mlrun.code_to_function(name='ensemble',\nkind='serving',\nfilename='model-server.py'\nimage='mlrun/ml-models')\n# Set the router class\n# You can set your own classes by simply changing the `class_name`\nfn.set_topology(class_name='mlrun.serving.routers.VotingEnsemble')\n# Add models\nfn.add_model(<model_name>, <model_path>, <model_class_name>)\nfn.add_model(<model_name>, <model_path>, <model_class_name>)\nHow to extend the VotingEnsemble:\nThe VotingEnsemble applies its logic using the logic(predictions) function.\nThe logic() function receives an array of (# samples, # predictors) which you\ncan then use to apply whatever logic you may need.\nIf we use this VotingEnsemble as an example, the logic() function tries to figure\nout whether you are trying to do a classification or a regression prediction by\nthe prediction type or by the given vote_type parameter.  Then we apply the appropriate\nmax_vote() or mean_vote() which calculates the actual prediction result and returns it\nas the VotingEnsemble’s prediction.\nParameters\ncontext – for internal use (passed in init)\nname – step name\nroutes – for internal use (routes passed in init)\nprotocol – serving API protocol (default “v2”)\nurl_prefix – url prefix for the router (default /v2/models)\nhealth_prefix – health api url prefix (default /v2/health)\ninput_path – when specified selects the key/path in the event to use as body\nthis require that the event body will behave like a dict, example:\nevent: {“data”: {“a”: 5, “b”: 7}}, input_path=”data.b” means request body will be 7\nresult_path – selects the key/path in the event to write the results to\nthis require that the event body will behave like a dict, example:\nevent: {“x”: 5} , result_path=”resp” means the returned response will be written\nto event[“y”] resulting in {“x”: 5, “resp”: <result>}\nvote_type – Voting type to be used (from VotingTypes).\nby default will try to self-deduct upon the first event:\n- float prediction type: regression\n- int prediction type: classification\nexecutor_type – Parallelism mechanism, out of ParallelRunnerModes, by default threads\nprediction_col_name – The dict key for the predictions column in the model’s responses output.\nExample: If the model returns\n{id: <id>, model_name: <name>, outputs: {…, prediction: [<predictions>], …}}\nthe prediction_col_name should be prediction.\nby default, prediction\nkwargs – extra arguments\ndo_event(event, *args, **kwargs)[source]#\nHandles incoming requests.\nParameters\nevent (nuclio.Event) – Incoming request as a nuclio.Event.\nReturns\nEvent response after running the requested logic\nReturn type\nResponse\nextract_results_from_response(response)[source]#\nExtracts the prediction from the model response.\nThis function is used to allow multiple model return types. and allow for easy\nextension to the user’s ensemble and models best practices.\nParameters\nresponse (Union[List, Dict]) – The model response’s output field.\nReturns\nThe model’s predictions\nReturn type\nList\nlogic(predictions)[source]#\npost_init(mode='sync')[source]#\nvalidate(request)[source]#\nValidate the event body (after preprocessing)\nParameters\nrequest (dict) – Event body.\nReturns\nEvent body after validation\nReturn type\ndict\nRaises\nException – inputs key not found in request\nException – inputs should be of type List\nmlrun.serving.create_graph_server(parameters={}, load_mode=None, graph=None, verbose=False, current_function=None, **kwargs) → mlrun.serving.server.GraphServer[source]#\ncreate graph server host/emulator for local or test runs\nUsage example:\nserver = create_graph_server(graph=RouterStep(), parameters={})\nserver.init(None, globals())\nserver.graph.add_route(\"my\", class_name=MyModelClass, model_path=\"{path}\", z=100)\nprint(server.test(\"/v2/models/my/infer\", testdata))\nclass mlrun.serving.remote.BatchHttpRequests(url: Optional[str] = None, subpath: Optional[str] = None, method: Optional[str] = None, headers: Optional[dict] = None, url_expression: Optional[str] = None, body_expression: Optional[str] = None, return_json: bool = True, input_path: Optional[str] = None, result_path: Optional[str] = None, retries=None, backoff_factor=None, timeout=None, **kwargs)[source]#\nclass for calling remote endpoints in parallel\nclass for calling remote endpoints in parallel\nsync and async graph step implementation for request/resp to remote service (class shortcut = “$remote”)\nurl can be an http(s) url (e.g. “https://myservice/path”) or an mlrun function uri ([project/]name).\nalternatively the url_expression can be specified to build the url from the event (e.g. “event[‘url’]”).\nexample pipeline:\nfunction = mlrun.new_function(\"myfunc\", kind=\"serving\")\nflow = function.set_topology(\"flow\", engine=\"async\")\nflow.to(\nBatchHttpRequests(\nurl_expression=\"event['url']\",\nbody_expression=\"event['data']\",\nmethod=\"POST\",\ninput_path=\"req\",\nresult_path=\"resp\",\n)\n).respond()\nserver = function.to_mock_server()\n# request contains a list of elements, each with url and data\nrequest = [{\"url\": f\"{base_url}/{i}\", \"data\": i} for i in range(2)]\nresp = server.test(body={\"req\": request})\nParameters\nurl – http(s) url or function [project/]name to call\nsubpath – path (which follows the url)\nmethod – HTTP method (GET, POST, ..), default to POST\nheaders – dictionary with http header values\nurl_expression – an expression for getting the url from the event, e.g. “event[‘url’]”\nbody_expression – an expression for getting the request body from the event, e.g. “event[‘data’]”\nreturn_json – indicate the returned value is json, and convert it to a py object\ninput_path – when specified selects the key/path in the event to use as body\nthis require that the event body will behave like a dict, example:\nevent: {“data”: {“a”: 5, “b”: 7}}, input_path=”data.b” means request body will be 7\nresult_path – selects the key/path in the event to write the results to\nthis require that the event body will behave like a dict, example:\nevent: {“x”: 5} , result_path=”resp” means the returned response will be written\nto event[“y”] resulting in {“x”: 5, “resp”: <result>}\nretries – number of retries (in exponential backoff)\nbackoff_factor – A backoff factor in seconds to apply between attempts after the second try\ntimeout – How long to wait for the server to send data before giving up, float in seconds\n__init__(url: Optional[str] = None, subpath: Optional[str] = None, method: Optional[str] = None, headers: Optional[dict] = None, url_expression: Optional[str] = None, body_expression: Optional[str] = None, return_json: bool = True, input_path: Optional[str] = None, result_path: Optional[str] = None, retries=None, backoff_factor=None, timeout=None, **kwargs)[source]#\nclass for calling remote endpoints in parallel\nsync and async graph step implementation for request/resp to remote service (class shortcut = “$remote”)\nurl can be an http(s) url (e.g. “https://myservice/path”) or an mlrun function uri ([project/]name).\nalternatively the url_expression can be specified to build the url from the event (e.g. “event[‘url’]”).\nexample pipeline:\nfunction = mlrun.new_function(\"myfunc\", kind=\"serving\")\nflow = function.set_topology(\"flow\", engine=\"async\")\nflow.to(\nBatchHttpRequests(\nurl_expression=\"event['url']\",\nbody_expression=\"event['data']\",\nmethod=\"POST\",\ninput_path=\"req\",\nresult_path=\"resp\",\n)\n).respond()\nserver = function.to_mock_server()\n# request contains a list of elements, each with url and data\nrequest = [{\"url\": f\"{base_url}/{i}\", \"data\": i} for i in range(2)]\nresp = server.test(body={\"req\": request})\nParameters\nurl – http(s) url or function [project/]name to call\nsubpath – path (which follows the url)\nmethod – HTTP method (GET, POST, ..), default to POST\nheaders – dictionary with http header values\nurl_expression – an expression for getting the url from the event, e.g. “event[‘url’]”\nbody_expression – an expression for getting the request body from the event, e.g. “event[‘data’]”\nreturn_json – indicate the returned value is json, and convert it to a py object\ninput_path – when specified selects the key/path in the event to use as body\nthis require that the event body will behave like a dict, example:\nevent: {“data”: {“a”: 5, “b”: 7}}, input_path=”data.b” means request body will be 7\nresult_path – selects the key/path in the event to write the results to\nthis require that the event body will behave like a dict, example:\nevent: {“x”: 5} , result_path=”resp” means the returned response will be written\nto event[“y”] resulting in {“x”: 5, “resp”: <result>}\nretries – number of retries (in exponential backoff)\nbackoff_factor – A backoff factor in seconds to apply between attempts after the second try\ntimeout – How long to wait for the server to send data before giving up, float in seconds\nclass mlrun.serving.remote.RemoteStep(url: str, subpath: Optional[str] = None, method: Optional[str] = None, headers: Optional[dict] = None, url_expression: Optional[str] = None, body_expression: Optional[str] = None, return_json: bool = True, input_path: Optional[str] = None, result_path: Optional[str] = None, max_in_flight=None, retries=None, backoff_factor=None, timeout=None, **kwargs)[source]#\nclass for calling remote endpoints\nclass for calling remote endpoints\nsync and async graph step implementation for request/resp to remote service (class shortcut = “$remote”)\nurl can be an http(s) url (e.g. “https://myservice/path”) or an mlrun function uri ([project/]name).\nalternatively the url_expression can be specified to build the url from the event (e.g. “event[‘url’]”).\nexample pipeline:\nflow = function.set_topology(\"flow\", engine=\"async\")\nflow.to(name=\"step1\", handler=\"func1\")\n.to(RemoteStep(name=\"remote_echo\", url=\"https://myservice/path\", method=\"POST\"))\n.to(name=\"laststep\", handler=\"func2\").respond()\nParameters\nurl – http(s) url or function [project/]name to call\nsubpath – path (which follows the url), use $path to use the event.path\nmethod – HTTP method (GET, POST, ..), default to POST\nheaders – dictionary with http header values\nurl_expression – an expression for getting the url from the event, e.g. “event[‘url’]”\nbody_expression – an expression for getting the request body from the event, e.g. “event[‘data’]”\nreturn_json – indicate the returned value is json, and convert it to a py object\ninput_path – when specified selects the key/path in the event to use as body\nthis require that the event body will behave like a dict, example:\nevent: {“data”: {“a”: 5, “b”: 7}}, input_path=”data.b” means request body will be 7\nresult_path – selects the key/path in the event to write the results to\nthis require that the event body will behave like a dict, example:\nevent: {“x”: 5} , result_path=”resp” means the returned response will be written\nto event[“y”] resulting in {“x”: 5, “resp”: <result>}\nretries – number of retries (in exponential backoff)\nbackoff_factor – A backoff factor in seconds to apply between attempts after the second try\ntimeout – How long to wait for the server to send data before giving up, float in seconds\n__init__(url: str, subpath: Optional[str] = None, method: Optional[str] = None, headers: Optional[dict] = None, url_expression: Optional[str] = None, body_expression: Optional[str] = None, return_json: bool = True, input_path: Optional[str] = None, result_path: Optional[str] = None, max_in_flight=None, retries=None, backoff_factor=None, timeout=None, **kwargs)[source]#\nclass for calling remote endpoints\nsync and async graph step implementation for request/resp to remote service (class shortcut = “$remote”)\nurl can be an http(s) url (e.g. “https://myservice/path”) or an mlrun function uri ([project/]name).\nalternatively the url_expression can be specified to build the url from the event (e.g. “event[‘url’]”).\nexample pipeline:\nflow = function.set_topology(\"flow\", engine=\"async\")\nflow.to(name=\"step1\", handler=\"func1\")\n.to(RemoteStep(name=\"remote_echo\", url=\"https://myservice/path\", method=\"POST\"))\n.to(name=\"laststep\", handler=\"func2\").respond()\nParameters\nurl – http(s) url or function [project/]name to call\nsubpath – path (which follows the url), use $path to use the event.path\nmethod – HTTP method (GET, POST, ..), default to POST\nheaders – dictionary with http header values\nurl_expression – an expression for getting the url from the event, e.g. “event[‘url’]”\nbody_expression – an expression for getting the request body from the event, e.g. “event[‘data’]”\nreturn_json – indicate the returned value is json, and convert it to a py object\ninput_path – when specified selects the key/path in the event to use as body\nthis require that the event body will behave like a dict, example:\nevent: {“data”: {“a”: 5, “b”: 7}}, input_path=”data.b” means request body will be 7\nresult_path – selects the key/path in the event to write the results to\nthis require that the event body will behave like a dict, example:\nevent: {“x”: 5} , result_path=”resp” means the returned response will be written\nto event[“y”] resulting in {“x”: 5, “resp”: <result>}\nretries – number of retries (in exponential backoff)\nbackoff_factor – A backoff factor in seconds to apply between attempts after the second try\ntimeout – How long to wait for the server to send data before giving up, float in seconds"}
{"text": "mlrun#\nclass mlrun.ArtifactType(value)[source]#\nPossible artifact types to log using the MLRun context decorator.\nmlrun.code_to_function(name: str = '', project: str = '', tag: str = '', filename: str = '', handler: str = '', kind: str = '', image: Optional[str] = None, code_output: str = '', embed_code: bool = True, description: str = '', requirements: Optional[Union[str, List[str]]] = None, categories: Optional[List[str]] = None, labels: Optional[Dict[str, str]] = None, with_doc: bool = True, ignored_tags=None) → Union[mlrun.runtimes.mpijob.v1alpha1.MpiRuntimeV1Alpha1, mlrun.runtimes.mpijob.v1.MpiRuntimeV1, mlrun.runtimes.function.RemoteRuntime, mlrun.runtimes.serving.ServingRuntime, mlrun.runtimes.daskjob.DaskCluster, mlrun.runtimes.kubejob.KubejobRuntime, mlrun.runtimes.local.LocalRuntime, mlrun.runtimes.sparkjob.spark2job.Spark2Runtime, mlrun.runtimes.sparkjob.spark3job.Spark3Runtime, mlrun.runtimes.remotesparkjob.RemoteSparkRuntime][source]#\nConvenience function to insert code and configure an mlrun runtime.\nEasiest way to construct a runtime type object. Provides the most often\nused configuration options for all runtimes as parameters.\nInstantiated runtimes are considered ‘functions’ in mlrun, but they are\nanything from nuclio functions to generic kubernetes pods to spark jobs.\nFunctions are meant to be focused, and as such limited in scope and size.\nTypically a function can be expressed in a single python module with\nadded support from custom docker images and commands for the environment.\nThe returned runtime object can be further configured if more\ncustomization is required.\nOne of the most important parameters is ‘kind’. This is what is used to\nspecify the chosen runtimes. The options are:\nlocal: execute a local python or shell script\njob: insert the code into a Kubernetes pod and execute it\nnuclio: insert the code into a real-time serverless nuclio function\nserving: insert code into orchestrated nuclio function(s) forming a DAG\ndask: run the specified python code / script as Dask Distributed job\nmpijob: run distributed Horovod jobs over the MPI job operator\nspark: run distributed Spark job using Spark Kubernetes Operator\nremote-spark: run distributed Spark job on remote Spark service\nLearn more about function runtimes here:\nhttps://docs.mlrun.org/en/latest/runtimes/functions.html#function-runtimes\nParameters\nname – function name, typically best to use hyphen-case\nproject – project used to namespace the function, defaults to ‘default’\ntag – function tag to track multiple versions of the same function, defaults to ‘latest’\nfilename – path to .py/.ipynb file, defaults to current jupyter notebook\nhandler – The default function handler to call for the job or nuclio function, in batch functions\n(job, mpijob, ..) the handler can also be specified in the .run() command, when not specified\nthe entire file will be executed (as main).\nfor nuclio functions the handler is in the form of module:function, defaults to ‘main:handler’\nkind – function runtime type string - nuclio, job, etc. (see docstring for all options)\nimage – base docker image to use for building the function container, defaults to None\ncode_output – specify ‘.’ to generate python module from the current jupyter notebook\nembed_code – indicates whether or not to inject the code directly into the function runtime spec,\ndefaults to True\ndescription – short function description, defaults to ‘’\nrequirements – list of python packages or pip requirements file path, defaults to None\ncategories – list of categories for mlrun Function Hub, defaults to None\nlabels – immutable name/value pairs to tag the function with useful metadata, defaults to None\nwith_doc – indicates whether to document the function parameters, defaults to True\nignored_tags – notebook cells to ignore when converting notebooks to py code (separated by ‘;’)\nReturns\npre-configured function object from a mlrun runtime class\nexample:\nimport mlrun\n# create job function object from notebook code and add doc/metadata\nfn = mlrun.code_to_function(\"file_utils\", kind=\"job\",\nhandler=\"open_archive\", image=\"mlrun/mlrun\",\ndescription = \"this function opens a zip archive into a local/mounted folder\",\ncategories = [\"fileutils\"],\nlabels = {\"author\": \"me\"})\nexample:\nimport mlrun\nfrom pathlib import Path\n# create file\nPath(\"mover.py\").touch()\n# create nuclio function object from python module call mover.py\nfn = mlrun.code_to_function(\"nuclio-mover\", kind=\"nuclio\",\nfilename=\"mover.py\", image=\"python:3.7\",\ndescription = \"this function moves files from one system to another\",\nrequirements = [\"pandas\"],\nlabels = {\"author\": \"me\"})\nmlrun.get_version()[source]#\nget current mlrun version\nmlrun.handler(labels: Optional[Dict[str, str]] = None, outputs: Optional[List[Optional[Union[Tuple[str, mlrun.run.ArtifactType], Tuple[str, str], Tuple[str, mlrun.run.ArtifactType, Dict[str, Any]], Tuple[str, str, Dict[str, Any]], str]]]] = None, inputs: Union[bool, Dict[str, Type]] = True)[source]#\nMLRun’s handler is a decorator to wrap a function and enable setting labels, automatic mlrun.DataItem parsing and\noutputs logging.\nParameters\nlabels – Labels to add to the run. Expecting a dictionary with the labels names as keys. Default: None.\noutputs – Logging configurations for the function’s returned values. Expecting a list of tuples and None\nvalues:\nstr - A string in the format of ‘{key}:{artifact_type}’. If a string was given without ‘:’ it willindicate the key and the artifact type will be according to the returned value\ntype.\ntuple - A tuple of:\n[0]: str - The key (name) of the artifact to use for the logged output.\n[1]: Union[ArtifactType, str] = “result” - An ArtifactType enum or an equivalent\nstring, that indicates how to log the returned value. The artifact types can be one of:\nDATASET = “dataset”\nDIRECTORY = “directory”\nFILE = “file”\nOBJECT = “object”\nPLOT = “plot”\nRESULT = “result”.\n[2]: Optional[Dict[str, Any]] - A keyword arguments dictionary with the properties to pass to\nthe relevant logging function (one of context.log_artifact, context.log_result,\ncontext.log_dataset).\nNone - Do not log the output.\nThe list length must be equal to the total amount of returned values from the function. Default is\nNone - meaning no outputs will be logged.\ninputs – Parsing configurations for the arguments passed as inputs via the run method of an MLRun function.\nCan be passed as a boolean value or a dictionary:\nTrue - Parse all found inputs to the assigned type hint in the function’s signature. If there is notype hint assigned, the value will remain an mlrun.DataItem.\nFalse - Do not parse inputs, leaving the inputs as mlrun.DataItem.\nDict[str, Type] - A dictionary with argument name as key and the expected type to parse themlrun.DataItem to.\nDefault: True.\nExample:\nimport mlrun\n@mlrun.handler(outputs=[\"my_array\", None, \"my_multiplier\"])\ndef my_handler(array: np.ndarray, m: int):\narray = array * m\nm += 1\nreturn array, \"I won't be logged\", m\n>>> mlrun_function = mlrun.code_to_function(\"my_code.py\", kind=\"job\")\n>>> run_object = mlrun_function.run(\n...     handler=\"my_handler\",\n...     inputs={\"array\": \"store://my_array_Artifact\"},\n...     params={\"m\": 2}\n... )\n>>> run_object.outputs\n{'my_multiplier': 3, 'my_array': 'store://...'}\nmlrun.import_function(url='', secrets=None, db='', project=None, new_name=None)[source]#\nCreate function object from DB or local/remote YAML file\nFunctions can be imported from function repositories (mlrun Function Hub (formerly Marketplace) or local db),\nor be read from a remote URL (http(s), s3, git, v3io, ..) containing the function YAML\nspecial URLs:\nfunction hub: hub://{name}[:{tag}]\nlocal mlrun db:       db://{project-name}/{name}[:{tag}]\nexamples:\nfunction = mlrun.import_function(\"hub://sklearn_classifier\")\nfunction = mlrun.import_function(\"./func.yaml\")\nfunction = mlrun.import_function(\"https://raw.githubusercontent.com/org/repo/func.yaml\")\nParameters\nurl – path/url to Function Hub, db or function YAML file\nsecrets – optional, credentials dict for DB or URL (s3, v3io, …)\ndb – optional, mlrun api/db path\nproject – optional, target project for the function\nnew_name – optional, override the imported function name\nReturns\nfunction object\nmlrun.set_environment(api_path: Optional[str] = None, artifact_path: str = '', project: str = '', access_key: Optional[str] = None, user_project=False, username: Optional[str] = None, env_file: Optional[str] = None, mock_functions: Optional[str] = None)[source]#\nset and test default config for: api path, artifact_path and project\nthis function will try and read the configuration from the environment/api\nand merge it with the user provided project name, artifacts path or api path/access_key.\nit returns the configured artifacts path, this can be used to define sub paths.\nNote: the artifact path is an mlrun data uri (e.g. s3://bucket/path) and can not be used with file utils.\nexample:\nfrom os import path\nproject_name, artifact_path = set_environment(project='my-project')\nset_environment(\"http://localhost:8080\", artifact_path=\"./\")\nset_environment(env_file=\"mlrun.env\")\nset_environment(\"<remote-service-url>\", access_key=\"xyz\", username=\"joe\")\nParameters\napi_path – location/url of mlrun api service\nartifact_path – path/url for storing experiment artifacts\nproject – default project name\naccess_key – set the remote cluster access key (V3IO_ACCESS_KEY)\nuser_project – add the current user name to the provided project name (making it unique per user)\nusername – name of the user to authenticate\nenv_file – path/url to .env file (holding MLRun config and other env vars), see: set_env_from_file()\nmock_functions – set to True to create local/mock functions instead of real containers,\nset to “auto” to auto determine based on the presence of k8s/Nuclio\nReturns\ndefault project name\nactual artifact path/url, can be used to create subpaths per task or group of artifacts"}
{"text": "storey.transformations - Graph transformations#\nGraph transformations are contained in the storey.transformations module. For convenience, they\ncan also be imported directly from the storey package. Note that the transformation functions are actually\nencapsulated in classes, so that they can be referenced by name of class from graph step definitions.\nclass storey.transformations.AggregateByKey(aggregates: Union[List[storey.dtypes.FieldAggregator], List[Dict[str, object]]], table: Union[storey.table.Table, str], key: Optional[Union[str, Callable[[storey.dtypes.Event], object]]] = None, emit_policy: Union[storey.dtypes.EmitPolicy, Dict[str, object]] = <storey.dtypes.EmitEveryEvent object>, augmentation_fn: Optional[Callable[[storey.dtypes.Event, Dict[str, object]], storey.dtypes.Event]] = None, enrich_with: Optional[List[str]] = None, aliases: Optional[Dict[str, str]] = None, use_windows_from_schema: bool = False, **kwargs)[source]#\nAggregates the data into the table object provided for later persistence,and outputs an event enriched with the requested aggregation features.\nPersistence is done via the NoSqlTarget step and based on the Cache object persistence settings.\nParameters\naggregates – List of aggregates to apply for each event.\naccepts either list of FieldAggregators or a dictionary describing FieldAggregators.\ntable – A Table object or name for persistence of aggregations.\nIf a table name is provided, it will be looked up in the context object passed in kwargs.\nkey – Key field to aggregate by, accepts either a string representing the key field or\na key extracting function. Defaults to the key in the event’s metadata. (Optional)\nemit_policy – Policy indicating when the data will be emitted. Defaults to EmitEveryEvent\naugmentation_fn – Function that augments the features into the event’s body. Defaults to\nupdating a dict. (Optional)\nenrich_with – List of attributes names from the associated storage object to be fetched\nand added to every event. (Optional)\naliases – Dictionary specifying aliases for enriched or aggregate columns, of the\nformat {‘col_name’: ‘new_col_name’}. (Optional)\nclass storey.transformations.Assert(**kwargs)[source]#\nExposes an API for testing the flow between steps.\nclass storey.transformations.Batch(max_events: Optional[int] = None, flush_after_seconds: Optional[int] = None, key: Optional[Union[str, Callable[[storey.dtypes.Event], str]]] = None, **kwargs)[source]#\nBatches events into lists of up to max_events events. Each emitted list contained max_events events, unless\nflush_after_seconds seconds have passed since the first event in the batch was received, at which the batch is\nemitted with potentially fewer than max_events event.\nParameters\nmax_events – Maximum number of events per emitted batch. Set to None to emit all events in one batch on flow\ntermination.\nflush_after_seconds – Maximum number of seconds to wait before a batch is emitted.\nkey – The key by which events are grouped. By default (None), events are not grouped.\nOther options may be:\nSet a ‘$key’ to group events by the Event.key property.\nset a ‘str’ key to group events by Event.body[str].\nset a Callable[Any, Any] to group events by a a custom key extractor.\nclass storey.transformations.Choice(choice_array, default=None, **kwargs)[source]#\nRedirects each input element into at most one of multiple downstreams.\nParameters\nchoice_array (tuple of (Flow, Function (Event=>boolean))) – a list of (downstream, condition) tuples, where downstream is a step and condition is a\nfunction. The first condition in the list to evaluate as true for an input element causes that element to\nbe redirected to that downstream step.\ndefault (Flow) – a default step for events that did not match any condition in choice_array. If not set, elements\nthat don’t match any condition will be discarded.\nname (string) – Name of this step, as it should appear in logs. Defaults to class name (Choice).\nfull_event (boolean) – Whether user functions should receive and/or return Event objects (when True),\nor only the payload (when False). Defaults to False.\nclass storey.transformations.Extend(fn, long_running=None, **kwargs)[source]#\nAdds fields to each incoming event.\nParameters\nfn (Function (Event=>Dict)) – Function to transform each event to a dictionary. The fields in the returned dictionary are then added\nto the original event.\nlong_running (boolean) – Whether fn is a long-running function. Long-running functions are run in an executor to avoid\nblocking other concurrent processing. Default is False.\nname (string) – Name of this step, as it should appear in logs. Defaults to class name (Extend).\nfull_event (boolean) – Whether user functions should receive and/or return Event objects (when True), or only the\npayload (when False). Defaults to False.\nclass storey.transformations.Filter(fn, long_running=None, **kwargs)[source]#\nFilters events based on a user-provided function.\nParameters\nfn (Function (Event=>boolean)) – Function to decide whether to keep each event.\nlong_running (boolean) – Whether fn is a long-running function. Long-running functions are run in an executor to\navoid blocking other concurrent processing. Default is False.\nname (string) – Name of this step, as it should appear in logs. Defaults to class name (Filter).\nfull_event (boolean) – Whether user functions should receive and/or return Event objects (when True), or only the\npayload (when False). Defaults to False.\nclass storey.transformations.FlatMap(fn, long_running=None, **kwargs)[source]#\nMaps, or transforms, each incoming event into any number of events.\nParameters\nfn (Function (Event=>list of Event)) – Function to transform each event to a list of events.\nlong_running (boolean) – Whether fn is a long-running function. Long-running functions are run in an executor\nto avoid blocking other concurrent processing. Default is False.\nname (string) – Name of this step, as it should appear in logs. Defaults to class name (FlatMap).\nfull_event (boolean) – Whether user functions should receive and/or return Event objects (when True), or only\nthe payload (when False). Defaults to False.\nstorey.transformations.Flatten(**kwargs)[source]#\nFlatten is equivalent to FlatMap(lambda x: x).\nclass storey.transformations.ForEach(fn, long_running=None, **kwargs)[source]#\nApplies given function on each event in the stream, passes original event downstream.\nclass storey.transformations.JoinWithTable(table: Union[storey.table.Table, str], key_extractor: Union[str, Callable[[storey.dtypes.Event], str]], attributes: Optional[List[str]] = None, inner_join: bool = False, join_function: Optional[Callable[[Any, Dict[str, object]], Any]] = None, **kwargs)[source]#\nJoins each event with data from the given table.\nParameters\ntable – A Table object or name to join with. If a table name is provided, it will be looked up in the context.\nkey_extractor – Key’s column name or a function for extracting the key, for table access from an event.\nattributes – A comma-separated list of attributes to be queried for. Defaults to all attributes.\ninner_join – Whether to drop events when the table does not have a matching entry (join_function won’t be\ncalled in such a case). Defaults to False.\njoin_function – Joins the original event with relevant data received from the storage. Event is dropped when\nthis function returns None. Defaults to assume the event’s body is a dict-like object and updating it.\nname – Name of this step, as it should appear in logs. Defaults to class name (JoinWithTable).\nfull_event – Whether user functions should receive and/or return Event objects (when True), or only the\npayload (when False). Defaults to False.\ncontext – Context object that holds global configurations and secrets.\nclass storey.transformations.Map(fn, long_running=None, **kwargs)[source]#\nMaps, or transforms, incoming events using a user-provided function.\nParameters\nfn (Function (Event=>Event)) – Function to apply to each event\nlong_running (boolean) – Whether fn is a long-running function. Long-running functions are run in an executor to\navoid blocking other concurrent processing. Default is False.\nname (string) – Name of this step, as it should appear in logs. Defaults to class name (Map).\nfull_event (boolean) – Whether user functions should receive and/or return Event objects (when True),\nor only the payload (when False). Defaults to False.\nclass storey.transformations.MapClass(long_running=None, **kwargs)[source]#\nSimilar to Map, but instead of a function argument, this class should be extended and its do()\nmethod overridden.\nclass storey.transformations.MapWithState(initial_state, fn, group_by_key=False, **kwargs)[source]#\nMaps, or transforms, incoming events using a stateful user-provided function, and an initial state,which may be a database table.\nParameters\ninitial_state (dictionary or Table if group_by_key is True. Any object otherwise.) – Initial state for the computation. If group_by_key is True, this must be a dictionary or\na Table object.\nfn (Function ((Event, state)=>(Event, state))) – A function to run on each event and the current state. Must yield an event and an updated state.\ngroup_by_key (boolean) – Whether the state is computed by key. Optional. Default to False.\nfull_event (boolean) – Whether fn will receive and return an Event object or only the body (payload).\nOptional. Defaults to False (body only).\nclass storey.transformations.Partition(predicate: Callable[[Any], bool], **kwargs)[source]#\nPartitions events by calling a predicate function on each event. Each processed event results in a Partitioned\nnamedtuple of (left=Optional[Event], right=Optional[Event]).\nFor a given event, if the predicate function results in True, the event is assigned to left. Otherwise, the\nevent is assigned to right.\nParameters\npredicate – A predicate function that results in a boolean.\nclass storey.transformations.ReifyMetadata(mapping: Iterable[str], **kwargs)[source]#\nInserts event metadata into the event body.\n:param mapping: Dictionary from event attribute name to entry key in the event body (which must be a\ndictionary). Alternatively, an iterable of names may be provided, and these will be used as both\nattribute name and entry key.\nParameters\nname (string) – Name of this step, as it should appear in logs. Defaults to class name (ReifyMetadata).\nclass storey.transformations.SampleWindow(window_size: int, emit_period: storey.steps.sample.EmitPeriod = EmitPeriod.FIRST, emit_before_termination: bool = False, key: Optional[Union[str, Callable[[storey.dtypes.Event], str]]] = None, **kwargs)[source]#\nEmits a single event in a window of window_size events, in accordance with emit_period and\nemit_before_termination.\nParameters\nwindow_size – The size of the window we want to sample a single event from.\nemit_period – What event should this step emit for each window_size (default: EmitPeriod.First).\nAvailable options:1.1) EmitPeriod.FIRST - will emit the first event in a window window_size events.\n1.2) EmitPeriod.LAST - will emit the last event in a window of window_size events.\nParameters\nemit_before_termination – On termination signal, should the step emit the last event it seen (default: False).\nAvailable options:2.1) True - The last event seen will be emitted downstream.\n2.2) False - The last event seen will NOT be emitted downstream.\nParameters\nkey – The key by which events are sampled. By default (None), events are not sampled by key.\nOther options may be:\nSet to ‘$key’ to sample events by the Event.key property.\nset to ‘str’ key to sample events by Event.body[str].\nset a Callable[[Event], str] to sample events by a custom key extractor.\nclass storey.transformations.SendToHttp(request_builder, join_from_response, **kwargs)[source]#\nJoins each event with data from any HTTP source. Used for event augmentation.\nParameters\nrequest_builder (Function (Event=>HttpRequest)) – Creates an HTTP request from the event. This request is then sent to its destination.\njoin_from_response (Function ((Event, HttpResponse)=>Event)) – Joins the original event with the HTTP response into a new event.\nname (string) – Name of this step, as it should appear in logs. Defaults to class name (SendToHttp).\nfull_event (boolean) – Whether user functions should receive and/or return Event objects (when True),\nor only the payload (when False). Defaults to False.\nclass storey.transformations.ToDataFrame(index: Optional[str] = None, columns: Optional[List[str]] = None, **kwargs)[source]#\nCreate pandas data frame from events. Can appear in the middle of the flow, as opposed to ReduceToDataFrame\nParameters\nindex – Name of the column to be used as index. Optional. If not set, DataFrame will be range indexed.\ncolumns – List of column names to be passed as-is to the DataFrame constructor. Optional.\nfor additional params, see documentation of  storey.flow.Flow"}
{"text": "MLRun architecture#\nMLRun started as a community effort to map the different components in the ML project lifecycle, provide a common metadata layer, and automate the operationalization process (a.k.a MLOps).\nInstead of a siloed, complex, and manual process, MLRun enables production pipeline design using a modular strategy,\nwhere the different parts contribute to a continuous, automated, and far simpler path from research and development to scalable\nproduction pipelines without refactoring code, adding glue logic, or spending significant efforts on data and ML engineering.\nMLRun uses Serverless Function technology: write the code once, using your preferred development environment and\nsimple “local” semantics, and then run it as-is on different platforms and at scale. MLRun automates the build process, execution,\ndata movement, scaling, versioning, parameterization, output tracking, CI/CD integration, deployment to production, monitoring, and more.\nThose easily developed data or ML “functions” can then be published or loaded from a hub and used later to form offline or real-time\nproduction pipelines with minimal engineering efforts.\nMLRun deployment#\nMLRun has two main components, the service and the client (SDK):\nThe MLRun service runs over Kubernetes (can also be deployed using local Docker for demo and test purposes). It can orchestrate and integrate with other open source open source frameworks, as shown in the following diagram.\nThe MLRun client SDK is installed in your development environment and interacts with the service using REST API calls.\nMLRun: an integrated and open approach#\nData preparation, model development, model and application delivery, and end to end monitoring are tightly connected:\nthey cannot be managed in silos. This is where MLRun MLOps orchestration comes in. ML, data, and DevOps/MLOps teams\ncollaborate using the same set of tools, practices, APIs, metadata, and version control.\nMLRun provides an open architecture that supports your existing development tools, services, and practices through an open API/SDK and pluggable architecture.\nMLRun simplifies & accelerates the time to production !\nWhile each component in MLRun is independent, the integration provides much greater value and simplicity. For example:\nThe training jobs obtain features from the feature store and update the feature store with metadata, which will be used in the serving or monitoring.\nThe real-time pipeline enriches incoming events with features stored in the feature store. It can also use feature metadata (policies, statistics, schema, etc.) to impute missing data or validate data quality.\nThe monitoring layer collects real-time inputs and outputs from the real-time pipeline and compares them with the features data/metadata from the feature store or model metadata generated by the training layer. Then, it writes all the fresh production data back to the feature store so it can be used for various tasks such as data analysis, model retraining (on fresh data), and model improvements.\nWhen one of the components detailed above is updated, it immediately impacts the feature generation, the model serving pipeline, and the monitoring. MLRun applies versioning to each component, as well as versioning and rolling upgrades across components."}
{"text": "Command-Line Interface#\nCLI commands\nBuilding and running a function from a Git Repository\nUsing a sources archive\nCLI commands#\nUse the following commands of the MLRun command-line interface (CLI) — mlrun — to build and run MLRun functions:\nbuild\nclean\nconfig\nget\nlogs\nproject\nrun\nversion\nwatch\nwatch-stream\nEach command supports many flags, some of which are listed in their relevant sections. To view all the flags of a command, run mlrun <command name> --help.\nbuild#\nUse the build CLI command to build all the function dependencies from the function specification into a function container (Docker image).\nUsage: mlrun build [OPTIONS] FUNC_URL\nExample:  mlrun build myfunc.yaml\nFlag\nDescription\n−−name TEXT\nFunction name\n−−project TEXT\nProject name\n−−tag TEXT\nFunction tag\n-i, −−image TEXT\nTarget image path\n-s, −−source TEXT\nPath/URL of the function source code. A PY file, or if `-a\n-b, −−base-image TEXT\nBase Docker image\n-c, −−command TEXT\nBuild commands; for example, ‘-c pip install pandas’\n−−secret-name TEXT\nName of a container-registry secret\n-a, −−archive TEXT\nPath/URL of a target function-sources archive directory: as part of the build, the function sources (see `-s\n−−silent\nDo not show build logs\n−−with-mlrun\nAdd the MLRun package (“mlrun”)\n−−db TEXT\nSave the run results to path or DB url\n-r, −−runtime TEXT\nFunction spec dict, for pipeline usage\n−−kfp\nRunning inside Kubeflow Piplines, do not use\n−−skip\nSkip if already deployed\nNote: For information about using the -a|--archive option to create a function-sources archive, see Using a Sources Archive later in this tutorial.\nclean#\nUse the clean CLI command to clean runtime resources. When run without any flags, it cleans the resources for all runs of all runtimes.\nUsage: mlrun clean [OPTIONS] [KIND] [id]\nExamples:\nClean resources for all runs of all runtimes:  mlrun clean\nClean resources for all runs of a specific kind (e.g. job):  mlrun clean job\nClean resources for specific job (by uid):  mlrun clean mpijob 15d04c19c2194c0a8efb26ea3017254b\nFlag\nDescription\n−−kind\nClean resources for all runs of a specific kind (e.g. job).\n−−id\nDelete the resources of the mlrun object twith this identifier. For most function runtimes, runtime resources are per Run, and the identifier is the Run’s UID. For DASK runtime, the runtime resources are per Function, and the identifier is the Function’s name.\nOptions\nDescription\n−−api\nURL of the mlrun-api service.\n-ls, −−label-selector\nDelete only runtime resources matching the label selector.\n-f, −−force\nDelete the runtime resource even if they’re not in terminal state or if the grace period didn’t pass.\n-gp, −−grace-period\nGrace period, in seconds, given to the runtime resource before they are actually removed, counted from the moment they moved to the terminal state.\nconfig#\nUse the config CLI command to show the mlrun client environment configuration, such as location of artifacts and api.\nExample:  mlrun config\nget#\nUse the get CLI command to list one or more objects per kind/class.\nUsage: get pods | runs | artifacts | func [name]\nExamples:\nmlrun get runs --project getting-started-admin\nmlrun get pods --project getting-started-admin\nmlrun get artifacts --project getting-started-admin\nmlrun get func prep-data --project getting-started-admin\nFlag\nDescription\n−−name\nName of object to return\n-s, −−selector\nLabel selector\n-n, −−namespace\nKubernetes namespace\n−−uid\nObject ID\n−−project\nProject name to return\n-t, −−tag\nArtifact/function tag of object to return\n−−db\ndb path/url of object to return\nlogs#\nUse the logs CLI command to get or watch task logs.\nUsage: logs [OPTIONS] uid\nExample:  mlrun logs ba409c0cb4904d60aa8f8d1c05b40a75 --project getting-started-admin\nFlag\nDescription\n-p, −−project TEXT\nProject name\n−−offset INTEGER\nRetrieve partial log, get up to size bytes starting at the offset from beginning of log\n−−db TEXT\nAPI service url\n-w, −−watch\nRetrieve logs of a running process, and watch the progress of the execution until it completes. Prints out the logs and continues to periodically poll for, and print, new logs as long as the state of the runtime that generates this log is either pending or running.\nproject#\nUse the project CLI command to load and/or run a project.\nUsage: mlrun project [OPTIONS] [CONTEXT]\nExample:  mlrun project -r workflow.py .\nFlag\nDescription\n-n, −−name TEXT\nProject name\n-u, −−url  TEXT\nRemote git or archive url of the project\n-r, −−run  TEXT\nRun workflow name of .py file\n-a, −−arguments TEXT\nKubeflow pipeline arguments name and value tuples (with -r flag), e.g. -a x=6\n-p, −−artifact_path TEXT\nTarget path/url for workflow artifacts.  The string {{workflow.uid}} is replaced by workflow id\n-x, −−param  TEXT\nmlrun project parameter name and value tuples, e.g. -p x=37 -p y=‘text’\n-s, −−secrets TEXT\nSecrets file= or env=ENV_KEY1,…\n−−namespace TEXT\nk8s namespace\n−−db TEXT\nAPI service url\n−−init_git\nFor new projects init git the context dir\n-c, −−clone\nForce override/clone into the context dir\n−−sync\nSync functions into db\n-w, −−watch\nWait for pipeline completion (with -r flag)\n-d, −−dirty\nAllow run with uncommitted git changes\n−−git_repo TEXT\ngit repo (org/repo) for git comments\n−−git_issue INTEGER\ngit issue number for git comments\n−−handler TEXT\nWorkflow function handler name\n−−engine TEXT\nWorkflow engine (kfp/local)\n−−local\nTry to run workflow functions locally\nrun#\nUse the run CLI command to execute a task and inject parameters by using a local or remote function.\nUsage: mlrun [OPTIONS] URL [ARGS]…\nExamples:\nmlrun run -f db://getting-started-admin/prep-data --project getting-started-admin\nmlrun run -f myfunc.yaml -w -p p1=3\nFlag\nDescription\n-p, −−param TEXT\nParameter name and value tuples; for example, -p x=37 -p y='text'\n-i, −−inputs TEXT\nInput artifact; for example, -i infile.txt=s3://mybucket/infile.txt\n−−in-path TEXT\nBase directory path/URL for storing input artifacts\n−−out-path TEXT\nBase directory path/URL for storing output artifacts\n-s, −−secrets TEXT\nSecrets, either as file=<filename> or env=<ENVAR>,...; for example, -s file=secrets.txt\n−−name TEXT\nRun name\n−−project TEXT\nProject name or ID\n-f, −−func-url TEXT\nPath/URL of a YAML function-configuration file, or db:///[:tag] for a DB function object\n−−task TEXT\nPath/URL of a YAML task-configuration file\n−−handler TEXT\nInvoke the function handler inside the code file\nversion#\nUse the version CLI command to get the mlrun server version.\nThe watch Command#\nUse the watch CLI command to read the current or previous task (pod) logs.\nUsage: mlrun watch [OPTIONS] POD\nExample:  mlrun watch prep-data-6rf7b\nFlag\nDescription\n-n, −−namespace\nkubernetes namespace\n-t, −−timeout\nTimeout in seconds\nwatch-stream#\nUse the watch-stream CLI command to watch a v3io stream and print data at a recurring interval.\nUsage: mlrun watch-stream [OPTIONS] URL\nExamples:\nmlrun watch-stream v3io:///users/my-test-stream\nmlrun watch-stream v3io:///users/my-test-stream -s 1\nmlrun watch-stream v3io:///users/my-test-stream -s 1 -s 2\nmlrun watch-stream v3io:///users/my-test-stream -s 1 -s 2 --seek EARLIEST\nFlag\nDescription\n-s, −−shard-ids\nShard id to listen on (can be multiple).\n–seek TEXT\nWhere to start/seek (EARLIEST or LATEST)\n-i, −−interval\nInterval in seconds. Default = 3\n-j, −−is-json\nIndicates that the payload is json (will be deserialized).\nBuilding and running a function from a Git repository#\nTo build and run a function from a Git repository, start out by adding a YAML function-configuration file in your local environment.\nThis file should describe the function and define its specification.\nFor example, create a myfunc.yaml file with the following content in your working directory:\nkind: job\nmetadata:\nname: remote-demo1\nproject: ''\nspec:\ncommand: 'examples/training.py'\nargs: []\nimage: .mlrun/func-default-remote-demo-ps-latest\nimage_pull_policy: Always\nbuild:\nbase_image: mlrun/mlrun:1.2.0\nsource: git://github.com/mlrun/mlrun\nThen, run the following CLI command and pass the path to your local function-configuration file as an argument to build the function’s container image according to the configured requirements.\nFor example, the following command builds the function using the myfunc.yaml file from the current directory:\nmlrun build myfunc.yaml\nWhen the build completes, you can use the run CLI command to run the function.\nSet the -f option to the path to the local function-configuration file, and pass the relevant parameters.\nFor example:\nmlrun run -f myfunc.yaml -w -p p1=3\nYou can also try the following function-configuration example, which is based on the MLRun CI demo:\nkind: job\nmetadata:\nname: remote-git-test\nproject: default\ntag: latest\nspec:\ncommand: 'myfunc.py'\nargs: []\nimage_pull_policy: Always\nbuild:\ncommands: []\nbase_image: mlrun/mlrun:1.2.0\nsource: git://github.com/mlrun/ci-demo.git\nUsing a sources archive#\nThe -a|--archive option of the CLI build command enables you to define a remote object path for storing TAR archive files with all the required code dependencies.\nThe remote location can be, for example, in an AWS S3 bucket or in a data container in an Iguazio MLOps Platform (“platform”) cluster.\nAlternatively, you can also set the archive path by using the MLRUN_DEFAULT_ARCHIVE environment variable.\nWhen an archive path is provided, the remote builder archives the configured function sources (see the -s|-source build option) into a TAR archive file, and then extracts (untars) all of the archive files (i.e., the function sources) into the configured archive location.\nTo use the archive option, first create a local function-configuration file.\nFor example, you can create a function.yaml file in your working directory with the following content; the specification describes the environment to use, defines a Python base image, adds several packages, and defines examples/training.py as the application to execute on run commands:\nkind: job\nmetadata:\nname: remote-demo4\nproject: ''\nspec:\ncommand: 'examples/training.py'\nargs: []\nimage_pull_policy: Always\nbuild:\ncommands: []\nbase_image: mlrun/mlrun:1.2.0\nNext, run the following MLRun CLI command to build the function; replace the <...> placeholders to match your configuration:\nmlrun build <function-configuration file path> -a <archive path/URL> [-s <function-sources path/URL>]\nFor example, the following command uses the function.yaml configuration file (.), relies on the default function-sources path (./), and sets the target archive path to v3io:///users/$V3IO_USERNAME/tars.\nSo, for a user named “admin”, for example, the function sources from the local working directory will be archived and then extracted into an admin/tars directory in the “users” data container of the configured platform cluster (which is accessed via the v3io data mount):\nmlrun build . -a v3io:///users/$V3IO_USERNAME/tars\nNote:\n. is a shorthand for a function.yaml configuration file in the local working directory.\nThe -a|--archive option is used to instruct MLRun to create an archive file from the function-code sources at the location specified by the -s|--sources option; the default sources location is the current directory (./).\nAfter the function build completes, you can run the function with some parameters.\nFor example:\nmlrun run -f . -w -p p1=3"}
{"text": "Automated experiment tracking#\nYou can write custom training functions or use built-in hub functions for training models using\ncommon open-source frameworks and/or cloud services (such as AzureML, Sagemaker, etc.).\nInside the ML function you can use the apply_mlrun() method, which automates the tracking and MLOps\nfunctionality.\nWith apply_mlrun() the following outputs are generated automatically:\nPlots — loss convergence, ROC, confusion matrix, feature importance, etc.\nMetrics — accuracy, loss, etc.\nDataset artifacts — like the dataset used for training and / or testing\nCustom code — like custom layers, metrics, and so on\nModel artifacts — enables versioning, monitoring and automated deployment\nIn addition it handles automation of various MLOps tasks like scaling runs over multiple containers\n(with Dask, Horovod, and Spark), run profiling, hyperparameter tuning, ML Pipeline, and CI/CD integration, etc.\napply_mlrun() accepts the model object and various optional parameters. For example:\napply_mlrun(model=model, model_name=\"my_model\",\nx_test=x_test, y_test=y_test)\nWhen specifying the x_test and y_test data it generates various plots and calculations to evaluate the model.\nMetadata and parameters are automatically recorded (from the MLRun context object) and don’t need to be specified.\napply_mlrun is framework specific and can be imported from MLRun’s frameworks\npackage — a collection of commonly used machine and deep learning frameworks fully supported by MLRun.\napply_mlrun can be used with its default settings, but it is highly flexible and rich with different options and\nconfigurations. Reading the docs of your favorite framework to get the most out of MLRun:\nSciKit-Learn\nTensorFlow (and Keras)\nPyTorch\nXGBoost\nLightGBM\nONNX"}
{"text": "Data and artifacts#\nOne of the biggest challenge in distributed systems is handling data given the different access methods, APIs, and\nauthentication mechanisms across types and providers.\nWorking with the abstractions enable you to securely access different data sources through a single API, many continuance methods (e.g. to/from DataFrame, get, download, list, …), automated data movement, and versioning.\nMLRun provides these main abstractions to access structured and unstructured data:\nIn this section\nData stores\nData items\nArtifacts\nModel Artifacts"}
{"text": "Decorators and auto-logging#\nWhile it is possible to log results and artifacts using the MLRun execution context, it is often more convenient to use the mlrun.handler() decorator.\nBasic example#\nAssume you have the following code in train.py\nimport pandas as pd\nfrom sklearn.svm import SVC\ndef train_and_predict(train_data,\npredict_input,\nlabel_column='label'):\nx = train_data.drop(label_column, axis=1)\ny = train_data[label_column]\nclf = SVC()\nclf.fit(x, y)\nreturn list(clf.predict(predict_input))\nWith the mlrun.handler the python function itself would not change, and logging of the inputs and outputs would be automatic. The resultant code is as follows:\nimport pandas as pd\nfrom sklearn.svm import SVC\nimport mlrun\n@mlrun.handler(labels={'framework':'scikit-learn'},\noutputs=['prediction:dataset'],\ninputs={\"train_data\": pd.DataFrame,\n\"predict_input\": pd.DataFrame})\ndef train_and_predict(train_data,\npredict_input,\nlabel_column='label'):\nx = train_data.drop(label_column, axis=1)\ny = train_data[label_column]\nclf = SVC()\nclf.fit(x, y)\nreturn list(clf.predict(predict_input))\nTo run the code, use the following example:\nimport mlrun\nproject = mlrun.get_or_create_project(\"mlrun-example\", context=\"./\", user_project=True)\ntrainer = project.set_function(\"train.py\", name=\"train_and_predict\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"train_and_predict\")\ntrainer_run = project.run_function(\n\"train_and_predict\",\ninputs={\"train_data\": mlrun.get_sample_path('data/iris/iris_dataset.csv'),\n\"predict_input\": mlrun.get_sample_path('data/iris/iris_to_predict.csv')\n}\n)\nThe outcome is a run with:\nA label with key “framework” and value “scikit-learn”.\nTwo inputs “train_data” and “predict_input” created from Pandas DataFrame.\nAn artifact called “prediction” of type “dataset”. The contents of the dataset will be the return value (in this case the prediction result).\nLabels#\nThe decorator gives you the option to set labels for the run. The labels parameter is a dictionary with keys and values to set for the labels.\nInput type parsing#\nThe mlrun.handler decorator can also parse the input types, if they are specified. An equivalent definition is as follows:\n@mlrun.handler(labels={'framework':'scikit-learn'},\noutputs=['prediction:dataset'])\ndef train_and_predict(train_data: pd.DataFrame,\npredict_input: pd.DataFrame,\nlabel_column='label'):\n...\nNote: If the inputs does not have a type input, the decorator assumes the parameter type in mlrun.datastore.DataItem. If you specify inputs=False, all the run inputs are assumed to be of type mlrun.datastore.DataItem. You also have the option to specify a dictionary where each key is the name of the input and the value is the type.\nLogging return values as artifacts#\nIf you specify the outputs parameter, the return values will be logged as the run artifacts. outputs expects a list; the length of the list must match the number of returned values.\nThe simplest option is to specify a list of strings. Each string contains the name of the artifact. You can also specify the artifact type by adding a colon after the artifact name followed by the type ('name:artifact_type'). The following are valid artifact types:\ndataset\ndirectory\nfile\nobject\nplot\nresult\nIf you use only the name without the type, the following mapping is used:\nPython type\nArtifact type\npandas.DataFrame\nDataset\npandas.Series\nDataset\nnumpy.ndarray\nDataset\ndict\nResult\nlist\nResult\ntuple\nResult\nstr\nResult\nint\nResult\nfloat\nResult\nbytes\nObject\nbytearray\nObject\nmatplotlib.pyplot.Figure\nPlot\nplotly.graph_objs.Figure\nPlot\nbokeh.plotting.Figure\nPlot\nAnother option is to specify a tuple in the form of (name, artifact_type) or (name, artifact_type, arguments). Refer to the mlrun.handler() for more details."}
{"text": "Kinds of functions (runtimes)#\nWhen you create an MLRun function you need to specify a runtime kind (e.g. kind='job'). Each runtime supports\nits own specific attributes (e.g. Jars for Spark, Triggers for Nuclio, Auto-scaling for Dask, etc.).\nMLRun supports real-time and batch runtimes.\nReal-time runtimes:\nnuclio - real-time serverless functions over Nuclio\nserving - higher level real-time Graph (DAG) over one or more Nuclio functions\nBatch runtimes:\nhandler - execute python handler (used automatically in notebooks or for debug)\nlocal - execute a Python or shell program\njob - run the code in a Kubernetes Pod\ndask - run the code as a Dask Distributed job (over Kubernetes)\nmpijob - run distributed jobs and Horovod over the MPI job operator, used mainly for deep learning jobs\nspark - run the job as a Spark job (using Spark Kubernetes Operator)\nremote-spark - run the job on a remote Spark service/cluster (e.g. Iguazio Spark service)\nCommon attributes for Kubernetes based functions\nAll the Kubernetes based runtimes (Job, Dask, Spark, Nuclio, MPIJob, Serving) support a common\nset of spec attributes and methods for setting the Pods:\nfunction.spec attributes (similar to k8s pod spec attributes):\nvolumes\nvolume_mounts\nenv\nresources\nreplicas\nimage_pull_policy\nservice_account\nimage_pull_secret\ncommon function methods:\nset_env(name, value)\nset_envs(env_vars)\ngpus(gpus, gpu_type)\nwith_limits(mem, cpu, gpus, gpu_type)\nwith_requests(mem, cpu)\nset_env_from_secret(name, secret, secret_key)\nIn this section"}
{"text": "MLRun execution context#\nAfter running a job, you need to be able to track it. To gain the maximum value, MLRun uses the job context object inside\nthe code. This provides access to job metadata, parameters, inputs, secrets, and API for logging and monitoring the results, as well as log text, files, artifacts, and labels.\nIf context is specified as the first parameter in the function signature, MLRun injects the current job context into it.\nAlternatively, if it does not run inside a function handler (e.g. in Python main or Notebook) you can obtain the context\nobject from the environment using the get_or_create_ctx() function.\nCommon context methods:\nget_secret(key: str) — get the value of a secret\nlogger.info(\"started experiment..\")  — textual logs\nlog_result(key: str, value) — log simple values\nset_label(key, value) — set a label tag for that task\nlog_artifact(key, body=None, local_path=None, ...) — log an artifact (body or local file)\nlog_dataset(key, df, ...) — log a dataframe object\nlog_model(key, ...) — log a model object\nExample function and usage of the context object:\nfrom mlrun.artifacts import ChartArtifact\nimport pandas as pd\ndef my_job(context, p1=1, p2=\"x\"):\n# load MLRUN runtime context (will be set by the runtime framework)\n# get parameters from the runtime context (or use defaults)\n# access input metadata, values, files, and secrets (passwords)\nprint(f\"Run: {context.name} (uid={context.uid})\")\nprint(f\"Params: p1={p1}, p2={p2}\")\nprint(\"accesskey = {}\".format(context.get_secret(\"ACCESS_KEY\")))\nprint(\"file\\n{}\\n\".format(context.get_input(\"infile.txt\", \"infile.txt\").get()))\n# Run some useful code e.g. ML training, data prep, etc.\n# log scalar result values (job result metrics)\ncontext.log_result(\"accuracy\", p1 * 2)\ncontext.log_result(\"loss\", p1 * 3)\ncontext.set_label(\"framework\", \"sklearn\")\n# log various types of artifacts (file, web page, table), will be versioned and visible in the UI\ncontext.log_artifact(\n\"model\",\nbody=b\"abc is 123\",\nlocal_path=\"model.txt\",\nlabels={\"framework\": \"xgboost\"},\n)\ncontext.log_artifact(\n\"html_result\", body=b\"<b> Some HTML <b>\", local_path=\"result.html\"\n)\n# create a chart output (will show in the pipelines UI)\nchart = ChartArtifact(\"chart\")\nchart.labels = {\"type\": \"roc\"}\nchart.header = [\"Epoch\", \"Accuracy\", \"Loss\"]\nfor i in range(1, 8):\nchart.add_row([i, i / 20 + 0.75, 0.30 - i / 20])\ncontext.log_artifact(chart)\nraw_data = {\n\"first_name\": [\"Jason\", \"Molly\", \"Tina\", \"Jake\", \"Amy\"],\n\"last_name\": [\"Miller\", \"Jacobson\", \"Ali\", \"Milner\", \"Cooze\"],\n\"age\": [42, 52, 36, 24, 73],\n\"testScore\": [25, 94, 57, 62, 70],\n}\ndf = pd.DataFrame(raw_data, columns=[\"first_name\", \"last_name\", \"age\", \"testScore\"])\ncontext.log_dataset(\"mydf\", df=df, stats=True)\nExample of creating the context objects from the environment:\nif __name__ == \"__main__\":\ncontext = mlrun.get_or_create_ctx('train')\np1 = context.get_param('p1', 1)\np2 = context.get_param('p2', 'a-string')\n# do something\ncontext.log_result(\"accuracy\", p1 * 2)\n# commit the tracking results to the DB (and mark as completed)\ncontext.commit(completed=True)\nNote that MLRun context is also a python context and can be used in a with statement (eliminating the need for commit).\nif __name__ == \"__main__\":\nwith mlrun.get_or_create_ctx('train') as context:\np1 = context.get_param('p1', 1)\np2 = context.get_param('p2', 'a-string')\n# do something\ncontext.log_result(\"accuracy\", p1 * 2)"}
{"text": "Model monitoring#\nBy definition, ML models in production make inferences on constantly changing data. Even models that have been trained on massive data sets, with the most meticulously labelled data, start to degrade over time, due to concept drift. Changes in the live environment due to changing behavioral patterns, seasonal shifts, new regulatory environments, market volatility, etc., can have a big impact on a trained model’s ability to make accurate predictions.\nModel performance monitoring is a basic operational task that is implemented after an AI model has been deployed. Model monitoring includes:\nBuilt-in model monitoring:\nMachine learning model monitoring is natively built in to the Iguazio MLOps Platform, along with a wide range of\nmodel management features and ML monitoring reports. It monitors all of your models in a single, simple, dashboard.\nAutomated drift detection:\nAutomatically detects concept drift, anomalies, data skew, and model drift in real-time. Even if you are running hundreds of\nmodels simultaneously, you can be sure to spot and remediate the one that has drifted.\nAutomated retraining:\nWhen drift is detected, Iguazio automatically starts the entire training pipeline to retrain the model, including all relevant\nsteps in the pipeline. The output is a production-ready challenger model, ready to be deployed. This keeps your models up to date,\nautomatically.\nNative feature store integration:\nFeature vectors and labels are stored and analyzed in the Iguazio feature store and are easily compared to the trained\nfeatures and labels running as part of the model development phase, making it easier for data science teams to\ncollaborate and maintain consistency between AI projects.\nSee full details and examples in Model monitoring."}
{"text": "Node affinity#\nYou can assign a node or a node group for services or for jobs executed by a service. When specified, the service or the pods of a function can only run on nodes whose\nlabels match the node selector entries configured for the specific service. If node selection for the service is not specified, the\nselection criteria defaults to the Kubernetes default behavior, and jobs run on a random node.\nFor MLRun and Nuclio, you can also specify node selectors on a per-job basis. The default node selectors (defined at the service level) are\napplied to all jobs unless you specifically override them for an individual job.\nYou can configure node affinity for:\nJupyter\nPresto (The node selection also affects any additional services that are directly affected by Presto, for example hive and mariadb,\nwhich are created if Enable hive is checked in the Presto service.)\nGrafana\nShell\nMLRun (default value applied to all jobs that can be overwritten for individual jobs)\nNuclio (default value applied to all jobs that can be overwritten for individual jobs)\nSee more about Kubernetes nodeSelector.\nUI configuration#\nConfigure node selection on the service level in the service’s Custom Parameters tab, under Resources, by adding or removing\nKey:Value pairs. For MLRun and Nuclio, this is the default node selection for all MLRun jobs and Nuclio functions.\nYou can also configure the node selection for individual MLRun jobs by going to Platform dashboard | Projects | New Job | Resources | Node\nselector, and adding or removing Key:Value pairs. Configure the node selection for individual Nuclio functions when creating a function in\nthe Confguration tab, under Resources, by adding Key:Value pairs.\nSDK configuration#\nConfigure node selection by adding the key:value pairs in your Jupyter notebook formatted as a Python dictionary.\nFor example:\nimport mlrun\nimport os\ntrain_fn = mlrun.code_to_function('training',\nkind='job',\nhandler='my_training_function')\ntrain_fn.with_preemption_mode(mode=\"prevent\")\ntrain_fn.run(inputs={\"dataset\" :my_data})\n# Add node selection\nfunc.with_node_selection(node_selector={name})\nSee with_node_selection."}
{"text": "Nuclio real-time functions#\nNuclio is a high-performance “serverless” framework focused on data, I/O, and compute intensive workloads. It is well integrated with popular\ndata science tools, such as Jupyter and Kubeflow; supports a variety of data and streaming sources; and supports execution over CPUs and GPUs.\nYou can use Nuclio through a fully managed application service (in the cloud or on-prem) in the Iguazio MLOps Platform. MLRun serving\nutilizes serverless Nuclio functions to create multi-stage real-time pipelines.\nThe underlying Nuclio serverless engine uses a high-performance parallel processing engine that maximizes the utilization of CPUs and GPUs,\nsupports 13 protocols and invocation methods (for example, HTTP, Cron, Kafka, Kinesis), and includes dynamic auto-scaling for HTTP and\nstreaming. Nuclio and MLRun support the full life cycle, including auto-generation of micro-services, APIs, load-balancing, logging,\nmonitoring, and configuration management—such that developers can focus on code, and deploy to production faster with minimal work.\nNuclio is extremely fast: a single function instance can process hundreds of thousands of HTTP requests or data records per second. To learn\nmore about how Nuclio works, see the Nuclio architecture documentation.\nNuclio is secure: Nuclio is integrated with Kaniko to allow a secure and production-ready way of building Docker images at run time.\nRead more in the Nuclio documentation and the open-source MLRun library.\nWhy another “serverless” project?#\nNone of the existing cloud and open-source serverless solutions addressed all the desired capabilities of a serverless framework:\nReal-time processing with minimal CPU/GPU and I/O overhead and maximum parallelism\nNative integration with a large variety of data sources, triggers, processing models, and ML frameworks\nStateful functions with data-path acceleration\nSimple debugging, regression testing, and multi-versioned CI/CD pipelines\nPortability across low-power devices, laptops, edge and on-prem clusters, and public clouds\nOpen-source but designed for the enterprise (including logging, monitoring, security, and usability)\nNuclio was created to fulfill these requirements. It was intentionally designed as an extendable open-source framework, using a modular and layered approach that supports constant addition of triggers and data sources, with the hope that many will join the effort of developing new modules, developer tools, and platforms for Nuclio."}
{"text": "Batch runs and workflows#\nIn this section\nMLRun execution context\nDecorators and auto-logging\nRunning a task (job)\nRunning a multi-stage workflow\nScheduled jobs and workflows"}
{"text": "Scheduled jobs and workflows#\nOftentimes you may want to run a job on a regular schedule. For example, fetching from a datasource every morning, compiling an analytics report every month, or detecting model drift every hour.\nCreating a job and scheduling it#\nMLRun makes it very simple to add a schedule to a given job. To showcase this, the following job runs the code below, which resides in a file titled schedule.py:\ndef hello(context):\nprint(\"You just ran a scheduled job!\")\nTo create the job, use the code_to_function syntax and specify the kind like below:\nimport mlrun\njob = mlrun.code_to_function(\nname=\"my-scheduled-job\",      # Name of the job (displayed in console and UI)\nfilename=\"schedule.py\",       # Python file or Jupyter notebook to run\nkind=\"job\",                   # Run as a job\nimage=\"mlrun/mlrun\",          # Use this Docker image\nhandler=\"hello\"               # Execute the function hello() within code.py\n)\nRunning the job using a schedule\nTo add a schedule, run the job and specify the schedule parameter using Cron syntax like so:\njob.run(schedule=\"0 * * * *\")\nThis runs the job every hour. An excellent resource for generating Cron schedules is Crontab.guru.\nScheduling a workflow#\nAfter loading the project (load_project), run the project with the scheduled workflow:\nproject.run(\"main\", schedule='0 * * * *')"}
{"text": "Running a task (job)#\nIn this section\nSubmit tasks (jobs) using run_function\nRun result object and UI\nSubmit tasks (jobs) using run_function#\nUse the run_function() method for invoking a job over MLRun batch functions.\nThe run_function method accepts various parameters such as name, handler, params, inputs, schedule, etc.\nAlternatively, you can pass a Task object (see: new_task()) that holds all of the\nparameters plus the advanced options.\nFunctions can host multiple methods (handlers). You can set the default handler per function. You\nneed to specify which handler you intend to call in the run command.\nYou can pass parameters (arguments) or data inputs (such as datasets, feature-vectors, models, or files) to the functions through the run method.\nInside the function you can access the parameters/inputs by simply adding them as parameters to the function, or you can get them from the context object (using get_param() and  get_input()).\nVarious data objects (files, tables, models, etc.) are passed to the function as data item objects. You can pass data objects using the\ninputs dictionary argument, where the dictionary keys match the function’s handler argument names and the MLRun data urls are provided\nas the values. The data is passed into the function as a DataItem object that handles data movement,\ntracking, and security in an optimal way. Read more about data objects in Data stores.\nYou can use run_function as a project methods, or as global (mlrun.) methods. For example:\n# run the \"train\" function in myproject\nrun_results = myproject.run_function(\"train\", inputs={\"data\": data_url})\n# run the \"train\" function in the current/active project (or in a pipeline)\nrun_results = mlrun.run_function(\"train\", inputs={\"data\": data_url})\nThe first parameter in run_function is the function name (in the project), or it can be a function object if you want to\nuse functions that you imported/created ad hoc, or modify a function spec, for example:\nrun_results = project.run_function(fn, params={\"label_column\": \"label\"}, inputs={'data': data_url})\nRun/simulate functions locally:\nFunctions can also run and be debugged locally by using the local runtime or by setting the local=True\nparameter in the run() method (for batch functions).\nMLRun also supports iterative jobs that can run and track multiple child jobs (for hyperparameter tasks, AutoML, etc.).\nSee Hyperparameter tuning optimization for details and examples.\nRun result object and UI#\nThe run_function() command returns an MLRun RunObject object that you can use to track the job and its results.\nIf you pass the parameter watch=True (default) the command blocks until the job completes.\nRun object has the following methods/properties:\nuid() — returns the unique ID.\nstate() — returns the last known state.\nshow() — shows the latest job state and data in a visual widget (with hyperlinks and hints).\noutputs — returns a dictionary of the run results and artifact paths.\nlogs(watch=True) — returns the latest logs.\nUse Watch=False to disable the interactive mode in running jobs.\nartifact(key) — returns an artifact for the provided key (as DataItem object).\noutput(key) — returns a specific result or an artifact path for the provided key.\nwait_for_completion() — wait for async run to complete\nrefresh() — refresh run state from the db/service\nto_dict(), to_yaml(), to_json() — converts the run object to a dictionary, YAML, or JSON format (respectively).\nYou can view the job details, logs, and artifacts in the UI. When you first open the Monitor\nJobs tab it displays the last jobs that ran and their data. Click a job name to view its run history, and click a run to view more of the\nrun’s data.\nSee full details and examples in Functions."}
{"text": "Running a multi-stage workflow#\nA workflow is a definition of execution of functions. It defines the order of execution of multiple dependent steps in a\ndirected acyclic graph (DAG). A workflow\ncan reference the project’s params, secrets, artifacts, etc. It can also use a function execution output as a function execution\ninput (which, of course, defines the order of execution).\nMLRun supports running workflows on a local or kubeflow pipeline engine. The local engine runs the workflow as a\nlocal process, which is simpler for debugging and running simple/sequential tasks. The kubeflow (“kfp”) engine runs as a task over the\ncluster and supports more advanced operations (conditions, branches, etc.). You can select the engine at runtime. Kubeflow-specific\ndirectives like conditions and branches are not supported by the local engine.\nWorkflows are saved/registered in the project using the set_workflow().\nWorkflows are executed using the run() method or using the CLI command mlrun project.\nRefer to the Tutorials and Examples for complete examples.\nIn this section\nComposing workflows\nSaving workflows\nRunning workflows\nComposing workflows#\nWorkflows are written as python functions that make use of  function operations (run, build, deploy)\nand can access project parameters, secrets, and artifacts using get_param(), get_secret() and get_artifact_uri().\nFor workflows to work in Kubeflow you need to add a decorator (@dsl.pipeline(..)) as shown below.\nExample workflow:\nfrom kfp import dsl\nimport mlrun\nfrom mlrun.model import HyperParamOptions\nfuncs = {}\nDATASET = \"iris_dataset\"\nin_kfp = True\n@dsl.pipeline(name=\"Demo training pipeline\", description=\"Shows how to use mlrun.\")\ndef newpipe():\nproject = mlrun.get_current_project()\n# build our ingestion function (container image)\nbuilder = mlrun.build_function(\"gen-iris\")\n# run the ingestion function with the new image and params\ningest = mlrun.run_function(\n\"gen-iris\",\nname=\"get-data\",\nparams={\"format\": \"pq\"},\noutputs=[DATASET],\n).after(builder)\n# train with hyper-parameters\ntrain = mlrun.run_function(\n\"train\",\nname=\"train\",\nparams={\"sample\": -1, \"label_column\": project.get_param(\"label\", \"label\"), \"test_size\": 0.10},\nhyperparams={\n\"model_pkg_class\": [\n\"sklearn.ensemble.RandomForestClassifier\",\n\"sklearn.linear_model.LogisticRegression\",\n\"sklearn.ensemble.AdaBoostClassifier\",\n]\n},\nhyper_param_options=HyperParamOptions(selector=\"max.accuracy\"),\ninputs={\"dataset\": ingest.outputs[DATASET]},\noutputs=[\"model\", \"test_set\"],\n)\nprint(train.outputs)\n# test and visualize our model\nmlrun.run_function(\n\"test\",\nname=\"test\",\nparams={\"label_column\": project.get_param(\"label\", \"label\")},\ninputs={\n\"models_path\": train.outputs[\"model\"],\n\"test_set\": train.outputs[\"test_set\"],\n},\n)\n# deploy our model as a serverless function, we can pass a list of models to serve\nserving = mlrun.import_function(\"hub://v2_model_server\", new_name=\"serving\")\ndeploy = mlrun.deploy_function(\nserving,\nmodels=[{\"key\": f\"{DATASET}:v1\", \"model_path\": train.outputs[\"model\"]}],\n)\n# test out new model server (via REST API calls), use imported function\ntester = mlrun.import_function(\"hub://v2_model_tester\", new_name=\"live_tester\")\nmlrun.run_function(\ntester,\nname=\"model-tester\",\nparams={\"addr\": deploy.outputs[\"endpoint\"], \"model\": f\"{DATASET}:v1\"},\ninputs={\"table\": train.outputs[\"test_set\"]},\n)\nSaving workflows#\nIf you want to use workflows as part of an automated flow, save them and register them in the project.\nUse the set_workflow() method to register workflows, to specify a workflow name,\nthe path to the workflow file, and the function handler name (or it looks for a handler named “pipeline”), and can\nset the default engine (local or kfp).\nWhen setting the embed flag to True, the workflow code is embedded in the project file (can be used if you want to\ndescribe the entire project using a single YAML file).\nYou can define the schema for workflow arguments (data type, default, doc, etc.) by setting the args_schema with a list\nof EntrypointParam objects.\nExample:\n# define agrument for the workflow\narg = mlrun.model.EntrypointParam(\n\"model_pkg_class\",\ntype=\"str\",\ndefault=\"sklearn.linear_model.LogisticRegression\",\ndoc=\"model package/algorithm\",\n)\n# register the workflow in the project and save the project\nproject.set_workflow(\"main\", \"./myflow.py\", handler=\"newpipe\", args_schema=[arg])\nproject.save()\n# run the workflow\nproject.run(\"main\", arguments={\"model_pkg_class\": \"sklearn.ensemble.RandomForestClassifier\"})\nRunning workflows#\nUse the run() method to execute workflows. Specify the workflow using its name\nor workflow_path (path to the workflow file) or workflow_handler (the workflow function handler).\nYou can specify the input arguments for the workflow and can override the system default artifact_path.\nWorkflows are asynchronous by default. You can set the watch flag to True and the run operation blocks until\ncompletion and prints out the workflow progress. Alternatively, you can use .wait_for_completion() on the run object.\nThe default workflow engine is kfp. You can override it by specifying the engine in the run() or set_workflow() methods.\nUsing the local engine executes the workflow state machine locally (its functions still run as cluster jobs).\nIf you set the local flag to True, the workflow uses the local engine AND the functions run as local process.\nThis mode is used for local debugging of workflows.\nWhen running workflows from a git enabled context it first verifies that there are no uncommitted git changes\n(to guarantee that workflows that load from git do not use old code versions). You can suppress that check by setting the dirty flag to True.\nExamples:\n# simple run of workflow 'main' with arguments, block until it completes (watch=True)\nrun = project.run(\"main\", arguments={\"param1\": 6}, watch=True)\n# run workflow specified with a function handler (my_pipe)\nrun = project.run(workflow_handler=my_pipe)\n# wait for pipeline completion\nrun.wait_for_completion()\n# run workflow in local debug mode\nrun = project.run(workflow_handler=my_pipe, local=True, arguments={\"param1\": 6})\nNotification#\nInstead of waiting for completion, you can set up a notification in Slack with a results summary, similar to:\nUse one of:\n# If you want to get slack notification after the run with the results summary, use\n# project.notifiers.slack(webhook=\"https://<webhook>\")\nor in a Jupyter notebook with the %env magic command:\n%env SLACK_WEBHOOK=<slack webhook url>"}
{"text": "Ingest and process data#\nMLRun provides a set of tools and capabilities to streamline the task of data ingestion and processing. For an\nend-to-end framework for data processing, management, and serving, MLRun has the feature-store capabilities, which are\ndescribed in Feature store. However, in many cases the full feature-store capabilities are\nnot needed, in which cases MLRun provides a set of utilities to facilitate data ingestion, collection and processing.\nIn this section\nUsing data sources and items\nLogging datasets\nSpark Operator runtime\nIngest data using the feature store\nIngest features with Spark"}
{"text": "Ingest data using the feature store#\nDefine the source and material targets, and start the ingestion process (as local process, using an MLRun job, real-time ingestion, or incremental ingestion).\nData can be ingested as a batch process either by running the ingest command on demand or as a scheduled job. Batch ingestion\ncan be done locally (i.e. running as a python process in the Jupyter pod) or as an MLRun job.\nThe data source can be a DataFrame or files (e.g. csv, parquet). Files can be either local files residing on a volume (e.g. v3io),\nor remote (e.g. S3, Azure blob). MLRun also supports Google BigQuery as a data source. If you define a transformation graph, then\nthe ingestion process runs the graph transformations, infers metadata and stats, and writes the results to a target data store.\nWhen targets are not specified, data is stored in the configured default targets (i.e. NoSQL for real-time and Parquet for offline).\nLimitations\nDo not name columns starting with either _ or aggr_. They are reserved for internal use. See\nalso general limitations in Attribute name restrictions.\nWhen using the pandas engine, do not use spaces ( ) or periods (.) in the column names. These cause errors in the ingestion.\nIn this section\nInferring data\nIngest data locally\nIngest data using an MLRun job\nReal-time ingestion\nIncremental ingestion\nData sources\nTarget stores\nInferring data#\nThere are two types of inferring:\nMetadata/schema: This is responsible for describing the dataset and generating its meta-data, such as deducing the\ndata-types of the features and listing the entities that are involved. Options belonging to this type are\nEntities, Features and Index. The InferOptions class has the InferOptions.schema() function which returns a value\ncontaining all the options of this type.\nStats/preview: Ths related to calculating statistics and generating a preview of the actual data in the dataset.\nOptions of this type are Stats, Histogram and Preview.\nThe InferOptions class has the following values:\nclass InferOptions:\nNull = 0\nEntities = 1\nFeatures = 2\nIndex = 4\nStats = 8\nHistogram = 16\nPreview = 32\nThe InferOptions class basically translates to a value that can be a combination of the above values. For example, passing a value of 24 means Stats + Histogram.\nWhen simultanesouly ingesting data and requesting infer options, part of the data might be ingested twice: once for inferring metadata/stats and once for the actual ingest. This is normal behavior.\nIngest data locally#\nUse a Feature Set to create the basic feature-set definition and then an ingest method to run a simple ingestion “locally” in the Jupyter Notebook pod.\n# Simple feature set that reads a csv file as a dataframe and ingests it as is\nstocks_set = FeatureSet(\"stocks\", entities=[Entity(\"ticker\")])\nstocks = pd.read_csv(\"stocks.csv\")\ndf = ingest(stocks_set, stocks)\n# Specify a csv file as source, specify a custom CSV target\nsource = CSVSource(\"mycsv\", path=\"stocks.csv\")\ntargets = [CSVTarget(\"mycsv\", path=\"./new_stocks.csv\")]\ningest(measurements, source, targets)\nTo learn more about ingest go to ingest.\nIngest data using an MLRun job#\nUse the ingest method with the run_config parameter for running the ingestion process using a serverless MLRun job.\nBy doing that, the ingestion process runs on its own pod or service on the kubernetes cluster.\nThis option is more robust since it can leverage the cluster resources, as opposed to running within the Jupyter Notebook.\nIt also enables you to schedule the job or use bigger/faster resources.\n# Running as remote job\nstocks_set = FeatureSet(\"stocks\", entities=[Entity(\"ticker\")])\nconfig = RunConfig(image='mlrun/mlrun')\ndf = ingest(stocks_set, stocks, run_config=config)\nReal-time ingestion#\nReal-time use cases (e.g. real time fraud detection) require feature engineering on live data (e.g. z-score calculation)\nwhile the data is coming from a streaming engine (e.g. kafka) or a live http endpoint.\nThe feature store enables you to start real-time ingestion service.\nWhen running the deploy_ingestion_service the feature store creates an elastic real-time serverless function\n(the nuclio function) that runs the pipeline and stores the data results in the “offline” and “online” feature store by default.\nThere are multiple data source options including http, kafka, kinesis, v3io stream, etc.\nDue to the asynchronous nature of feature store’s execution engine, errors are not returned, but rather logged and pushed to the defined\nerror stream.\n# Create a real time function that receives http requests\n# the \"ingest\" function runs the feature engineering logic on live events\nsource = HTTPSource()\nfunc = mlrun.code_to_function(\"ingest\", kind=\"serving\").apply(mount_v3io())\nconfig = RunConfig(function=func)\nfstore.deploy_ingestion_service(my_set, source, run_config=config)\nTo learn more about deploy_ingestion_service go to deploy_ingestion_service.\nIncremental ingestion#\nYou can schedule an ingestion job for a feature set on an ongoing basis. The first scheduled job runs on all the data in the source and the subsequent jobs ingest only the deltas since the previous run (from the last timestamp of the previous run until datetime.now).\nExample:\ncron_trigger = \"* */1 * * *\" #will run every hour\nsource = ParquetSource(\"myparquet\", path=path, time_field=\"time\", schedule=cron_trigger)\nfeature_set = fs.FeatureSet(name=name, entities=[fs.Entity(\"first_name\")], timestamp_key=\"time\",)\nfs.ingest(feature_set, source, run_config=fs.RunConfig())\nThe default value for the overwrite parameter in the ingest function for scheduled ingest is False, meaning that the target from the previous ingest is not deleted.\nFor the storey engine, the feature is currently implemented for ParquetSource only. (CsvSource will be supported in a future release). For Spark engine, other sources are also supported.\nData sources#\nFor batch ingestion the feature store supports dataframes and files (i.e. csv & parquet).\nThe files can reside on S3, NFS, Azure blob storage, or the Iguazio platform. MLRun also supports Google BigQuery as a data source.\nWhen working with S3/Azure, there are additional requirements. Use pip install mlrun[s3] or pip install mlrun[azure-blob-storage] to install them.\nAzure: define the environment variable AZURE_STORAGE_CONNECTION_STRING.\nS3: define AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_BUCKET.\nFor real time ingestion the source can be http, kafka or v3io stream, etc.\nWhen defining a source, it maps to nuclio event triggers.\nYou can also create a custom source to access various databases or data sources.\nTarget stores#\nBy default, the feature sets are saved in parquet and the Iguazio NoSQL DB (NoSqlTarget).\nThe parquet file is ideal for fetching large set of data for training while the key value is ideal for an online application since it supports low latency data retrieval based on key access.\nNote\nWhen working with the Iguazio MLOps platform the default feature set storage location is under the “Projects” container: <project name>/fs/.. folder.\nThe default location can be modified in mlrun config or specified per ingest operation. The parquet/csv files can be stored in NFS, S3, Azure blob storage, Redis, and on Iguazio DB/FS.\nRedis target store (Tech preview)#\nThe Redis online target is called, in MLRun, RedisNoSqlTarget. The functionality of the RedisNoSqlTarget is identical to the NoSqlTarget except for:\nThe RedisNoSqlTarget does not support the spark engine, (only supports the storey engine).\nThe RedisNoSqlTarget accepts path parameter in the form <redis|rediss>://[<username>]:[<password>]@<host>[:port]\nFor example: rediss://:abcde@localhost:6379 creates a redis target, where:\nThe client/server protocol (rediss) is TLS protected (vs. “redis” if no TLS is established)\nThe server is password protected (password=“abcde”)\nThe server location is localhost port 6379.\nA default path can be configured in redis.url config (mlrun client has priority over mlrun server), and can be overwritten by MLRUN_REDIS__URL env var.\nTwo types of Redis servers are supported: StandAlone and Cluster (no need to specify the server type in the config).\nA feature set supports one online target only. Therefore RedisNoSqlTarget and NoSqlTarget cannot be used as two targets of the same feature set.\nTo use the Redis online target store, you can either change the default to be parquet and Redis, or you can specify the Redis target\nexplicitly each time with the path parameter, for example:\nRedisNoSqlTarget(path =\"redis://1.2.3.4:6379\")"}
{"text": "Using data sources and items#\nIn this section\nConnecting to data sources\nData processing\nConnecting to data sources#\nAccessing data from multiple source types is possible through MLRun’s DataItem object. This object plugs into the\ndata-stores framework to connect to various types of data sources and download content. For example, to download\ndata which is stored on S3 and load it into a DataFrame, use the following code:\n# Access object in AWS S3, in the \"input-data\" bucket\nimport mlrun\n# Access credentials\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"<access key ID>\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<access key>\"\nsource_url = \"s3://input-data/input_data.csv\"\ninput_data = mlrun.get_dataitem(source_url).as_df()\nThis code runs locally (for example, in Jupyter) and relies on environment variables to supply credentials for data\naccess. See Data stores for more info on the available data-stores, accessing them locally and\nremotely, and how to provide credentials for connecting.\nRunning the code locally is very useful for easy debugging and development of the code.\nWhen the code moves to a stable status, it is usually recommended to run it “remotely” on a pod running in the\nKubernetes cluster. This allows setting up specific resources to the processing pod\n(such as memory, CPU and execution priority).\nMLRun provides facilities to create DataItem objects as inputs to running code. For example, this is a basic\ndata ingestion function:\ndef ingest_data(context, source_url: mlrun.DataItem):\n# Load the data from its source, and convert to a DataFrame\ndf = source_url.as_df()\n# Perform data cleaning and processing\n# ...\n# Save the processed data to the artifact store\ncontext.log_dataset('cleaned_data', df=df, format='csv')\nThis code can be placed in a python file, or as a cell in the Python notebook. For example, if the code above was saved\nto a file, the following code creates an MLRun function from it and executes it remotely in a pod:\n# create a function from py or notebook (ipynb) file, specify the default function handler\ningest_func = mlrun.code_to_function(name='ingest_data', filename='./ingest_data.py',\nkind='job', image='mlrun/mlrun')\nsource_url = \"s3://input-data/input_data.csv\"\ningest_data_run = ingest_func.run(name='ingest_data',\nhandler=ingest_data,\ninputs={'source_url': source_url},\nlocal=False)\nAs the source_url is part of the function’s inputs, MLRun automatically wraps it up with a DataItem. The output\nis logged to the function’s artifact_path, and can be obtained from the run result:\ncleaned_data_frame = ingest_data_run.artifact('cleaned_data').as_df()\nNote that running the function remotely may require attaching storage to the function, as well as passing storage\ncredentials through project secrets. See the following pages for more details:\nAttach storage to functions\nWorking with secrets\nData processing#\nOnce the data is imported from its source, it can be processed using any framework. MLRun natively supports working\nwith Pandas DataFrames and converting from and to its DataItem object.\nFor distributed processing of very large datasets, MLRun integrates with the Spark processing engine, and provides\nfacilities for executing pySpark code using a Spark service (which can be deployed by the platform when running MLRun\nas part of an Iguazio system) or through submitting the processing task to Spark-operator. The following page provides\nadditional details and code-samples:\nSpark operator\nIn a similar manner, Dask can be used for parallel processing of the data. To read data as a Dask DataFrame, use the\nfollowing code:\nimport dask.dataframe as dd\ndata_item = mlrun.get_dataitem(source_url)\ndask_df: dd.DataFrame = data_item.as_df(df_module=dd)"}
{"text": "Logging datasets#\nStoring datasets is important in order to have a record of the data that was used to train\nmodels, as well as storing any processed data. MLRun comes with built-in support for the DataFrame format. MLRun not\nonly stores the DataFrame, but it also provides information about the data, such as statistics.\nThe simplest way to store a dataset is with the following code:\ncontext.log_dataset(key='my_data', df=df)\nWhere key is the name of the artifact and df is the DataFrame. By default, MLRun stores a short preview of 20 lines.\nYou can change the number of lines by changing the value of the preview parameter.\nMLRun also calculates statistics on the DataFrame on all numeric fields. You can enable statistics regardless to the\nDataFrame size by setting the stats parameter to True.\nLogging a dataset from a job#\nThe following example shows how to work with datasets from a job:\nfrom os import path\nfrom mlrun.execution import MLClientCtx\nfrom mlrun.datastore import DataItem\n# Ingest a data set into the platform\ndef get_data(context: MLClientCtx, source_url: DataItem, format: str = 'csv'):\niris_dataset = source_url.as_df()\ntarget_path = path.join(context.artifact_path, 'data')\n# Optionally print data to your logger\ncontext.logger.info('Saving Iris data set to {} ...'.format(target_path))\n# Store the data set in your artifacts database\ncontext.log_dataset('iris_dataset', df=iris_dataset, format=format,\nindex=False, artifact_path=target_path)\nYou can run this function locally or as a job. For example, to run it locally:\nfrom os import path\nfrom mlrun import new_project, run_local, mlconf\nproject_name = 'my-project'\nproject_path = path.abspath('conf')\nproject = new_project(project_name, project_path, init_git=True)\n# Target location for storing pipeline artifacts\nartifact_path = path.abspath('jobs')\n# MLRun DB path or API service URL\nmlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\nsource_url = 'https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv'\n# Run get-data function locally\nget_data_run = run_local(name='get_data',\nhandler=get_data,\ninputs={'source_url': source_url},\nproject=project_name,\nartifact_path=artifact_path)\nThe dataset location is returned in the outputs field, therefore you can get the location by calling\nget_data_run.artifact('iris_dataset') to get the dataset itself.\n# Read your data set\nget_data_run.artifact('iris_dataset').as_df()\n# Visualize an artifact in Jupyter (image, html, df, ..)\nget_data_run.artifact('confusion-matrix').show()\nThe dataset returned from the run result is of the DataItem type. It allows access to the data itself as a Pandas\nDataframe by calling the dataset.as_df(). It also contains the metadata of the artifact, accessed by the using\ndataset.meta. This artifact metadata object contains in it the statistics calculated, the schema of the dataset and\nother fields describing the dataset. For example, call dataset.meta.stats to obtain the data statistics."}
{"text": "Batch inference#\nBatch inference or offline inference addresses the need to run machine learning model on large datasets. It is the process of generating outputs on a batch of observations.\nWith batch inference, the batch runs are typically generated during some recurring schedule (e.g., hourly, or daily). These inferences are then stored in a database or a file and can be made available to developers or end users. With batch inference, the goal is usually tied to time constraints and the service-level agreement (SLA) of the job. Conversely, in real time serving, the goal is usually to optimize the number of transactions per second that the model can process. An online application displays a result to the user.\nBatch inference can sometimes take advantage of big data technologies such as Spark to generate predictions. Big data technologies allows data scientists and machine learning engineers to take advantage of scalable compute resources to generate many predictions at once.\nTest your model#\nTo evaluate batch model prior to deployment, you should use the evaluate handler of the auto_trainer function.\nThis is typically done during model development. For more information refer to the Evaluate handler documentation. For example:\nimport mlrun\n# Set the base project name\nproject_name_base = 'batch-inference'\n# Initialize the MLRun project object\nproject = mlrun.get_or_create_project(project_name_base, context=\"./\", user_project=True)\nauto_trainer = mlrun.import_function(mlrun.import_function(\"hub://auto_trainer\"))\nevaluate_run = project.run_function(\nauto_trainer,\nhandler=\"evaluate\",\ninputs={\"dataset\": train_run.outputs['test_set']},\nparams={\n\"model\": train_run.outputs['model'],\n\"label_columns\": \"labels\",\n},\n)\nDeploy your model#\nBatch inference is implemented in MLRun by running the function with an input dataset. With MLRun you can easily create any custom logic in a function, including loading a model and calling it.\nThe Function Hub batch inference function is used for running the models in batch as well as performing drift analysis. The function supports the following frameworks:\nScikit-learn\nXGBoost\nLightGBM\nTensorflow/Keras\nPyTorch\nONNX\nInternally the function uses MLRun’s out-of-the-box capability to load run a model via the mlrun.frameworks.auto_mlrun.auto_mlrun.AutoMLRun class.\nBasic example#\nThe simplest example to run the function is as follows:\nCreate project#\nImport MLRun and create a project:\nimport mlrun\nproject = mlrun.get_or_create_project('batch-inference', context=\"./\", user_project=True)\nbatch_inference = mlrun.import_function(\"hub://batch_inference\")\nGet the model#\nGet the model. The model is a decision tree classifier from scikit-learn. Note that if you previously trained your model using MLRun, you can reference the model artifact produced during that training process.\nmodel_path = mlrun.get_sample_path('models/batch-predict/model.pkl')\nmodel_artifact = project.log_model(\nkey=\"model\",\nmodel_file=model_path,\nframework=\"sklearn\"\n)\nGet the data#\nGet the dataset to perform the inference. The dataset is in parquet format.\nprediction_set_path = mlrun.get_sample_path('data/batch-predict/prediction_set.parquet')\nRun the batch inference function#\nRun the inference. In the first example we will not perform any drift analysis\nbatch_run = project.run_function(\nbatch_inference,\ninputs={\"dataset\": prediction_set_path},\nparams={\"model\": model_artifact.uri},\n)\nFunction output#\nThe output of the function is an artifact called prediction:\nbatch_run.artifact(\"prediction\").as_df().head()\nfeature_0\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\nfeature_7\nfeature_8\nfeature_9\n...\nfeature_11\nfeature_12\nfeature_13\nfeature_14\nfeature_15\nfeature_16\nfeature_17\nfeature_18\nfeature_19\npredicted_label\n0\n-2.059506\n-1.314291\n2.721516\n-2.132869\n-0.693963\n0.376643\n3.017790\n3.876329\n-1.294736\n0.030773\n...\n2.775699\n2.361580\n0.173441\n0.879510\n1.141007\n4.608280\n-0.518388\n0.129690\n2.794967\n0\n1\n-1.190382\n0.891571\n3.726070\n0.673870\n-0.252565\n-0.729156\n2.646563\n4.782729\n0.318952\n-0.781567\n...\n1.101721\n3.723400\n-0.466867\n-0.056224\n3.344701\n0.194332\n0.463992\n0.292268\n4.665876\n0\n2\n-0.996384\n-0.099537\n3.421476\n0.162771\n-1.143458\n-1.026791\n2.114702\n2.517553\n-0.154620\n-0.465423\n...\n1.729386\n2.820340\n-1.041428\n-0.331871\n2.909172\n2.138613\n-0.046252\n-0.732631\n4.716266\n0\n3\n-0.289976\n-1.680019\n3.126478\n-0.704451\n-1.149112\n1.174962\n2.860341\n3.753661\n-0.326119\n2.128411\n...\n2.328688\n3.397321\n-0.932060\n-1.442370\n2.058517\n3.881936\n2.090635\n-0.045832\n4.197315\n1\n4\n-0.294866\n1.044919\n2.924139\n0.814049\n-1.455054\n-0.270432\n3.380195\n2.339669\n1.029101\n-1.171018\n...\n1.283565\n0.677006\n-2.147444\n-0.494150\n3.222041\n6.219348\n-1.914110\n0.317786\n4.143443\n1\n5 rows × 21 columns\nView the results in the UI#\nThe output is saved as a parquet file under the project artifact path. In the UI you can go to the batch-inference-infer job –> artifact tab to view the details.\nScheduling a batch run#\nTo schedule a run, you can set the schedule parameter of the run method. The scheduling is done by using a cron format.\nYou can also schedule runs from the dashboard. On the Projects > Jobs and Workflows page, you can create a new job using the New Job wizard. At the end of the wizard you can set the job scheduling. In the following example, the job is set to run every 30 minutes.\nbatch_run = project.run_function(\nbatch_inference,\ninputs={\"dataset\": prediction_set_path},\nparams={\"model\": model_artifact.uri},\nschedule='*/30 * * * *'\n)\nDrift analysis#\nBy default, if a model has a sample set statistics, batch_inference performs drift analysis and will produce a data drift table artifact, as well as numerical drift metrics.\nTo provide sample set statistics for the model you can either:\nTrain the model using MLRun. This allows you to create the sample set during training.\nLog an external model using project.log_model method and provide the training set in the training_set parameter.\nProvide the set explicitly when calling the batch_inference function via the sample_set input.\nIn the example below, we will provide the training set as the sample set\ntraining_set_path = mlrun.get_sample_path('data/batch-predict/training_set.parquet')\nbatch_run = project.run_function(\nbatch_inference,\ninputs={\n\"dataset\": prediction_set_path,\n\"sample_set\": training_set_path\n},\nparams={\"model\": model_artifact.uri,\n\"label_columns\": \"label\",\n\"perform_drift_analysis\" : True}\n)\nIn this case, instead of just prediction, you will get drift analysis. The drift table plot that compares the drift between the training data and prediction data per feature:\nbatch_run.artifact(\"drift_table_plot\").show()\nYou also get a numerical drift metric and boolean flag denoting whether or not data drift is detected:\nprint(batch_run.status.results)\n{'drift_status': False, 'drift_metric': 0.29934242566253266}\n# Data/concept drift per feature (use batch_run.artifact(\"features_drift_results\").get() to obtain the raw data)\nbatch_run.artifact(\"features_drift_results\").show()\n{'feature_0': 0.028086840976606773,\n'feature_1': 0.04485072701663093,\n'feature_2': 0.7391279921664593,\n'feature_3': 0.043769819014849734,\n'feature_4': 0.042755641152500176,\n'feature_5': 0.05184219833790496,\n'feature_6': 0.7262042202197605,\n'feature_7': 0.7297906294873706,\n'feature_8': 0.039060131873550404,\n'feature_9': 0.04468363504674985,\n'feature_10': 0.042567035578799796,\n'feature_11': 0.7221431701127441,\n'feature_12': 0.7034787615778625,\n'feature_13': 0.04239724655474124,\n'feature_14': 0.046364723781764774,\n'feature_15': 0.6329075683793959,\n'feature_16': 0.7181622588902428,\n'feature_17': 0.03587785749574268,\n'feature_18': 0.04443732609382538,\n'feature_19': 0.7902698698155215,\n'label': 0.017413285340161608}\nbatch_inference Parameters#\nModel Parameters\nmodel: str — The model store path.\nInference parameters\nParameters to specify the dataset for inference.\ndataset: DatasetType — The dataset to infer through the model.\nCan be passed in inputs as either a Dataset artifact / Feature vector URI or\nin parameters as a list, dictionary or numpy array.\ndrop_columns: Union[str, int, List[str], List[int]] — A string / integer or a list of strings / integers that\nrepresent the column names / indices to drop. When the dataset is a list or a numpy array this parameter must\nbe represented by integers.\nlabel_columns: Union[str, List[str]] — The target label(s) of the column(s) in the dataset for Regression\nor classification tasks. The label column can be accessed from the model object, or the feature vector provided\nif available.\npredict_kwargs: Dict[str, Any] — Additional parameters to pass to the prediction of the model.\nDrift parameters\nParameters that affect the drift calculation.\nperform_drift_analysis: bool = None — Whether to perform drift analysis between the sample set of the\nmodel object to the dataset given. By default, None, which means it will perform drift analysis if the model has\na sample set statistics. Perform drift analysis will produce a data drift table artifact.\nsample_set: DatasetType — A sample dataset to give to compare the inputs in the drift analysis. The\ndefault chosen sample set will always be the one who is set in the model artifact itself.\ndrift_threshold: float = 0.7 — The threshold of which to mark drifts. Default is 0.7.\npossible_drift_threshold: float = 0.5 — The threshold of which to mark possible drifts. Default is 0.5.\ninf_capping: float = 10.0 — The value to set for when it reached infinity. Default is 10.0.\nLogging parameters\nParameters to control the automatic logging feature of MLRun. You can adjust the logging outputs as relevant and if\nnot passed, a default list of artifacts and metrics is produced and calculated.\nlog_result_set: bool = True — Whether to log the result set - a DataFrame of the given inputs concatenated\nwith the predictions. Default is True.\nresult_set_name: str = \"prediction\" — The db key to set name of the prediction result and the filename\nDefault is ‘prediction’.\nartifacts_tag: str — Tag to use for all the artifacts resulted from the function."}
{"text": "Deploy models and applications#\nOne of the advantages of using MLRun, is simplifying the deployment process. Deployment is more than just model\ndeployment. Models usually run as part of a greater system which requires data processing before and after\nexecuting the model as well as being part of a business application.\nGenerally, there are two main modes of deployment:\nReal-time deployment: this is the process of having a data and models pipeline respond for real-time events.\nThe challenge here is usually ensuring that the data processing is performed in the same way that the batch\ntraining was done and sending the response in low latency. MLRun includes a specialized serving graph that\neases that creation of a data transformation pipeline as part of the model serving. Feature store support\nis another way of ensuring feature calculation remain consistent between the training process and the serving\nprocess. For an end-to-end demo of model serving, refer to the\nServing pre-trained ML/DL models tutorial.\nBatch inference: this includes a process that runs on a large dataset. The data is usually read from\nan offline source, such as files or databases and the result is also written to offline targets. It is common\nto set-up a schedule when running batch inference. For an end-to-end demo of batch inference, refer to the\nbatch inference and drift detection tutorial.\nIn this section\nReal-time deployment\nServing with the feature store\nBatch inference\nCanary and rolling upgrades"}
{"text": "Develop and train models#\nIn this section\nModel training and tracking\nTraining with the feature store"}
{"text": "Model training and tracking#\nIn this section\nCreate a basic training job\nWorking with data and model artifacts\nAutomated experiment tracking\nUsing the built-in training function\nHyperparameter tuning optimization"}
{"text": "Feature store example (stocks)#\nThis notebook demonstrates the following:\nGenerate features and feature-sets\nBuild complex transformations and ingest to offline and real-time data stores\nFetch feature vectors for training\nSave feature vectors for re-use in real-time pipelines\nAccess features and their statistics in real-time\nNote\nBy default, this demo works with the online feature store, which is currently not part of the Open Source MLRun default deployment.\nIn this section\nGet started\nCreate sample data for demo\nDefine, infer and ingest feature sets\nGet an offline feature vector for training\nInitialize an online feature service and use it for real-time inference\nGet started#\nInstall the latest MLRun package and restart the notebook.\nSetting up the environment and project:\nimport mlrun\nmlrun.set_environment(project=\"stocks\")\n> 2021-05-23 09:04:04,507 [warning] Failed resolving version info. Ignoring and using defaults\n> 2021-05-23 09:04:07,033 [warning] Unable to parse server or client version. Assuming compatible: {'server_version': '0.6.4-rc3', 'client_version': 'unstable'}\n('stocks', 'v3io:///projects/{{run.project}}/artifacts')\nCreate sample data for demo#\nimport pandas as pd\nquotes = pd.DataFrame(\n{\n\"time\": [\npd.Timestamp(\"2016-05-25 13:30:00.023\"),\npd.Timestamp(\"2016-05-25 13:30:00.023\"),\npd.Timestamp(\"2016-05-25 13:30:00.030\"),\npd.Timestamp(\"2016-05-25 13:30:00.041\"),\npd.Timestamp(\"2016-05-25 13:30:00.048\"),\npd.Timestamp(\"2016-05-25 13:30:00.049\"),\npd.Timestamp(\"2016-05-25 13:30:00.072\"),\npd.Timestamp(\"2016-05-25 13:30:00.075\")\n],\n\"ticker\": [\n\"GOOG\",\n\"MSFT\",\n\"MSFT\",\n\"MSFT\",\n\"GOOG\",\n\"AAPL\",\n\"GOOG\",\n\"MSFT\"\n],\n\"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n\"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]\n}\n)\ntrades = pd.DataFrame(\n{\n\"time\": [\npd.Timestamp(\"2016-05-25 13:30:00.023\"),\npd.Timestamp(\"2016-05-25 13:30:00.038\"),\npd.Timestamp(\"2016-05-25 13:30:00.048\"),\npd.Timestamp(\"2016-05-25 13:30:00.048\"),\npd.Timestamp(\"2016-05-25 13:30:00.048\")\n],\n\"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n\"price\": [51.95, 51.95, 720.77, 720.92, 98.0],\n\"quantity\": [75, 155, 100, 100, 100]\n}\n)\nstocks = pd.DataFrame(\n{\n\"ticker\": [\"MSFT\", \"GOOG\", \"AAPL\"],\n\"name\": [\"Microsoft Corporation\", \"Alphabet Inc\", \"Apple Inc\"],\n\"exchange\": [\"NASDAQ\", \"NASDAQ\", \"NASDAQ\"]\n}\n)\nimport datetime\ndef move_date(df, col):\nmax_date = df[col].max()\nnow_date = datetime.datetime.now()\ndelta = now_date - max_date\ndf[col] = df[col] + delta\nreturn df\nquotes = move_date(quotes, \"time\")\ntrades = move_date(trades, \"time\")\nView the demo data#\nquotes\ntime\nticker\nbid\nask\n0\n2021-05-23 09:04:07.013574\nGOOG\n720.50\n720.93\n1\n2021-05-23 09:04:07.013574\nMSFT\n51.95\n51.96\n2\n2021-05-23 09:04:07.020574\nMSFT\n51.97\n51.98\n3\n2021-05-23 09:04:07.031574\nMSFT\n51.99\n52.00\n4\n2021-05-23 09:04:07.038574\nGOOG\n720.50\n720.93\n5\n2021-05-23 09:04:07.039574\nAAPL\n97.99\n98.01\n6\n2021-05-23 09:04:07.062574\nGOOG\n720.50\n720.88\n7\n2021-05-23 09:04:07.065574\nMSFT\n52.01\n52.03\ntrades\ntime\nticker\nprice\nquantity\n0\n2021-05-23 09:04:07.041766\nMSFT\n51.95\n75\n1\n2021-05-23 09:04:07.056766\nMSFT\n51.95\n155\n2\n2021-05-23 09:04:07.066766\nGOOG\n720.77\n100\n3\n2021-05-23 09:04:07.066766\nGOOG\n720.92\n100\n4\n2021-05-23 09:04:07.066766\nAAPL\n98.00\n100\nstocks\nticker\nname\nexchange\n0\nMSFT\nMicrosoft Corporation\nNASDAQ\n1\nGOOG\nAlphabet Inc\nNASDAQ\n2\nAAPL\nApple Inc\nNASDAQ\nDefine, infer and ingest feature sets#\nimport mlrun.feature_store as fstore\nfrom mlrun.feature_store.steps import *\nfrom mlrun.features import MinMaxValidator\nBuild and ingest simple feature set (stocks)#\n# add feature set without time column (stock ticker metadata)\nstocks_set = fstore.FeatureSet(\"stocks\", entities=[fstore.Entity(\"ticker\")])\nfstore.ingest(stocks_set, stocks, infer_options=fstore.InferOptions.default())\nname\nexchange\nticker\nMSFT\nMicrosoft Corporation\nNASDAQ\nGOOG\nAlphabet Inc\nNASDAQ\nAAPL\nApple Inc\nNASDAQ\nBuild an advanced feature set - with feature engineering pipeline#\nDefine a feature set with custom data processing and time aggregation functions:\n# create a new feature set\nquotes_set = fstore.FeatureSet(\"stock-quotes\", entities=[fstore.Entity(\"ticker\")])\nDefine a custom pipeline step (python class)\nclass MyMap(MapClass):\ndef __init__(self, multiplier=1, **kwargs):\nsuper().__init__(**kwargs)\nself._multiplier = multiplier\ndef do(self, event):\nevent[\"multi\"] = event[\"bid\"] * self._multiplier\nreturn event\nBuild and show the transformatiom pipeline\nUse storey stream processing classes along with library and custom classes:\nquotes_set.graph.to(\"MyMap\", multiplier=3)\\\n.to(\"storey.Extend\", _fn=\"({'extra': event['bid'] * 77})\")\\\n.to(\"storey.Filter\", \"filter\", _fn=\"(event['bid'] > 51.92)\")\\\n.to(FeaturesetValidator())\nquotes_set.add_aggregation(\"ask\", [\"sum\", \"max\"], \"1h\", \"10m\", name=\"asks1\")\nquotes_set.add_aggregation(\"ask\", [\"sum\", \"max\"], \"5h\", \"10m\", name=\"asks5\")\nquotes_set.add_aggregation(\"bid\", [\"min\", \"max\"], \"1h\", \"10m\", name=\"bids\")\n# add feature validation policy\nquotes_set[\"bid\"] = fstore.Feature(validator=MinMaxValidator(min=52, severity=\"info\"))\n# add default target definitions and plot\nquotes_set.set_targets()\nquotes_set.plot(rankdir=\"LR\", with_targets=True)\nTest and show the pipeline results locally (allow to quickly develop and debug)\nfstore.preview(\nquotes_set,\nquotes,\nentity_columns=[\"ticker\"],\ntimestamp_key=\"time\",\noptions=fstore.InferOptions.default(),\n)\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.013574 args={'min': 52, 'value': 51.95}\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.020574 args={'min': 52, 'value': 51.97}\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.031574 args={'min': 52, 'value': 51.99}\nasks1_sum_1h\nasks1_max_1h\nasks5_sum_5h\nasks5_max_5h\nbids_min_1h\nbids_max_1h\ntime\nbid\nask\nmulti\nextra\nticker\nGOOG\n720.93\n720.93\n720.93\n720.93\n720.50\n720.50\n2021-05-23 09:04:07.013574\n720.50\n720.93\n2161.50\n55478.50\nMSFT\n51.96\n51.96\n51.96\n51.96\n51.95\n51.95\n2021-05-23 09:04:07.013574\n51.95\n51.96\n155.85\n4000.15\nMSFT\n103.94\n51.98\n103.94\n51.98\n51.95\n51.97\n2021-05-23 09:04:07.020574\n51.97\n51.98\n155.91\n4001.69\nMSFT\n155.94\n52.00\n155.94\n52.00\n51.95\n51.99\n2021-05-23 09:04:07.031574\n51.99\n52.00\n155.97\n4003.23\nGOOG\n1441.86\n720.93\n1441.86\n720.93\n720.50\n720.50\n2021-05-23 09:04:07.038574\n720.50\n720.93\n2161.50\n55478.50\nAAPL\n98.01\n98.01\n98.01\n98.01\n97.99\n97.99\n2021-05-23 09:04:07.039574\n97.99\n98.01\n293.97\n7545.23\nGOOG\n2162.74\n720.93\n2162.74\n720.93\n720.50\n720.50\n2021-05-23 09:04:07.062574\n720.50\n720.88\n2161.50\n55478.50\nMSFT\n207.97\n52.03\n207.97\n52.03\n51.95\n52.01\n2021-05-23 09:04:07.065574\n52.01\n52.03\n156.03\n4004.77\n# print the feature set object\nprint(quotes_set.to_yaml())\nkind: FeatureSet\nmetadata:\nname: stock-quotes\nspec:\nentities:\n- name: ticker\nvalue_type: str\nfeatures:\n- name: asks1_sum_1h\nvalue_type: float\naggregate: true\n- name: asks1_max_1h\nvalue_type: float\naggregate: true\n- name: asks5_sum_5h\nvalue_type: float\naggregate: true\n- name: asks5_max_5h\nvalue_type: float\naggregate: true\n- name: bids_min_1h\nvalue_type: float\naggregate: true\n- name: bids_max_1h\nvalue_type: float\naggregate: true\n- name: bid\nvalue_type: float\nvalidator:\nkind: minmax\nseverity: info\nmin: 52\n- name: ask\nvalue_type: float\n- name: multi\nvalue_type: float\n- name: extra\nvalue_type: float\npartition_keys: []\ntimestamp_key: time\nsource:\npath: None\ntargets:\n- name: parquet\nkind: parquet\n- name: nosql\nkind: nosql\ngraph:\nstates:\nMyMap:\nkind: task\nclass_name: MyMap\nclass_args:\nmultiplier: 3\nstorey.Extend:\nkind: task\nclass_name: storey.Extend\nclass_args:\n_fn: '({''extra'': event[''bid''] * 77})'\nafter:\n- MyMap\nfilter:\nkind: task\nclass_name: storey.Filter\nclass_args:\n_fn: (event['bid'] > 51.92)\nafter:\n- storey.Extend\nFeaturesetValidator:\nkind: task\nclass_name: mlrun.feature_store.steps.FeaturesetValidator\nclass_args:\nfeatureset: .\ncolumns: null\nafter:\n- filter\nAggregates:\nkind: task\nclass_name: storey.AggregateByKey\nclass_args:\naggregates:\n- name: asks1\ncolumn: ask\noperations:\n- sum\n- max\nwindows:\n- 1h\nperiod: 10m\n- name: asks5\ncolumn: ask\noperations:\n- sum\n- max\nwindows:\n- 5h\nperiod: 10m\n- name: bids\ncolumn: bid\noperations:\n- min\n- max\nwindows:\n- 1h\nperiod: 10m\ntable: .\nafter:\n- FeaturesetValidator\noutput_path: v3io:///projects/{{run.project}}/artifacts\nstatus:\nstate: created\nstats:\nticker:\ncount: 8\nunique: 3\ntop: MSFT\nfreq: 4\nasks1_sum_1h:\ncount: 8.0\nmean: 617.9187499999999\nmin: 51.96\nmax: 2162.74\nstd: 784.8779804245735\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 1\n- - 51.96\n- 157.499\n- 263.03799999999995\n- 368.57699999999994\n- 474.11599999999993\n- 579.655\n- 685.194\n- 790.733\n- 896.2719999999999\n- 1001.8109999999999\n- 1107.35\n- 1212.889\n- 1318.4279999999999\n- 1423.9669999999999\n- 1529.5059999999999\n- 1635.0449999999998\n- 1740.5839999999998\n- 1846.1229999999998\n- 1951.6619999999998\n- 2057.2009999999996\n- 2162.74\nasks1_max_1h:\ncount: 8.0\nmean: 308.59625\nmin: 51.96\nmax: 720.93\nstd: 341.7989955655851\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 51.96\n- 85.4085\n- 118.857\n- 152.3055\n- 185.754\n- 219.2025\n- 252.65099999999998\n- 286.0995\n- 319.54799999999994\n- 352.9964999999999\n- 386.44499999999994\n- 419.89349999999996\n- 453.3419999999999\n- 486.7904999999999\n- 520.2389999999999\n- 553.6875\n- 587.136\n- 620.5844999999999\n- 654.0329999999999\n- 687.4815\n- 720.93\nasks5_sum_5h:\ncount: 8.0\nmean: 617.9187499999999\nmin: 51.96\nmax: 2162.74\nstd: 784.8779804245735\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 1\n- - 51.96\n- 157.499\n- 263.03799999999995\n- 368.57699999999994\n- 474.11599999999993\n- 579.655\n- 685.194\n- 790.733\n- 896.2719999999999\n- 1001.8109999999999\n- 1107.35\n- 1212.889\n- 1318.4279999999999\n- 1423.9669999999999\n- 1529.5059999999999\n- 1635.0449999999998\n- 1740.5839999999998\n- 1846.1229999999998\n- 1951.6619999999998\n- 2057.2009999999996\n- 2162.74\nasks5_max_5h:\ncount: 8.0\nmean: 308.59625\nmin: 51.96\nmax: 720.93\nstd: 341.7989955655851\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 51.96\n- 85.4085\n- 118.857\n- 152.3055\n- 185.754\n- 219.2025\n- 252.65099999999998\n- 286.0995\n- 319.54799999999994\n- 352.9964999999999\n- 386.44499999999994\n- 419.89349999999996\n- 453.3419999999999\n- 486.7904999999999\n- 520.2389999999999\n- 553.6875\n- 587.136\n- 620.5844999999999\n- 654.0329999999999\n- 687.4815\n- 720.93\nbids_min_1h:\ncount: 8.0\nmean: 308.41125\nmin: 51.95\nmax: 720.5\nstd: 341.59667259325835\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 51.95\n- 85.3775\n- 118.80499999999999\n- 152.2325\n- 185.65999999999997\n- 219.08749999999998\n- 252.515\n- 285.94249999999994\n- 319.36999999999995\n- 352.79749999999996\n- 386.22499999999997\n- 419.6524999999999\n- 453.0799999999999\n- 486.50749999999994\n- 519.935\n- 553.3625\n- 586.79\n- 620.2175\n- 653.645\n- 687.0725\n- 720.5\nbids_max_1h:\ncount: 8.0\nmean: 308.42625\nmin: 51.95\nmax: 720.5\nstd: 341.58380276661245\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 51.95\n- 85.3775\n- 118.80499999999999\n- 152.2325\n- 185.65999999999997\n- 219.08749999999998\n- 252.515\n- 285.94249999999994\n- 319.36999999999995\n- 352.79749999999996\n- 386.22499999999997\n- 419.6524999999999\n- 453.0799999999999\n- 486.50749999999994\n- 519.935\n- 553.3625\n- 586.79\n- 620.2175\n- 653.645\n- 687.0725\n- 720.5\ntime:\ncount: 8\nmean: '2021-05-23 09:04:07.035699200'\nmin: '2021-05-23 09:04:07.013574'\nmax: '2021-05-23 09:04:07.065574'\nbid:\ncount: 8.0\nmean: 308.42625\nmin: 51.95\nmax: 720.5\nstd: 341.58380276661245\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 51.95\n- 85.3775\n- 118.80499999999999\n- 152.2325\n- 185.65999999999997\n- 219.08749999999998\n- 252.515\n- 285.94249999999994\n- 319.36999999999995\n- 352.79749999999996\n- 386.22499999999997\n- 419.6524999999999\n- 453.0799999999999\n- 486.50749999999994\n- 519.935\n- 553.3625\n- 586.79\n- 620.2175\n- 653.645\n- 687.0725\n- 720.5\nask:\ncount: 8.0\nmean: 308.59\nmin: 51.96\nmax: 720.93\nstd: 341.79037903369954\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 51.96\n- 85.4085\n- 118.857\n- 152.3055\n- 185.754\n- 219.2025\n- 252.65099999999998\n- 286.0995\n- 319.54799999999994\n- 352.9964999999999\n- 386.44499999999994\n- 419.89349999999996\n- 453.3419999999999\n- 486.7904999999999\n- 520.2389999999999\n- 553.6875\n- 587.136\n- 620.5844999999999\n- 654.0329999999999\n- 687.4815\n- 720.93\nmulti:\ncount: 8.0\nmean: 925.27875\nmin: 155.85000000000002\nmax: 2161.5\nstd: 1024.7514082998375\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 155.85000000000002\n- 256.13250000000005\n- 356.415\n- 456.6975\n- 556.98\n- 657.2625\n- 757.545\n- 857.8275\n- 958.11\n- 1058.3925\n- 1158.6750000000002\n- 1258.9575\n- 1359.2399999999998\n- 1459.5225\n- 1559.8049999999998\n- 1660.0875\n- 1760.37\n- 1860.6525000000001\n- 1960.935\n- 2061.2175\n- 2161.5\nextra:\ncount: 8.0\nmean: 23748.82125\nmin: 4000.15\nmax: 55478.5\nstd: 26301.95281302916\nhist:\n- - 4\n- 1\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 3\n- - 4000.15\n- 6574.0675\n- 9147.985\n- 11721.9025\n- 14295.82\n- 16869.7375\n- 19443.655000000002\n- 22017.572500000002\n- 24591.49\n- 27165.4075\n- 29739.325\n- 32313.2425\n- 34887.16\n- 37461.0775\n- 40034.995\n- 42608.9125\n- 45182.83\n- 47756.747500000005\n- 50330.665\n- 52904.582500000004\n- 55478.5\npreview:\n- - asks1_sum_1h\n- asks1_max_1h\n- asks5_sum_5h\n- asks5_max_5h\n- bids_min_1h\n- bids_max_1h\n- time\n- bid\n- ask\n- multi\n- extra\n- - 720.93\n- 720.93\n- 720.93\n- 720.93\n- 720.5\n- 720.5\n- 2021-05-23T09:04:07.013574\n- 720.5\n- 720.93\n- 2161.5\n- 55478.5\n- - 51.96\n- 51.96\n- 51.96\n- 51.96\n- 51.95\n- 51.95\n- 2021-05-23T09:04:07.013574\n- 51.95\n- 51.96\n- 155.85000000000002\n- 4000.15\n- - 103.94\n- 51.98\n- 103.94\n- 51.98\n- 51.95\n- 51.97\n- 2021-05-23T09:04:07.020574\n- 51.97\n- 51.98\n- 155.91\n- 4001.69\n- - 155.94\n- 52.0\n- 155.94\n- 52.0\n- 51.95\n- 51.99\n- 2021-05-23T09:04:07.031574\n- 51.99\n- 52.0\n- 155.97\n- 4003.23\n- - 1441.86\n- 720.93\n- 1441.86\n- 720.93\n- 720.5\n- 720.5\n- 2021-05-23T09:04:07.038574\n- 720.5\n- 720.93\n- 2161.5\n- 55478.5\n- - 98.01\n- 98.01\n- 98.01\n- 98.01\n- 97.99\n- 97.99\n- 2021-05-23T09:04:07.039574\n- 97.99\n- 98.01\n- 293.96999999999997\n- 7545.23\n- - 2162.74\n- 720.93\n- 2162.74\n- 720.93\n- 720.5\n- 720.5\n- 2021-05-23T09:04:07.062574\n- 720.5\n- 720.88\n- 2161.5\n- 55478.5\n- - 207.97\n- 52.03\n- 207.97\n- 52.03\n- 51.95\n- 52.01\n- 2021-05-23T09:04:07.065574\n- 52.01\n- 52.03\n- 156.03\n- 4004.77\nIngest data into offline and online stores#\nThis writes to both targets (Parquet and NoSQL).\n# save ingest data and print the FeatureSet spec\ndf = fstore.ingest(quotes_set, quotes)\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.013574 args={'min': 52, 'value': 51.95}\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.020574 args={'min': 52, 'value': 51.97}\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.031574 args={'min': 52, 'value': 51.99}\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.013574 args={'min': 52, 'value': 51.95}\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.020574 args={'min': 52, 'value': 51.97}\ninfo! bid value is smaller than min, key=['MSFT'] time=2021-05-23 09:04:07.031574 args={'min': 52, 'value': 51.99}\nGet an offline feature vector for training#\nExample of combining features from 3 sources with time travel join of 3 tables with time travel.\nSpecify a set of features and request the feature vector offline result as a dataframe:\nfeatures = [\n\"stock-quotes.multi\",\n\"stock-quotes.asks5_sum_5h as total_ask\",\n\"stock-quotes.bids_min_1h\",\n\"stock-quotes.bids_max_1h\",\n\"stocks.*\",\n]\nvector = fstore.FeatureVector(\"stocks-vec\", features, description=\"stocks demo feature vector\")\nvector.save()\nresp = fstore.get_offline_features(vector, entity_rows=trades, entity_timestamp_column=\"time\")\nresp.to_dataframe()\nprice\nquantity\nmulti\ntotal_ask\nbids_min_1h\nbids_max_1h\nname\nexchange\n0\n51.95\n75\n155.97\n155.94\n51.95\n51.99\nMicrosoft Corporation\nNASDAQ\n1\n51.95\n155\n155.97\n155.94\n51.95\n51.99\nMicrosoft Corporation\nNASDAQ\n2\n720.77\n100\n2161.50\n2162.74\n720.50\n720.50\nAlphabet Inc\nNASDAQ\n3\n720.92\n100\n2161.50\n2162.74\n720.50\n720.50\nAlphabet Inc\nNASDAQ\n4\n98.00\n100\n293.97\n98.01\n97.99\n97.99\nApple Inc\nNASDAQ\nInitialize an online feature service and use it for real-time inference#\nservice = fstore.get_online_feature_service(\"stocks-vec\")\nRequest feature vector statistics, can be used for imputing or validation\nservice.vector.get_stats_table()\ncount\nmean\nmin\nmax\nstd\nhist\nunique\ntop\nfreq\nmulti\n8.0\n925.27875\n155.85\n2161.50\n1024.751408\n[[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\nNaN\nNaN\nNaN\ntotal_ask\n8.0\n617.91875\n51.96\n2162.74\n784.877980\n[[4, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,...\nNaN\nNaN\nNaN\nbids_min_1h\n8.0\n308.41125\n51.95\n720.50\n341.596673\n[[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\nNaN\nNaN\nNaN\nbids_max_1h\n8.0\n308.42625\n51.95\n720.50\n341.583803\n[[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\nNaN\nNaN\nNaN\nname\n3.0\nNaN\nNaN\nNaN\nNaN\nNaN\n3.0\nAlphabet Inc\n1.0\nexchange\n3.0\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\nNASDAQ\n3.0\nReal-time feature vector request\nservice.get([{\"ticker\": \"GOOG\"}, {\"ticker\": \"MSFT\"}])\n[{'asks5_sum_5h': 2162.74,\n'bids_min_1h': 720.5,\n'bids_max_1h': 720.5,\n'multi': 2161.5,\n'name': 'Alphabet Inc',\n'exchange': 'NASDAQ',\n'total_ask': None},\n{'asks5_sum_5h': 207.97,\n'bids_min_1h': 51.95,\n'bids_max_1h': 52.01,\n'multi': 156.03,\n'name': 'Microsoft Corporation',\n'exchange': 'NASDAQ',\n'total_ask': None}]\nservice.get([{\"ticker\": \"AAPL\"}])\n[{'asks5_sum_5h': 98.01,\n'bids_min_1h': 97.99,\n'bids_max_1h': 97.99,\n'multi': 293.97,\n'name': 'Apple Inc',\n'exchange': 'NASDAQ',\n'total_ask': None}]\nservice.close()"}
{"text": "Part 1: Data Ingestion#\nThis demo showcases financial fraud prevention. It uses the MLRun feature store to define complex features that help identify fraud.\nFraud prevention is a special challenge since it requires processing raw transaction and events in real-time and being able to quickly respond and block transactions before they occur.\nTo address this, you’ll create a development pipeline and a production pipeline. Both pipelines share the same feature engineering and model code, but serve data very differently. Furthermore, MLRun automates the data and model monitoring process, drift identification, and trigger retraining in a CI/CD pipeline. This process is described in the diagram below:\nThe raw data is described as follows:\nTRANSACTIONS\n║\nUSER EVENTS\nage\nage group value 0-6. Some values are marked as U for unknown\n║\nsource\nThe party/entity related to the event\ngender\nA character to define the age\n║\nevent\nevent, such as login or password change\nzipcodeOri\nZIP code of the person originating the transaction\n║\ntimestamp\nThe date and time of the event\nzipMerchant\nZIP code of the merchant receiving the transaction\n║\ncategory\ncategory of the transaction (e.g., transportation, food, etc.)\n║\namount\nthe total amount of the transaction\n║\nfraud\nwhether the transaction is fraudulent\n║\ntimestamp\nthe date and time in which the transaction took place\n║\nsource\nthe ID of the party/entity performing the transaction\n║\ntarget\nthe ID of the party/entity receiving the transaction\n║\ndevice\nthe device ID used to perform the transaction\n║\nThis notebook introduces how to Ingest different data sources to the Feature Store.\nThe following FeatureSets are created:\nTransactions: Monetary transactions between a source and a target.\nEvents: Account events such as account login or a password change.\nLabel: Fraud label for the data.\nBy the end of this tutorial you’ll know how to:\nCreate an ingestion pipeline for each data source.\nDefine preprocessing, aggregation, and validation of the pipeline.\nRun the pipeline locally within the notebook.\nLaunch a real-time function to ingest live data.\nSchedule a cron to run the task when needed.\nproject_name = 'fraud-demo'\nimport mlrun\n# Initialize the MLRun project object\nproject = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)\n> 2022-03-16 05:45:07,703 [info] loaded project fraud-demo from MLRun DB\nStep 1 - Fetch, process and ingest the datasets#\n1.1 - Transactions#\nTransactions#\n# Helper functions to adjust the timestamps of our data\n# while keeping the order of the selected events and\n# the relative distance from one event to the other\ndef date_adjustment(sample, data_max, new_max, old_data_period, new_data_period):\n'''\nAdjust a specific sample's date according to the original and new time periods\n'''\nsample_dates_scale = ((data_max - sample) / old_data_period)\nsample_delta = new_data_period * sample_dates_scale\nnew_sample_ts = new_max - sample_delta\nreturn new_sample_ts\ndef adjust_data_timespan(dataframe, timestamp_col='timestamp', new_period='2d', new_max_date_str='now'):\n'''\nAdjust the dataframe timestamps to the new time period\n'''\n# Calculate old time period\ndata_min = dataframe.timestamp.min()\ndata_max = dataframe.timestamp.max()\nold_data_period = data_max-data_min\n# Set new time period\nnew_time_period = pd.Timedelta(new_period)\nnew_max = pd.Timestamp(new_max_date_str)\nnew_min = new_max-new_time_period\nnew_data_period = new_max-new_min\n# Apply the timestamp change\ndf = dataframe.copy()\ndf[timestamp_col] = df[timestamp_col].apply(lambda x: date_adjustment(x, data_max, new_max, old_data_period, new_data_period))\nreturn df\nimport pandas as pd\n# Fetch the transactions dataset from the server\ntransactions_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/data.csv', parse_dates=['timestamp'], nrows=500)\n# Adjust the samples timestamp for the past 2 days\ntransactions_data = adjust_data_timespan(transactions_data, new_period='2d')\n# Preview\ntransactions_data.head(3)\nstep\nage\ngender\nzipcodeOri\nzipMerchant\ncategory\namount\nfraud\ntimestamp\nsource\ntarget\ndevice\n0\n0\n4\nM\n28007\n28007\nes_transportation\n4.55\n0\n2022-03-15 16:13:54.851486383\nC1093826151\nM348934600\nf802e61d76564b7a89a83adcdfa573da\n1\n0\n2\nM\n28007\n28007\nes_transportation\n39.68\n0\n2022-03-14 09:21:09.710448366\nC352968107\nM348934600\n38ef7fc3eb7442c8ae64579a483f1d2b\n2\n0\n4\nF\n28007\n28007\nes_transportation\n26.89\n0\n2022-03-15 22:41:20.666966912\nC2054744914\nM1823072687\n7a851d0758894078b5846851ae32d5e3\nTransactions - create a feature set and preprocessing pipeline#\nCreate the feature set (data pipeline) definition for the credit transaction processing that describes the offline/online data transformations and aggregations.\nThe feature store automatically adds an offline parquet target and an online NoSQL target by using set_targets().\nThe data pipeline consists of:\nExtracting the data components (hour, day of week)\nMapping the age values\nOne hot encoding for the transaction category and the gender\nAggregating the amount (avg, sum, count, max over 2/12/24 hour time windows)\nAggregating the transactions per category (over 14 days time windows)\nWriting the results to offline (Parquet) and online (NoSQL) targets\n# Import MLRun's Feature Store\nimport mlrun.feature_store as fstore\nfrom mlrun.feature_store.steps import OneHotEncoder, MapValues, DateExtractor\n# Define the transactions FeatureSet\ntransaction_set = fstore.FeatureSet(\"transactions\",\nentities=[fstore.Entity(\"source\")],\ntimestamp_key='timestamp',\ndescription=\"transactions feature set\")\n# Define and add value mapping\nmain_categories = [\"es_transportation\", \"es_health\", \"es_otherservices\",\n\"es_food\", \"es_hotelservices\", \"es_barsandrestaurants\",\n\"es_tech\", \"es_sportsandtoys\", \"es_wellnessandbeauty\",\n\"es_hyper\", \"es_fashion\", \"es_home\", \"es_contents\",\n\"es_travel\", \"es_leisure\"]\n# One Hot Encode the newly defined mappings\none_hot_encoder_mapping = {'category': main_categories,\n'gender': list(transactions_data.gender.unique())}\n# Define the graph steps\ntransaction_set.graph\\\n.to(DateExtractor(parts = ['hour', 'day_of_week'], timestamp_col = 'timestamp'))\\\n.to(MapValues(mapping={'age': {'U': '0'}}, with_original_features=True))\\\n.to(OneHotEncoder(mapping=one_hot_encoder_mapping))\n# Add aggregations for 2, 12, and 24 hour time windows\ntransaction_set.add_aggregation(name='amount',\ncolumn='amount',\noperations=['avg','sum', 'count','max'],\nwindows=['2h', '12h', '24h'],\nperiod='1h')\n# Add the category aggregations over a 14 day window\nfor category in main_categories:\ntransaction_set.add_aggregation(name=category,column=f'category_{category}',\noperations=['count'], windows=['14d'], period='1d')\n# Add default (offline-parquet & online-nosql) targets\ntransaction_set.set_targets()\n# Plot the pipeline so we can see the different steps\ntransaction_set.plot(rankdir=\"LR\", with_targets=True)\nTransactions - ingestion#\n# Ingest the transactions dataset through the defined pipeline\ntransactions_df = fstore.ingest(transaction_set, transactions_data,\ninfer_options=fstore.InferOptions.default())\ntransactions_df.head(3)\npersist count = 0\npersist count = 100\npersist count = 200\npersist count = 300\npersist count = 400\npersist count = 500\npersist count = 600\npersist count = 700\npersist count = 800\npersist count = 900\npersist count = 1000\namount_count_2h\namount_count_12h\namount_count_24h\namount_max_2h\namount_max_12h\namount_max_24h\namount_sum_2h\namount_sum_12h\namount_sum_24h\namount_avg_2h\n...\ncategory_es_contents\ncategory_es_travel\ncategory_es_leisure\namount\nfraud\ntimestamp\ntarget\ndevice\ntimestamp_hour\ntimestamp_day_of_week\nsource\nC1093826151\n1.0\n1.0\n1.0\n4.55\n4.55\n4.55\n4.55\n4.55\n4.55\n4.55\n...\n0\n0\n0\n4.55\n0\n2022-03-15 16:13:54.851486383\nM348934600\nf802e61d76564b7a89a83adcdfa573da\n16\n1\nC352968107\n1.0\n1.0\n1.0\n39.68\n39.68\n39.68\n39.68\n39.68\n39.68\n39.68\n...\n0\n0\n0\n39.68\n0\n2022-03-14 09:21:09.710448366\nM348934600\n38ef7fc3eb7442c8ae64579a483f1d2b\n9\n0\nC2054744914\n1.0\n1.0\n1.0\n26.89\n26.89\n26.89\n26.89\n26.89\n26.89\n26.89\n...\n0\n0\n0\n26.89\n0\n2022-03-15 22:41:20.666966912\nM1823072687\n7a851d0758894078b5846851ae32d5e3\n22\n1\n3 rows × 57 columns\n1.2 - User events#\nUser events - fetching#\n# Fetch the user_events dataset from the server\nuser_events_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/events.csv',\nindex_col=0, quotechar=\"\\'\", parse_dates=['timestamp'], nrows=500)\n# Adjust to the last 2 days to see the latest aggregations in our online feature vectors\nuser_events_data = adjust_data_timespan(user_events_data, new_period='2d')\n# Preview\nuser_events_data.head(3)\nsource\nevent\ntimestamp\n0\nC1974668487\ndetails_change\n2022-03-15 15:03:17.518565985\n1\nC1973547259\nlogin\n2022-03-15 18:05:50.652706656\n2\nC515668508\nlogin\n2022-03-15 14:37:49.845093748\nUser events - create a feature set and preprocessing pipeline#\nDefine the events feature set.\nThis is a fairly straightforward pipeline in which you only “one hot encode” the event categories and save the data to the default targets.\nuser_events_set = fstore.FeatureSet(\"events\",\nentities=[fstore.Entity(\"source\")],\ntimestamp_key='timestamp',\ndescription=\"user events feature set\")\n# Define and add value mapping\nevents_mapping = {'event': list(user_events_data.event.unique())}\n# One Hot Encode\nuser_events_set.graph.to(OneHotEncoder(mapping=events_mapping))\n# Add default (offline-parquet & online-nosql) targets\nuser_events_set.set_targets()\n# Plot the pipeline so we can see the different steps\nuser_events_set.plot(rankdir=\"LR\", with_targets=True)\nUser events - ingestion#\n# Ingestion of the newly created events feature set\nevents_df = fstore.ingest(user_events_set, user_events_data)\nevents_df.head(3)\npersist count = 0\npersist count = 100\npersist count = 200\npersist count = 300\npersist count = 400\npersist count = 500\nevent_details_change\nevent_login\nevent_password_change\ntimestamp\nsource\nC1974668487\n1\n0\n0\n2022-03-15 15:03:17.518565985\nC1973547259\n0\n1\n0\n2022-03-15 18:05:50.652706656\nC515668508\n0\n1\n0\n2022-03-15 14:37:49.845093748\nStep 2 - Create a labels dataset for model training#\nLabel set - create a feature set#\nThis feature set contains the label for the fraud demo, it is ingested directly to the default targets without any changes\ndef create_labels(df):\nlabels = df[['fraud','source','timestamp']].copy()\nlabels = labels.rename(columns={\"fraud\": \"label\"})\nlabels['timestamp'] = labels['timestamp'].astype(\"datetime64[ms]\")\nlabels['label'] = labels['label'].astype(int)\nlabels.set_index('source', inplace=True)\nreturn labels\n# Define the \"labels\" feature set\nlabels_set = fstore.FeatureSet(\"labels\",\nentities=[fstore.Entity(\"source\")],\ntimestamp_key='timestamp',\ndescription=\"training labels\",\nengine=\"pandas\")\nlabels_set.graph.to(name=\"create_labels\", handler=create_labels)\n# specify only Parquet (offline) target since its not used for real-time\nlabels_set.set_targets(['parquet'], with_defaults=False)\nlabels_set.plot(with_targets=True)\nLabel set - ingestion#\n# Ingest the labels feature set\nlabels_df = fstore.ingest(labels_set, transactions_data)\nlabels_df.head(3)\nlabel\ntimestamp\nsource\nC1093826151\n0\n2022-03-15 16:13:54.851\nC352968107\n0\n2022-03-14 09:21:09.710\nC2054744914\n0\n2022-03-15 22:41:20.666\nStep 3 - Deploy a real-time pipeline#\nWhen dealing with real-time aggregation, it’s important to be able to update these aggregations in real-time.\nFor this purpose, you’ll create live serving functions that update the online feature store of the transactions FeatureSet and Events FeatureSet.\nUsing MLRun’s serving runtime, create a nuclio function loaded with the feature set’s computational graph definition\nand an HttpSource to define the HTTP trigger.\nNotice that the implementation below does not require any rewrite of the pipeline logic.\n3.1 - Transactions#\nTransactions - deploy the feature set live endpoint#\n# Create iguazio v3io stream and transactions push API endpoint\ntransaction_stream = f'v3io:///projects/{project.name}/streams/transaction'\ntransaction_pusher = mlrun.datastore.get_stream_pusher(transaction_stream)\n# Define the source stream trigger (use v3io streams)\n# Define the `key` and `time` fields (extracted from the Json message).\nsource = mlrun.datastore.sources.StreamSource(path=transaction_stream , key_field='source', time_field='timestamp')\n# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n# you can use the run_config parameter to pass function/service specific configuration\ntransaction_set_endpoint = fstore.deploy_ingestion_service(featureset=transaction_set, source=source)\n> 2022-03-16 05:45:43,035 [info] Starting remote function deploy\n2022-03-16 05:45:43  (info) Deploying function\n2022-03-16 05:45:43  (info) Building\n2022-03-16 05:45:43  (info) Staging files and preparing base images\n2022-03-16 05:45:43  (warn) Python 3.6 runtime is deprecated and will soon not be supported. Please migrate your code and use Python 3.7 runtime (`python:3.7`) or higher\n2022-03-16 05:45:43  (info) Building processor image\n2022-03-16 05:47:03  (info) Build complete\n2022-03-16 05:47:08  (info) Function deploy complete\n> 2022-03-16 05:47:08,835 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-fraud-demo-admin-transactions-ingest.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['fraud-demo-admin-transactions-ingest-fraud-demo-admin.default-tenant.app.xtvtjecfcssi.iguazio-cd1.com/']}\nTransactions - test the feature set HTTP endpoint#\nBy defining the transactions feature set you can now use MLRun and Storey to deploy it as a live endpoint, ready to ingest new data!\nUsing MLRun’s serving runtime, create a nuclio function loaded with the feature set’s computational graph definition and an HttpSource to define the HTTP trigger.\nimport requests\nimport json\n# Select a sample from the dataset and serialize it to JSON\ntransaction_sample = json.loads(transactions_data.sample(1).to_json(orient='records'))[0]\ntransaction_sample['timestamp'] = str(pd.Timestamp.now())\ntransaction_sample\n{'step': 0,\n'age': '5',\n'gender': 'M',\n'zipcodeOri': 28007,\n'zipMerchant': 28007,\n'category': 'es_transportation',\n'amount': 2.19,\n'fraud': 0,\n'timestamp': '2022-03-16 05:47:08.884971',\n'source': 'C546957379',\n'target': 'M348934600',\n'device': '8ee3b24f2eb143759e938b6148da547c'}\n# Post the sample to the ingestion endpoint\nrequests.post(transaction_set_endpoint, json=transaction_sample).text\n'{\"id\": \"c3f8a087-3932-45c0-8dcb-1fd8efabe681\"}'\n3.2 - User events#\nUser events - deploy the feature set live endpoint#\nDeploy the events feature set’s ingestion service using the feature set and all the previously defined resources.\n# Create iguazio v3io stream and transactions push API endpoint\nevents_stream = f'v3io:///projects/{project.name}/streams/events'\nevents_pusher = mlrun.datastore.get_stream_pusher(events_stream)\n# Define the source stream trigger (use v3io streams)\n# Define the `key` and `time` fields (extracted from the Json message).\nsource = mlrun.datastore.sources.StreamSource(path=events_stream , key_field='source', time_field='timestamp')\n# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n# you can use the run_config parameter to pass function/service specific configuration\nevents_set_endpoint = fstore.deploy_ingestion_service(featureset=user_events_set, source=source)\n> 2022-03-16 05:47:09,035 [info] Starting remote function deploy\n2022-03-16 05:47:09  (info) Deploying function\n2022-03-16 05:47:09  (info) Building\n2022-03-16 05:47:09  (info) Staging files and preparing base images\n2022-03-16 05:47:09  (warn) Python 3.6 runtime is deprecated and will soon not be supported. Please migrate your code and use Python 3.7 runtime (`python:3.7`) or higher\n2022-03-16 05:47:09  (info) Building processor image\nUser events - test the feature set HTTP endpoint#\n# Select a sample from the events dataset and serialize it to JSON\nuser_events_sample = json.loads(user_events_data.sample(1).to_json(orient='records'))[0]\nuser_events_sample['timestamp'] = str(pd.Timestamp.now())\nuser_events_sample\n# Post the sample to the ingestion endpoint\nrequests.post(events_set_endpoint, json=user_events_sample).text\nDone!#\nYou’ve completed Part 1 of the data-ingestion with the feature store.\nProceed to Part 2 to learn how to train an ML model using the feature store data."}
{"text": "Part 2: Training#\nThis part shows how to use MLRun’s Feature Store to easily define a Feature Vector and create the dataset you need to run the training process.\nBy the end of this tutorial you’ll learn how to:\nCombine multiple data sources to a single Feature Vector\nCreate training dataset\nCreate a model using an MLRun Hub function\nproject_name = 'fraud-demo'\nimport mlrun\n# Initialize the MLRun project object\nproject = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)\n> 2021-09-19 17:59:27,165 [info] loaded project fraud-demo from MLRun DB\nStep 1 - Create a feature vector#\nIn this section you create the Feature Vector.\nThe Feature vector has a name so you can reference to it later via the URI or the serving function, and a list of features from the available FeatureSets.  You can add a feature from a feature set by adding <FeatureSet>.<Feature> to the list, or add <FeatureSet>.* to add all the FeatureSet’s available features.\nBy default, the first FeatureSet in the feature list acts as the spine, meaning that all the other features will be joined to it.\nFor example, in this instance the spine is the early_sense sensor data, so for each early_sense event we will create produce a row in the resulted Feature Vector.\n# Define the list of features you will be using\nfeatures = ['transactions.amount_max_2h',\n'transactions.amount_sum_2h',\n'transactions.amount_count_2h',\n'transactions.amount_avg_2h',\n'transactions.amount_max_12h',\n'transactions.amount_sum_12h',\n'transactions.amount_count_12h',\n'transactions.amount_avg_12h',\n'transactions.amount_max_24h',\n'transactions.amount_sum_24h',\n'transactions.amount_count_24h',\n'transactions.amount_avg_24h',\n'transactions.es_transportation_count_14d',\n'transactions.es_health_count_14d',\n'transactions.es_otherservices_count_14d',\n'transactions.es_food_count_14d',\n'transactions.es_hotelservices_count_14d',\n'transactions.es_barsandrestaurants_count_14d',\n'transactions.es_tech_count_14d',\n'transactions.es_sportsandtoys_count_14d',\n'transactions.es_wellnessandbeauty_count_14d',\n'transactions.es_hyper_count_14d',\n'transactions.es_fashion_count_14d',\n'transactions.es_home_count_14d',\n'transactions.es_travel_count_14d',\n'transactions.es_leisure_count_14d',\n'transactions.gender_F',\n'transactions.gender_M',\n'transactions.step',\n'transactions.amount',\n'transactions.timestamp_hour',\n'transactions.timestamp_day_of_week',\n'events.*']\n# Import MLRun's Feature Store\nimport mlrun.feature_store as fstore\n# Define the feature vector name for future reference\nfv_name = 'transactions-fraud'\n# Define the feature vector using our Feature Store (fstore)\ntransactions_fv = fstore.FeatureVector(fv_name,\nfeatures,\nlabel_feature=\"labels.label\",\ndescription='Predicting a fraudulent transaction')\n# Save the feature vector in the Feature Store\ntransactions_fv.save()\nStep 2 - Preview the feature vector data#\nObtain the values of the features in the feature vector, to ensure the data appears as expected.\n# Import the Parquet Target so you can directly save the dataset as a file\nfrom mlrun.datastore.targets import ParquetTarget\n# Get offline feature vector as dataframe and save the dataset to parquet\ntrain_dataset = fstore.get_offline_features(fv_name, target=ParquetTarget())\n> 2021-09-19 17:59:28,415 [info] wrote target: {'name': 'parquet', 'kind': 'parquet', 'path': 'v3io:///projects/fraud-demo-admin/FeatureStore/transactions-fraud/parquet/vectors/transactions-fraud-latest.parquet', 'status': 'ready', 'updated': '2021-09-19T17:59:28.415727+00:00', 'size': 1915182}\n# Preview the dataset\ntrain_dataset.to_dataframe().tail(5)\namount_max_2h\namount_sum_2h\namount_count_2h\namount_avg_2h\namount_max_12h\namount_sum_12h\namount_count_12h\namount_avg_12h\namount_max_24h\namount_sum_24h\n...\ngender_F\ngender_M\nstep\namount\ntimestamp_hour\ntimestamp_day_of_week\nevent_details_change\nevent_login\nevent_password_change\nlabel\n49995\n2.95\n2.95\n1.0\n2.950\n2.95\n2.95\n1.0\n2.950\n2.95\n2.95\n...\n1\n0\n41\n2.95\n17\n6\n0.0\n0.0\n1.0\n0\n49996\n37.40\n37.40\n1.0\n37.400\n37.40\n37.40\n1.0\n37.400\n37.40\n37.40\n...\n0\n1\n40\n37.40\n17\n6\n0.0\n0.0\n1.0\n0\n49997\n7.75\n7.75\n1.0\n7.750\n7.75\n12.99\n2.0\n6.495\n61.23\n112.76\n...\n1\n0\n91\n7.75\n17\n6\n1.0\n0.0\n0.0\n0\n49998\n28.89\n28.89\n1.0\n28.890\n38.35\n107.76\n4.0\n26.940\n52.97\n249.41\n...\n0\n1\n56\n28.89\n17\n6\n1.0\n0.0\n0.0\n0\n49999\n78.18\n105.43\n2.0\n52.715\n78.18\n153.78\n3.0\n51.260\n78.18\n220.19\n...\n1\n0\n76\n78.18\n17\n6\n1.0\n0.0\n0.0\n0\n5 rows × 36 columns\nStep 3 - Train models and choose highest accuracy#\nWith MLRun, one can easily train different models and compare the results. The code below trains three different models, and chooses the model with the highest accuracy.\nEach uses a different algorithm (random forest, XGBoost, adabost).\n# Import the Sklearn classifier function from the function hub\nclassifier_fn = mlrun.import_function('hub://sklearn-classifier')\n# Prepare the parameters list for the training function\n# Use 3 different models\ntraining_params = {\"model_name\": ['transaction_fraud_rf',\n'transaction_fraud_xgboost',\n'transaction_fraud_adaboost'],\n\"model_pkg_class\": ['sklearn.ensemble.RandomForestClassifier',\n'sklearn.ensemble.GradientBoostingClassifier',\n'sklearn.ensemble.AdaBoostClassifier']}\n# Define the training task, including the feature vector, label and hyperparams definitions\ntrain_task = mlrun.new_task('training',\ninputs={'dataset': transactions_fv.uri},\nparams={'label_column': 'label'}\n)\ntrain_task.with_hyper_params(training_params, strategy='list', selector='max.accuracy')\n# Specify the cluster image\nclassifier_fn.spec.image = 'mlrun/mlrun'\n# Run training\nclassifier_fn.run(train_task, local=False)\n> 2021-09-19 17:59:28,799 [info] starting run training uid=9349c60dd9f24a33b536c59e89978e7b DB=http://mlrun-api:8080\n> 2021-09-19 17:59:29,042 [info] Job is running in the background, pod: training-2jntc\n> 2021-09-19 17:59:47,926 [info] best iteration=1, used criteria max.accuracy\n> 2021-09-19 17:59:48,990 [info] run executed, status=completed\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\nfraud-demo-admin\n...89978e7b\n0\nSep 19 17:59:32\ncompleted\ntraining\nv3io_user=adminkind=jobowner=admin\ndataset\nlabel_column=label\nbest_iteration=1accuracy=0.9901828681424446test-error=0.009817131857555342rocauc=0.9556168449721417brier_score=0.008480115495668912f1-score=0.6666666666666667precision_score=0.7846153846153846recall_score=0.5795454545454546\ntest_setprobability-calibrationconfusion-matrixfeature-importancesprecision-recall-binaryroc-binarymodeliteration_results\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-09-19 17:59:51,574 [info] run executed, status=completed\n<mlrun.model.RunObject at 0x7f464baf0c50>\nStep 4 - Perform feature selection#\nAs part of our data science process, try to reduce the training dataset’s size to get rid of bad or unuseful features and save computation time.\nUse the ready-made feature selection function from the hub hub://feature_selection to select the best features to keep on a sample from the dataset, and run the function on that.\nfeature_selection_fn = mlrun.import_function('hub://feature_selection')\nfeature_selection_run = feature_selection_fn.run(\nparams={'sample_ratio':0.25,\n'output_vector_name':fv_name + \"-short\",\n'ignore_type_errors': True},\ninputs={'df_artifact': transactions_fv.uri},\nname='feature_extraction',\nhandler='feature_selection',\nlocal=False)\n> 2021-09-19 17:59:51,768 [info] starting run feature_extraction uid=3a50bd0e4175459fb53873d8f78a440a DB=http://mlrun-api:8080\n> 2021-09-19 17:59:52,004 [info] Job is running in the background, pod: feature-extraction-lf46d\n> 2021-09-19 17:59:59,099 [info] Couldn't calculate chi2 because of: Input X must be non-negative.\n> 2021-09-19 18:00:04,008 [info] votes needed to be selected: 3\n> 2021-09-19 18:00:05,329 [info] wrote target: {'name': 'parquet', 'kind': 'parquet', 'path': 'v3io:///projects/fraud-demo-admin/FeatureStore/transactions-fraud-short/parquet/vectors/transactions-fraud-short-latest.parquet', 'status': 'ready', 'updated': '2021-09-19T18:00:05.329695+00:00', 'size': 668722}\n> 2021-09-19 18:00:05,677 [info] run executed, status=completed\nPass k=5 as keyword args. From version 0.25 passing these as positional arguments will result in an error\nLiblinear failed to converge, increase the number of iterations.\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\nfraud-demo-admin\n...f78a440a\n0\nSep 19 17:59:56\ncompleted\nfeature_extraction\nv3io_user=adminkind=jobowner=adminhost=feature-extraction-lf46d\ndf_artifact\nsample_ratio=0.25output_vector_name=transactions-fraud-shortignore_type_errors=True\ntop_features_vector=store://feature-vectors/fraud-demo-admin/transactions-fraud-short\nf_classifmutual_info_classiff_regressionLinearSVCLogisticRegressionExtraTreesClassifierfeature_scoresmax_scaled_scores_feature_scoresselected_features_countselected_features\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-09-19 18:00:07,537 [info] run executed, status=completed\nmlrun.get_dataitem(feature_selection_run.outputs['top_features_vector']).as_df().tail(5)\namount_max_2h\namount_sum_2h\namount_count_2h\namount_avg_2h\namount_max_12h\nlabel\n49996\n37.40\n37.40\n1.0\n37.400000\n37.40\n0\n49997\n7.75\n7.75\n1.0\n7.750000\n7.75\n0\n49998\n28.89\n28.89\n1.0\n28.890000\n38.35\n0\n49999\n78.18\n105.43\n2.0\n52.715000\n78.18\n0\n50000\n19.37\n24.61\n3.0\n8.203333\n19.37\n0\nStep 5 - Train the models with top features#\nFollowing the feature selection, you train new models using the resultant features. You can observe the accuracy and other results remain high,\nmeaning you get a model that requires less features to be accurate and thus less error-prone.\n# Defining our training task, including our feature vector, label and hyperparams definitions\nensemble_train_task = mlrun.new_task('training',\ninputs={'dataset': feature_selection_run.outputs['top_features_vector']},\nparams={'label_column': 'label'}\n)\nensemble_train_task.with_hyper_params(training_params, strategy='list', selector='max.accuracy')\nclassifier_fn.run(ensemble_train_task)\n> 2021-09-19 18:00:07,661 [info] starting run training uid=a6d9ae72cfd3462cace205f8b363d214 DB=http://mlrun-api:8080\n> 2021-09-19 18:00:08,077 [info] Job is running in the background, pod: training-v2bt4\n> 2021-09-19 18:00:20,781 [info] best iteration=3, used criteria max.accuracy\n> 2021-09-19 18:00:21,696 [info] run executed, status=completed\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\nfraud-demo-admin\n...b363d214\n0\nSep 19 18:00:11\ncompleted\ntraining\nv3io_user=adminkind=jobowner=admin\ndataset\nlabel_column=label\nbest_iteration=3accuracy=0.9899143672692674test-error=0.010085632730732635rocauc=0.9655151930226706brier_score=0.19856508884931476f1-score=0.6490066225165563precision_score=0.7205882352941176recall_score=0.5903614457831325\ntest_setprobability-calibrationconfusion-matrixfeature-importancesprecision-recall-binaryroc-binarymodeliteration_results\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-09-19 18:00:27,561 [info] run executed, status=completed\n<mlrun.model.RunObject at 0x7f464baed490>\nDone!#\nYou’ve completed Part 2 of the model training with the feature store.\nProceed to Part 3 to learn how to deploy and monitor the model."}
{"text": "Part 3: Serving#\nIn this part you use MLRun’s serving runtime to deploy the trained models from the previous stage a Voting Ensemble using max vote logic.\nYou will also use MLRun’s Feature store to receive the latest tag of the online Feature Vector we defined in the previous stage.\nBy the end of this tutorial you’ll learn how to:\nDefine a model class to load the models, run preprocessing, and predict on the data\nDefine Voting Ensemble function on top of our models\nTest the serving function locally using the mock server\nDeploy the function to the cluster and test it live\nEnvironment setup#\nFirst, make sure SciKit-Learn is installed in the correct version:\n!pip install -U scikit-learn==1.0.2\nRestart your kernel post installing.\nSecondly, since the work is done in this project scope, define the project itself for all your MLRun work in this notebook.\nproject_name = 'fraud-demo'\nimport mlrun\n# Initialize the MLRun project object\nproject = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)\n> 2021-10-28 11:59:01,033 [info] loaded project fraud-demo from MLRun DB\nDefine model class#\nLoad models\nPredict from the FS Online service via the source key\n# mlrun: start-code\nimport numpy as np\nfrom cloudpickle import load\nfrom mlrun.serving.v2_serving import V2ModelServer\nclass ClassifierModel(V2ModelServer):\ndef load(self):\n\"\"\"load and initialize the model and/or other elements\"\"\"\nmodel_file, extra_data = self.get_model('.pkl')\nself.model = load(open(model_file, 'rb'))\ndef predict(self, body: dict) -> list:\n\"\"\"Generate model predictions from sample\"\"\"\nprint(f\"Input -> {body['inputs']}\")\nfeats = np.asarray(body['inputs'])\nresult: np.ndarray = self.model.predict(feats)\nreturn result.tolist()\n# mlrun: end-code\nDefine a serving function#\nMLRun serving can produce managed real-time serverless pipelines from various tasks, including MLRun models or standard model files.\nThe pipelines use the Nuclio real-time serverless engine, which can be deployed anywhere.\nNuclio is a high-performance open-source serverless framework that’s focused on data, I/O, and compute-intensive workloads.\nThe EnrichmentVotingEnsemble and the EnrichmentModelRouter router classes auto enrich the request with data from the feature store.\nThe router input accepts lists of inference request (each request can be a dict or list of incoming features/keys). It enriches the request with data from the specified feature vector (feature_vector_uri).\nIn many cases the features can have null values (None, NaN, Inf, …). The Enrichment routers can substitute the null value with fixed or statistical value per feature. This is done through the impute_policy parameter, which accepts the impute policy per feature (where * is used to specify the default). The value can be a fixed number for constants or $mean, $max, $min, $std, $count for statistical values. to substitute the value with the equivalent feature stats (taken from the feature store).\nThe code below performs the following steps:\nGather ClassifierModel code from this notebook\nDefine EnrichmentVotingEnsemble - Max-Vote based ensemble with feature enrichment and imputing\nAdd the previously trained models to the ensemble\n# Create the serving function from the code above\nserving_fn = mlrun.code_to_function('transaction-fraud', kind='serving', image=\"mlrun/mlrun\")\nserving_fn.set_topology('router', 'mlrun.serving.routers.EnrichmentVotingEnsemble', name='VotingEnsemble',\nfeature_vector_uri=\"transactions-fraud-short\", impute_policy={\"*\": \"$mean\"})\nmodel_names = [\n'RandomForestClassifier',\n'GradientBoostingClassifier',\n'AdaBoostClassifier'\n]\nfor i, name in enumerate(model_names, start=1):\nserving_fn.add_model(name, class_name=\"ClassifierModel\", model_path=project.get_artifact_uri(f\"training_model#{i}:latest\"))\n# Plot the ensemble configuration\nserving_fn.spec.graph.plot()\nTest the server locally#\nBefore deploying the serving function, test it in the current notebook and check the model output.\n# Create a mock server from the serving function\nlocal_server = serving_fn.to_mock_server()\n> 2021-10-28 11:59:11,260 [info] model RandomForestClassifier was loaded\n> 2021-10-28 11:59:11,306 [info] model GradientBoostingClassifier was loaded\n> 2021-10-28 11:59:11,350 [info] model AdaBoostClassifier was loaded\n# Choose an id for the test\nsample_id = 'C76780537'\nmodel_inference_path = '/v2/models/infer'\n# Send our sample ID for prediction\nlocal_server.test(path=model_inference_path,\nbody={'inputs': [[sample_id]]})\n# Notice the input vector is printed 3 times (once per child model) and is enriched with data from the feature store\nInput -> [[14.68, 14.68, 1.0, 14.68, 70.81]]\nInput -> [[14.68, 14.68, 1.0, 14.68, 70.81]]\nInput -> [[14.68, 14.68, 1.0, 14.68, 70.81]]\n{'id': '757c736c985a4c42b3ebd58f3c50f1b2',\n'model_name': 'VotingEnsemble',\n'outputs': [0],\n'model_version': 'v1'}\nAccessing the real-time feature vector directly#\nYou can also directly query the feature store values using the get_online_feature_service method. This method is used internally in the EnrichmentVotingEnsemble router class\nimport mlrun.feature_store as fstore\n# Create the online feature service\nsvc = fstore.get_online_feature_service('transactions-fraud-short:latest', impute_policy={\"*\": \"$mean\"})\n# Get sample feature vector\nsample_fv = svc.get([{'source': sample_id}])\nsample_fv\n[{'amount_max_2h': 14.68,\n'amount_max_12h': 70.81,\n'amount_sum_2h': 14.68,\n'amount_count_2h': 1.0,\n'amount_avg_2h': 14.68}]\nDeploying the function on the kubernetes cluster#\nYou can now deploy the function. Once it’s deployed you get a function with an http trigger that can be called from other locations.\nimport os\n# Enable model monitoring\nserving_fn.set_tracking()\nproject.set_model_monitoring_credentials(os.getenv('V3IO_ACCESS_KEY'))\n# Deploy the serving function\nserving_fn.deploy()\n> 2021-10-28 11:59:17,554 [info] Starting remote function deploy\n2021-10-28 11:59:17  (info) Deploying function\n2021-10-28 11:59:17  (info) Building\n2021-10-28 11:59:17  (info) Staging files and preparing base images\n2021-10-28 11:59:17  (info) Building processor image\n2021-10-28 11:59:19  (info) Build complete\n2021-10-28 11:59:25  (info) Function deploy complete\n> 2021-10-28 11:59:25,657 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-fraud-demo-admin-transaction-fraud.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['default-tenant.app.yh38.iguazio-cd2.com:32287']}\n'http://default-tenant.app.yh38.iguazio-cd2.com:32287'\nTest the server#\nTest the serving function and examine the model output.\n# Choose an id for the test\nsample_id = 'C76780537'\nmodel_inference_path = '/v2/models/infer'\n# Send the sample ID for prediction\nserving_fn.invoke(path=model_inference_path,\nbody={'inputs': [[sample_id]]})\n> 2021-10-28 11:59:25,722 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-fraud-demo-admin-transaction-fraud.default-tenant.svc.cluster.local:8080/v2/models/infer'}\n{'id': '4b9c4914-964f-4bd5-903d-c4885ed7c090',\n'model_name': 'VotingEnsemble',\n'outputs': [0],\n'model_version': 'v1'}\nYou can also directly query the feature store values, which are used in the enrichment.\nSimulate incoming data#\n# Load the dataset\ndata = mlrun.get_dataitem('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/data.csv').as_df()\n# Sample 50k lines\ndata = data.sample(50000)\n# keys\nsample_ids = data['source'].to_list()\nfrom random import choice, uniform\nfrom time import sleep\n# Sending random requests\nfor _ in range(4000):\ndata_point = choice(sample_ids)\ntry:\nresp = serving_fn.invoke(path=model_inference_path, body={'inputs': [[data_point]]})\nprint(resp)\nsleep(uniform(0.2, 1.7))\nexcept OSError:\npass\n> 2021-10-28 12:00:23,079 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-fraud-demo-admin-transaction-fraud.default-tenant.svc.cluster.local:8080/v2/models/infer'}\n{'id': '6b813638-e9ef-4e92-85c8-cfbd0b74fe32', 'model_name': 'VotingEnsemble', 'outputs': [0], 'model_version': 'v1'}\n> 2021-10-28 12:00:23,857 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-fraud-demo-admin-transaction-fraud.default-tenant.svc.cluster.local:8080/v2/models/infer'}\n{'id': 'f84bf2ec-a718-4e90-a7d5-fe08e254f3c8', 'model_name': 'VotingEnsemble', 'outputs': [0], 'model_version': 'v1'}\n> 2021-10-28 12:00:24,545 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-fraud-demo-admin-transaction-fraud.default-tenant.svc.cluster.local:8080/v2/models/infer'}\n{'id': '7bb023f7-edbc-47a6-937b-4a15c8380b74', 'model_name': 'VotingEnsemble', 'outputs': [0], 'model_version': 'v1'}\n> 2021-10-28 12:00:24,921 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-fraud-demo-admin-transaction-fraud.default-tenant.svc.cluster.local:8080/v2/models/infer'}\n{'id': '57882cca-537a-43e1-9986-1bbc72fb84b7', 'model_name': 'VotingEnsemble', 'outputs': [0], 'model_version': 'v1'}"}
{"text": "Part 4: Automated ML pipeline#\nMLRun Project is a container for all your work on a particular activity: sll the associated code, functions,\njobs/workflows and artifacts. Projects can be mapped to git repositories which enables versioning, collaboration, and CI/CD.\nUsers can create project definitions using the SDK or a yaml file and store those in MLRun DB, file, or archive.\nOnce the project is loaded you can run jobs/workflows which refer to any project element by name, allowing separation between configuration and code.\nProjects contain workflows that execute the registered functions in a sequence/graph (DAG). It can reference project parameters, secrets and artifacts by name. The following notebook demonstrate how to build an automated workflow with feature selection, training, testing, and deployment.\nStep 1: Setting up your project#\nTo run a pipeline, you first need to get or create a project object and define/import the required functions for its execution.\nSee load projects for details.\nThe following code gets or creates a user project named “fraud-demo-<username>”.\n# Set the base project name\nproject_name = 'fraud-demo'\nimport mlrun\n# Initialize the MLRun project object\nproject = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)\n> 2021-10-28 13:54:45,892 [info] loaded project fraud-demo from MLRun DB\nStep 2: Updating project and function definitions#\nYou need to save the definitions for the function you use in the projects so it is possible to automatically convert code to functions or import external functions whenever you load new versions of the code or when you run automated CI/CD workflows. In addition you may want to set other project attributes such as global parameters, secrets, and data.\nThe code can be stored in Python files, notebooks, external repositories, packaged containers, etc. Use the project.set_function() method to register the code in the project. The definitions are saved to the project object, as well as in a YAML file in the root of our project.\nFunctions can also be imported from MLRun Function Hub (using the hub:// schema).\nYou used the following functions in this tutorial:\nfeature_selection — the first function, which determines the top features to be used for training\ntrain — the model-training function\ntest-classifier — the model-testing function\nmlrun-model — the model-serving function\nNote\nset_function uses the code_to_function and import_function methods under the hood (used in the previous notebooks), but in addition it saves the function configurations in the project spec for use in automated workflows and CI/CD.\nAdd the function definitions to the project along with parameters and data artifacts and save the project.\nproject.set_function('hub://feature_selection', 'feature_selection')\nproject.set_function('hub://sklearn-classifier','train')\nproject.set_function('hub://test_classifier', 'test')\nproject.set_function('hub://v2_model_server', 'serving')\n<mlrun.runtimes.serving.ServingRuntime at 0x7f6229497190>\n# set project level parameters and save\nproject.spec.params = {'label_column': 'label'}\nproject.save()\nWhen you save the project it stores the project definitions in the project.yaml. This means that you can load the project from the source control (GIT) and run it with a single command or API call.\nThe project YAML for this project can be printed using:\nprint(project.to_yaml())\nkind: project\nmetadata:\nname: fraud-demo-admin\ncreated: '2021-08-05T15:59:59.434655'\nspec:\nparams:\nlabel_column: label\nfunctions:\n- url: hub://feature_selection\nname: feature_selection\n- url: hub://sklearn-classifier\nname: train\n- url: hub://test_classifier\nname: test\n- url: hub://v2_model_server\nname: serving\nworkflows:\n- name: main\npath: workflow.py\nengine: null\nartifacts: []\ndesired_state: online\ndisable_auto_mount: false\nstatus:\nstate: online\nSaving and loading projects from GIT#\nAfter you save the project and its elements (functions, workflows, artifacts, etc.) you can commit all the changes to a GIT repository. Do this using standard GIT tools or using MLRun project methods such as pull, push, remote that call the Git API for you.\nProjects can then be loaded from Git using MLRun load_project method, for example:\nproject = mlrun.load_project(\"./myproj\", \"git://github.com/mlrun/project-demo.git\", name=project_name)\nor using MLRun CLI:\nmlrun project -n myproj -u \"git://github.com/mlrun/project-demo.git\" ./myproj\nRead load projects for more details.\nUsing Kubeflow pipelines#\nYou’re now ready to create a full ML pipeline.\nThis is done by using Kubeflow Pipelines —\nan open-source framework for building and deploying portable, scalable, machine-learning workflows based on Docker containers.\nMLRun leverages this framework to take your existing code and deploy it as steps in the pipeline.\nStep 3: Defining and saving a pipeline workflow#\nA pipeline is created by running an MLRun “workflow”.\nThe following code defines a workflow and writes it to a file in your local directory;\n(the file name is workflow.py).\nThe workflow describes a directed acyclic graph (DAG) for execution using Kubeflow Pipelines, and depicts the connections between the functions and the data as part of an end-to-end pipeline.\nThe workflow file has a definition of a pipeline DSL for connecting the function inputs and outputs.\nThe defined pipeline includes the following steps:\nPerform feature selection (feature_selection).\nTrain and the model (train).\nTest the model with its test data set.\nDeploy the model as a real-time serverless function (deploy).\nNote: A pipeline can also include continuous build integration and deployment (CI/CD) steps, such as building container images and deploying models.\n%%writefile workflow.py\nimport mlrun\nfrom kfp import dsl\nfrom mlrun.model import HyperParamOptions\nfrom mlrun import (\nbuild_function,\ndeploy_function,\nimport_function,\nrun_function,\n)\n@dsl.pipeline(\nname=\"Fraud Detection Pipeline\",\ndescription=\"Detecting fraud from a transactions dataset\"\n)\ndef kfpipeline(vector_name='transactions-fraud'):\nproject = mlrun.get_current_project()\n# Feature selection\nfeature_selection = run_function(\n\"feature_selection\",\nname=\"feature_selection\",\nparams={'sample_ratio':0.25,'output_vector_name': \"short\",\n'ignore_type_errors': True},\ninputs={'df_artifact': project.get_artifact_uri(vector_name, 'feature-vector')},\noutputs=['top_features_vector'])\n# train with hyper-paremeters\ntrain = run_function(\n\"train\",\nname=\"train\",\nparams={\"sample\": -1,\n\"label_column\": project.get_param('label_column', 'label'),\n\"test_size\": 0.10},\nhyperparams={\"model_name\": ['transaction_fraud_rf',\n'transaction_fraud_xgboost',\n'transaction_fraud_adaboost'],\n'model_pkg_class': [\"sklearn.ensemble.RandomForestClassifier\",\n\"sklearn.linear_model.LogisticRegression\",\n\"sklearn.ensemble.AdaBoostClassifier\"]},\nhyper_param_options=HyperParamOptions(selector=\"max.accuracy\"),\ninputs={\"dataset\": feature_selection.outputs['top_features_vector']},\noutputs=['model', 'test_set'])\n# test and visualize our model\ntest = run_function(\n\"test\",\nname=\"test\",\nparams={\"label_column\": project.get_param('label_column', 'label')},\ninputs={\n\"models_path\": train.outputs[\"model\"],\n\"test_set\": train.outputs[\"test_set\"]})\n# route the serving model to use enrichment\nfuncs['serving'].set_topology('router',\n'mlrun.serving.routers.EnrichmentModelRouter',\nname='EnrichmentModelRouter',\nfeature_vector_uri=\"transactions-fraud-short\",\nimpute_policy={\"*\": \"$mean\"},\nexist_ok=True)\n# deploy the model as a serverless function, you can pass a list of models to serve\ndeploy = deploy_function(\"serving\", models=[{\"key\": 'fraud', \"model_path\": train.outputs[\"model\"]}])\nOverwriting workflow.py\nStep 4: Registering the workflow#\nUse the set_workflow MLRun project method to register your workflow with MLRun.\nThe following code sets the name parameter to the selected workflow name (“main”) and the code parameter to the name of the workflow file that is found in your project directory (workflow.py).\n# Register the workflow file as \"main\"\nproject.set_workflow('main', 'workflow.py')\nStep 5: Running a pipeline#\nFirst run the following code to save your project:\nproject.save()\nUse the run MLRun project method to execute your workflow pipeline with Kubeflow Pipelines.\nYou can pass arguments or set the artifact_path to specify a unique path for storing the workflow artifacts.\nrun_id = project.run(\n'main',\narguments={},\ndirty=True, watch=True)\nRun ResultsWorkflow 34db6d3c-858e-4bb5-9a6c-547baec5d0a7 finished, state=Succeededclick the hyper links below to see detailed results\nuid\nstart\nstate\nname\nresults\nartifacts\n...f4ecf295\nOct 28 13:56:30\ncompleted\ntest-classifier\naccuracy=0.9883058032451396test-error=0.0116941967548604rocauc=0.8130881224506281brier_score=0.22075754415862567f1-score=0.36507936507936506precision_score=0.6052631578947368recall_score=0.26136363636363635\nprobability-calibrationconfusion-matrixfeature-importancesprecision-recall-binaryroc-binarytest_set_preds\n...51758356\nOct 28 13:55:22\ncompleted\nsklearn-classifier\nbest_iteration=7accuracy=0.9896594661902441test-error=0.010340533809755834rocauc=0.8228432450474152brier_score=0.2209646484723041f1-score=0.3612040133779264precision_score=0.6206896551724138recall_score=0.25471698113207547\ntest_setprobability-calibrationconfusion-matrixfeature-importancesprecision-recall-binaryroc-binarymodeliteration_results\n...802f80a4\nOct 28 13:54:57\ncompleted\nfeature-selection\ntop_features_vector=store://feature-vectors/fraud-demo-admin/short\nf_classifmutual_info_classiff_regressionLinearSVCLogisticRegressionExtraTreesClassifierfeature_scoresmax_scaled_scores_feature_scoresselected_features_countselected_features\nStep 6: Test the model end point#\nNow that your model is deployed using the pipeline, you can invoke it as usual:\n# Define the serving function\nserving_fn = project.func('serving')\n# Choose an id for the test\nsample_id = 'C76780537'\nmodel_inference_path = '/v2/models/fraud/infer'\n# Send the sample ID for predcition\nserving_fn.invoke(path=model_inference_path,\nbody={'inputs': [[sample_id]]})\n> 2021-10-28 13:56:56,170 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-fraud-demo-admin-v2-model-server.default-tenant.svc.cluster.local:8080/v2/models/fraud/infer'}\n{'id': '90f4b67c-c9e0-4e35-917f-979b71c5ad75',\n'model_name': 'fraud',\n'outputs': [0.0]}\nDone!#"}
{"text": "Feature store end-to-end demo#\nThis demo shows the usage of MLRun and the feature store:\nData ingestion & preparation\nModel training & testing\nModel serving\nBuilding an automated ML pipeline\nFraud prevention, specifically, is a challenge since it requires processing raw transactions and events in real-time and being able to\nquickly respond and block transactions before they occur. Consider, for example, a case where you would like to evaluate the\naverage transaction amount. When training the model, it is common to take a DataFrame and just calculate the average. However,\nwhen dealing with real-time/online scenarios, this average has to be calculated incrementally.\nThis demo illustrates how to Ingest different data sources to the Feature Store. Specifically, it covers two types of data:\nTransactions: Monetary activity between two parties to transfer funds.\nEvents: Activity performed by a party, such as login or password change.\nThe demo walks through creation of an ingestion pipeline for each data source with all the needed preprocessing and validation. It runs the pipeline locally within the notebook and then launches a real-time function to ingest live data or schedule a cron to run the task when needed.\nFollowing the ingestion, you create a feature vector, select the most relevant features and create a final model. Then you deploy the model and showcase the feature vector and model serving.\nPart 1: Data Ingestion\nPart 2: Training\nPart 3: Serving\nPart 4: Automated ML pipeline"}
{"text": "Feature sets#\nIn MLRun, a group of features can be ingested together and stored in logical group called feature set.\nFeature sets take data from offline or online sources, build a list of features through a set of transformations, and\nstore the resulting features along with the associated metadata and statistics.\nA feature set can be viewed as a database table with multiple material implementations for batch and real-time access,\nalong with the data pipeline definitions used to produce the features.\nThe feature set object contains the following information:\nMetadata — General information which is helpful for search and organization. Examples are project, name, owner, last update, description, labels, etc.\nKey attributes — Entity (the join key), timestamp key (optional), label column.\nFeatures — The list of features along with their schema, metadata, validation policies and statistics.\nSource — The online or offline data source definitions and ingestion policy (file, database, stream, http endpoint, etc.).\nTransformation — The data transformation pipeline (e.g. aggregation, enrichment etc.).\nTarget stores — The type (i.e. parquet/csv or key value), location and status for the feature set materialized data.\nFunction — The type (storey, pandas, spark) and attributes of the data pipeline serverless functions.\nIn this section\nCreate a Feature Set\nAdd transformations\nSimulate and debug the data pipeline with a small dataset\nSee also Ingest data using the feature store\nCreate a feature set#\nCreate a new FeatureSet with the base definitions:\nname—The feature set name is a unique name within a project.\nentities—Each feature set must be associated with one or more index column. When joining feature sets, the entity is used as the key column.\ntimestamp_key—(optional) Used for specifying the time field when joining by time.\nengine—The processing engine type:\nSpark\npandas\nstorey (some advanced functionalities are in the Beta state)\nExample:\n#Create a basic feature set example\nstocks_set = FeatureSet(\"stocks\", entities=[Entity(\"ticker\")])\nTo learn more about Feature Sets go to FeatureSet.\nNote\nFeature sets can also be created in the UI. To create a feature set:\nSelect a project and press Feature store, then press Create Set.\nAfter completing the form, press Save and Ingest to start the process, or Save to save the set for later ingestion.\nAdd transformations#\nDefine the data processing steps using a transformations graph (DAG).\nA feature set data pipeline takes raw data from online or offline sources and transforms it to meaningful features.\nThe MLRun feature store supports three processing engines (storey, pandas, spark) that can run in the client\n(e.g. Notebook) for interactive development or in elastic serverless functions for production and scale.\nThe data pipeline is defined using MLRun graph (DAG) language. Graph steps can be pre-defined operators\n(such as aggregate, filter, encode, map, join, impute, etc.) or custom python classes/functions.\nRead more about the graph in Real-time serving pipelines (graphs).\nThe pandas and spark engines are good for simple batch transformations, while the storey stream processing engine (the default engine)\ncan handle complex workflows and real-time sources.\nThe results from the transformation pipeline are stored in one or more material targets.  Data for offline\naccess, such as training, is usually stored in Parquet files. Data for online access such as serving is stored\nin the Iguazio NoSQL DB ( NoSqlTarget). You can use the default targets or add/replace with additional custom targets. See Target stores(#target-stores).\nGraph example (storey engine):\nimport mlrun.feature_store as fstore\nfeature_set = fstore.FeatureSet(\"measurements\", entities=[Entity(key)], timestamp_key=\"timestamp\")\n# Define the computational graph including the custom functions\nfeature_set.graph.to(DropColumns(drop_columns))\\\n.to(RenameColumns(mapping={'bad': 'bed'}))\nfeature_set.add_aggregation('hr', ['avg'], [\"1h\"])\nfeature_set.plot()\nfstore.ingest(feature_set, data_df)\nGraph example (pandas engine):\ndef myfunc1(df, context=None):\ndf = df.drop(columns=[\"exchange\"])\nreturn df\nstocks_set = fstore.FeatureSet(\"stocks\", entities=[Entity(\"ticker\")], engine=\"pandas\")\nstocks_set.graph.to(name=\"s1\", handler=\"myfunc1\")\ndf = fstore.ingest(stocks_set, stocks_df)\nThe graph steps can use built-in transformation classes, simple python classes, or function handlers.\nSee more details in Feature set transformations.\nSimulate and debug the data pipeline with a small dataset#\nDuring the development phase it’s pretty common to check the feature set definition and to simulate the creation of the feature set before ingesting the entire dataset, since ingesting the entire feature set can take time.\nThis allows you to get a preview of the results (in the returned dataframe). The simulation method is called infer. It infers the source data schema as well as processing the graph logic (assuming there is one) on a small subset of data.\nThe infer operation also learns the feature set schema and does statistical analysis on the result by default.\ndf = fstore.preview(quotes_set, quotes)\n# print the featue statistics\nprint(quotes_set.get_stats_table())"}
{"text": "Feature store overview#\nIn machine-learning scenarios, generating a new feature, called feature engineering, takes a tremendous amount of work. The same features\nmust be used both for training, based on historical data, and for the model prediction based on the online or real-time data. This creates a\nsignificant additional engineering effort, and leads to model inaccuracy when the online and offline features do not match. Furthermore,\nmonitoring solutions must be built to track features and results and send alerts of data or model drift.\nConsider a scenario in which you train a model and one of its features is a comparison of the current amount to the average amount spent\nduring the last 3 months by the same person. Creating such a feature is easy when you have the full dataset in training, but in serving,\nthis feature must be calculated in an online manner. The “brute-force” way to address this is to have an ML engineer create an online\npipeline that reimplements all the feature calculations done in the offline process. This is not just time-consuming and error-prone, but\nvery difficult to maintain over time, and results in a lengthy deployment time. This is exacerbated when having to deal with thousands of\nfeatures with an increasing number of data engineers and data scientists that are creating and using the features.\nWith MLRun’s feature store you can easily define features during the training, that are deployable to serving, without having to define all the\n“glue” code. You simply create the necessary building blocks to define features and integration, with offline and online storage systems to access the features.\nThe feature store is comprised of the following:\nFeature — In machine-learning, a feature is an individual measurable property or characteristic of a phenomenon being observed.\nThis can be raw data (e.g., transaction amount, image pixel, etc.) or a calculation derived from one or more other features (e.g., deviation\nfrom average, pattern on image, etc.).\nFeature sets —  A grouping of features that are ingested together and stored in a logical group. Feature sets take data from\noffline or online sources, build a list of features through a set of transformations, and store the resulting features, along with the\nassociated metadata and statistics. For example, a transaction may be grouped by the ID of a person performing the transfer or by the device\nidentifier used to perform the transaction. You can also define in the timestamp source in the feature set, and ingest data into a\nfeature set.\nExecution — A set of operations performed on the data while it is\ningested. The graph contains steps that represent data sources and targets, and can also contain steps that transform and enrich the data that is passed through the feature set. For a deeper dive, see Feature set transformations.\nFeature vectors — A set of features, taken from one or more feature sets. The feature vector is defined prior to model\ntraining and serves as the input to the model training process. During model serving, the feature values in the vector are obtained from an online service.\nHow the feature store works#\nThe common flow when working with the feature store is to first define the feature set with its source, transformation graph, and targets.\nMLRun’s robust transformation engine performs complex operations with just a few lines of Python code. To test the\nexecution process, call the infer method with a sample DataFrame. This runs all operations in memory without storing the results. Once the\ngraph is defined, it’s time to ingest the data.\nYou can ingest data directly from a DataFrame, by calling the feature set ingest method. You can also define an ingestion\nprocess that runs as a Kubernetes job. This is useful if there is a large ingestion process, or if there is a recurrent ingestion and you\nwant to schedule the job.\nMLRun can also leverage Nuclio to perform real-time ingestion by calling the deploy_ingestion_service function. This means that during\nserving you can update feature values, and not just read them. For example, you can update a sliding window aggregation as part of a model\nserving process.\nThe next step is to define the feature vector. Call the get_offline_features function to join together features across different feature sets.\nTraining and serving using the feature store#\nNext, extract a versioned offline static dataset for training, based on the parquet target defined in the feature sets. You can train a\nmodel with the feature vector data by providing the input in the form of 'store://feature-vectors/{project}/{feature_vector_name}'.\nTraining functions generate models and various model statistics. Use MLRun’s auto logging capabilities to store the models along with all\nthe relevant data, metadata and measurements.\nMLRun can apply all the MLOps functionality by using the framework specific apply_mlrun() method, which manages the training process and\nautomatically logs all the framework specific model details, data, metadata and metrics.\nThe training job automatically generates a set of results and versioned artifacts (run train_run.outputs to view the job outputs).\nFor serving, once you validate the feature vector, use the online feature service, based on the\nnosql target defined in the feature set for real-time serving. For serving, you define a serving class derived from\nmlrun.serving.V2ModelServer. In the class load method, call the get_online_feature_service function with the vector name, which returns\na feature service object. In the class preprocess method, call the feature service get method to get the values of those features.\nUsing this feature store centric process, using one computation graph definition for a feature set, you receive an automatic online and\noffline implementation for the feature vectors, with data versioning both in terms of the actual graph that was used to calculate each data\npoint, and the offline datasets that were created to train each model.\nSee more information in training with the feature store and Serving with the feature store."}
{"text": "Feature store#\nA feature store provides a single pane of glass for sharing all available features across\nthe organization along with their metadata. MLRun Feature store support security, versioning,\nand data snapshots, enabling better data lineage, compliance, and manageability.\nAs illustrated in the diagram below,\nfeature stores provide a mechanism (Feature Sets) to read data from various online or offline sources,\nconduct a set of data transformations, and persist the data in online and offline\nstorage. Features are stored and cataloged along with all their metadata (schema,\nlabels, statistics, etc.), allowing users to compose Feature Vectors and use them for training\nor serving. The Feature Vectors are generated when needed, taking into account data versioning and time\ncorrectness (time traveling). Different function kinds (Nuclio, Spark, Dask) are used for feature retrieval, real-time\nengine for serving, and batch one for training.\nIn this section\nFeature store overview\nFeature sets\nFeature set transformations\nCreating and using feature vectors\nFeature store end-to-end demo"}
{"text": "Creating and using feature vectors#\nYou can define a group of features from different feature sets as a FeatureVector.\nFeature vectors are used as an input for models, allowing you to define the feature vector once, and in turn create and track the\ndatasets created from it or the online manifestation of the vector for real-time prediction needs.\nThe feature vector handles all the merging logic for you using an asof merge type merge that accounts for both the time and the entity.\nIt ensures that all the latest relevant data is fetched, without concerns about “seeing the future” or other types of common time related errors.\nIn this section\nCreating a feature vector\nUsing a feature vector\nCreating a feature vector#\nThe feature vector object holds the following information:\nName — the feature vector’s name as will be later addressed in the store reference store://feature_vectors/<project>/<feature-vector-name> and the UI (after saving the vector).\nDescription — a string description of the feature vector.\nFeatures — a list of features that comprise the feature vector.\nThe feature list is defined by specifying the <feature-set>.<feature-name> for specific features or <feature-set>.* for all the feature set’s features.\nLabel feature — the feature that is the label for this specific feature vector, as a <feature-set>.<feature-name> string specification.\nExample of creating a feature vector:\nimport mlrun.feature_store as fstore\n# Feature vector definitions\nfeature_vector_name = 'example-fv'\nfeature_vector_description = 'Example feature vector'\nfeatures = ['data_source_1.*',\n'data_source_2.feature_1',\n'data_source_2.feature_2',\n'data_source_3.*']\nlabel_feature = 'label_source_1.label_feature'\n# Feature vector creation\nfv = fstore.FeatureVector(name=feature_vector_name,\nfeatures=features,\nlabel_feature=label_feature,\ndescription=feature_vector_description)\n# Save the feature vector in the MLRun DB\n# so it will could be referenced by the `store://`\n# and show in the UI\nfv.save()\nAfter saving the feature vector, it appears in the UI:\nYou can also view some metadata about the feature vector, including all the features, their types, a preview and statistics:\nUsing a feature vector#\nAfter a feature vector is saved, it can be used to create both offline (static) datasets and online (real-time) instances to supply as input to a machine learning model.\nCreating an offline feature vector#\nUse the feature store’s get_offline_features() function to produce a dataset from the feature vector.\nIt creates the dataset (asynchronously if possible), saves it to the requested target, and returns a OfflineVectorResponse.\nDue to the async nature of this action, the response object contains an fv_response.status indicator that, once completed, could be directly turned into a dataframe, parquet or a csv.\nget_offline_features expects to receive:\nfeature_vector —  a feature vector store reference or object.\nentity_rows —  an optional dataframe that the features will be joined to.\nDefaults to the first feature set defined in the features vector’s features list, and acts as the base for the vector’s joins.\nentity_timestamp_column —  an optional specific timestamp column (from the defined features) to act as the base timestamp column.\nDefaults to the base feature set’s timestamp entity.\ntarget —  a Feature Store target to write the results to.\nDefaults to return as a return value to the caller.\nrun_config —  an optional function or a RunConfig to run the feature vector creation process in a remote function.\ndrop_columns —  a list of columns to drop from the resulting feature vector.\nOptional.\nstart_time —  datetime, low limit of time needed to be filtered. Optional.\nend_time —  datetime, high limit of time needed to be filtered. Optional.\nYou can add a time-based filter condition when running get_offline_feature with a given vector. You can also filter with the query argument on all the other features as relevant.\nHere’s an example of a new dataset from a parquet target:\n# Import the Parquet Target, so you can build your dataset from a parquet file\nfrom mlrun.datastore.targets import ParquetTarget\n# Get offline feature vector based on vector and parquet target\noffline_fv = fstore.get_offline_features(feature_vector_name, target=ParquetTarget())\n# Return dataset\ndataset = offline_fv.to_dataframe()\nOnce an offline feature vector is created with a static target (such as ParquetTarget()) the reference to this dataset is saved as part of the feature vector’s metadata and can now be referenced directly through the store as a function input using store://feature-vectors/{project}/{feature_vector_name}.\nFor example:\nfn = mlrun.import_function('hub://sklearn-classifier').apply(auto_mount())\n# Define the training task, including the feature vector and label\ntask = mlrun.new_task('training',\ninputs={'dataset': f'store://feature-vectors/{project}/{feature_vector_name}'},\nparams={'label_column': 'label'}\n)\n# Run the function\nrun = fn.run(task)\nYou can see a full example of using the offline feature vector to create an ML model in part 2 of the end-to-end demo.\nCreating an online feature vector#\nThe online feature vector provides real-time feature vectors to the model using the latest data available.\nFirst create an Online Feature Service using get_online_feature_service(). Then feed the Entity of the feature vector to the service and receive the latest feature vector.\nTo create the OnlineVectorService you only need to pass it the feature vector’s store reference.\nimport mlrun.feature_store as fstore\n# Create the Feature Vector Online Service\nfeature_vector = 'store://feature-vectors/{project}/{feature_vector_name}'\nsvc = fstore.get_online_feature_service(feature_vector)\nThe online feature service supports value imputing (substitute NaN/Inf values with statistical or constant value). You\ncan set the impute_policy parameter with the imputing policy, and specify which constant or statistical value will be used\ninstead of NaN/Inf value. This can be defined per column or for all the columns (\"*\").\nThe replaced value can be a fixed number for constants or $mean, $max, $min, $std, $count for statistical values.\n\"*\" is used to specify the default for all features, for example:\nsvc = fstore.get_online_feature_service(feature_vector, impute_policy={\"*\": \"$mean\", \"age\": 33})\nTo use the online feature service you need to supply a list of entities you want to get the feature vectors for.\nThe service returns the feature vectors as a dictionary of {<feature-name>: <feature-value>} or simply a list of values as numpy arrays.\nFor example:\n# Define the wanted entities\nentities = [{<feature-vector-entity-column-name>: <entity>}]\n# Get the feature vectors from the service\nsvc.get(entities)\nThe entities can be a list of dictionaries as shown in the example, or a list of lists where the values in the internal\nlist correspond to the entity values (e.g. entities = [[\"Joe\"], [\"Mike\"]]). The .get() method returns a dict by default.\nIf you want to return an ordered list of values, set the as_list parameter to True. The list input is required by many ML\nframeworks and this eliminates additional glue logic.\nSee a full example of using the online feature service inside a serving function in part 3 of the end-to-end demo."}
{"text": "Training with the feature store#\nIn this section\nCreating an offline dataset\nTraining\nCreating an offline dataset#\nAn offline dataset is a specific instance of the feature vector definition. To create this instance, use the feature store’s\nget_offline_features(<feature_vector>, <target>) function on the feature vector using the store://<project_name>/<feature_vector>\nreference and an offline target (as in Parquet, CSV, etc.).\nYou can add a time-based filter condition when running get_offline_feature with a given vector. You can also filter with the query argument on all the other features as you like. See get_offline_features().\nimport mlrun.feature_store as fstore\nfeature_vector = '<feature_vector_name>'\noffline_fv = fstore.get_offline_features(feature_vector=feature_vector, target=ParquetTarget())\nBehind the scenes, get_offline_features() runs a local or Kubernetes job (can be specific by the run_config parameter) to retrieve\nall the relevant data from the feature sets, merge them and return it to the specified target which can be a local parquet,\nAZ Blob store or any other type of available storage.\nOnce instantiated with a target, the feature vector holds a reference to the instantiated dataset and references it as its current offline source.\nYou can also use MLRun’s log_dataset() to log the specific dataset to the project as a specific dataset resource.\nTraining#\nTraining your model using the feature store is a fairly simple task. (The offline dataset can also be used for your EDA.)\nTo retrieve a feature vector’s offline dataset, use MLRun’s data item mechanism, referencing the feature vector and\nspecifying to receive it as a DataFrame.\ndf = mlrun.get_dataitem(f'store://feature-vectors/{project}/patient-deterioration').as_df()\nWhen trying to retrieve the dataset in your training function, you can put the feature vector reference as an input to\nthe function and use the as_df() function to retrieve it automatically.\n# A sample MLRun training function\ndef my_training_function(context, # MLRun context\ndataset, # our feature vector reference\n**kwargs):\n# retrieve the dataset\ndf = dataset.as_df()\n# The rest of your training code...\nAnd now you can create the MLRun function and run it locally or over the kubernetes cluster:\n# Creating the training MLRun function with the code\nfn = mlrun.code_to_function('training',\nkind='job',\nhandler='my_training_function')\n# Creating the task to run the function with its dataset\ntask = mlrun.new_task('training',\ninputs={'dataset': f'store://feature-vectors/{project}/{feature_vector_name}'}) # The feature vector is given as an input to the function\n# Running the function over the kubernetes cluster\nfn.run(task) # Set local=True to run locally"}
{"text": "Serving with the feature store#\nIn this section\nGet online features\nIncorporating to the serving model\nGet online features#\nThe online features are created ad-hoc using MLRun’s feature store online feature service and are served from the nosql target for real-time performance needs.\nTo use it, first create an online feature service with the feature vector.\nimport mlrun.feature_store as fstore\nsvc = fstore.get_online_feature_service(<feature vector name>)\nAfter creating the service you can use the feature vector’s entity to get the latest feature vector for it.\nPass a list of {<key name>: <key value>} pairs to receive a batch of feature vectors.\nfv = svc.get([{<key name>: <key value>}])\nIncorporating to the serving model#\nYou can serve your models using the Real-time serving pipelines (graphs). (See a V2 Model Server (SKLearn) example.)\nYou define a serving model class and the computational graph required to run your entire prediction pipeline, and deploy it as a serverless function using nuclio.\nTo embed the online feature service in your model server, just create the feature vector service once when the model initializes, and then use it to retrieve the feature vectors of incoming keys.\nYou can import ready-made classes and functions from the MLRun Function Hub or write your own.\nAs example of a scikit-learn based model server:\nfrom cloudpickle import load\nimport numpy as np\nimport mlrun\nimport os\nclass ClassifierModel(mlrun.serving.V2ModelServer):\ndef load(self):\n\"\"\"load and initialize the model and/or other elements\"\"\"\nmodel_file, extra_data = self.get_model('.pkl')\nself.model = load(open(model_file, 'rb'))\n# Setup FS Online service\nself.feature_service = mlrun.feature_store.get_online_feature_service('patient-deterioration')\n# Get feature vector statistics for imputing\nself.feature_stats = self.feature_service.vector.get_stats_table()\ndef preprocess(self, body: dict, op) -> list:\n# Get patient feature vector\n# from the patient_id given in the request\nvectors = self.feature_service.get([{'patient_id': patient_id} for patient_id in body['inputs']])\n# Impute inf's in the data to the feature's mean value\n# using the collected statistics from the Feature store\nfeature_vectors = []\nfor fv in vectors:\nnew_vec = []\nfor f, v in fv.items():\nif np.isinf(v):\nnew_vec.append(self.feature_stats.loc[f, 'mean'])\nelse:\nnew_vec.append(v)\nfeature_vectors.append(new_vec)\n# Set the final feature vector as the inputs\n# to pass to the predict function\nbody['inputs'] = feature_vectors\nreturn body\ndef predict(self, body: dict) -> list:\n\"\"\"Generate model predictions from sample\"\"\"\nfeats = np.asarray(body['inputs'])\nresult: np.ndarray = self.model.predict(feats)\nreturn result.tolist()\nWhich you can deploy with:\n# Create the serving function from the code above\nfn = mlrun.code_to_function(<function_name>,\nkind='serving')\n# Add a specific model to the serving function\nfn.add_model(<model_name>,\nclass_name='ClassifierModel',\nmodel_path=<store_model_file_reference>)\n# Enable MLRun's model monitoring\nfn.set_tracking()\n# Add the system mount to the function so\n# it will have access to the model files\nfn.apply(mlrun.mount_v3io())\n# Deploy the function to the cluster\nfn.deploy()\nAnd test using:\nfn.invoke('/v2/models/infer', body={<key name>: <key value>})"}
{"text": "Feature set transformations#\nA feature set contains an execution graph of operations that are performed when data is ingested, or\nwhen simulating data flow for inferring its metadata. This graph utilizes MLRun’s\nReal-time serving pipelines (graphs).\nThe graph contains steps that represent data sources and targets, and may also contain steps whose\npurpose is transformations and enrichment of the data passed through the feature set. These transformations\ncan be provided in one of three ways:\nAggregations — MLRun supports adding aggregate features to a feature set through the\nadd_aggregation() function.\nBuilt-in transformations — MLRun is equipped with a set of transformations\nprovided through the storey.transformations package. These transformations can be added to the\nexecution graph to perform common operations and transformations.\nCustom transformations — You can extend the built-in functionality by\nadding new classes that perform any custom operation and use them in the serving graph.\nOnce a feature-set is created, its internal execution graph can be observed by calling the feature-set’s\nplot() function, which generates a graphviz plot based on the internal\ngraph. This is very useful when running within a Jupyter notebook, and produces a graph such as the\nfollowing example:\nThis plot shows various transformations and aggregations being used as part of the feature-set processing, as well as\nthe targets where results are saved to (in this case two targets). Feature-sets can also be observed in the MLRun\nUI, where the full graph can be seen and specific step properties can be observed:\nFor a full end-to-end example of feature-store and usage of the functionality described in this page, refer\nto the feature store example.\nIn this section\nAggregations\nBuilt-in transformations\nCustom transformations\nAggregations#\nAggregations, being a common tool in data preparation and ML feature engineering, are available directly through\nthe MLRun FeatureSet class. These transformations allow adding a new feature to the\nfeature-set that is created by performing some aggregate function over feature’s values within a time-based\nsliding window.\nFor example, if a feature-set contains stock trading data including the specific bid price for each bid at any\ngiven time, you could introduce aggregate features that show the minimal and maximal bidding price over all\nthe bids in the last hour, per stock ticker (which is the entity in question). To do that, use the code:\nimport mlrun.feature_store as fstore\n# create a new feature set\nquotes_set = fstore.FeatureSet(\"stock-quotes\", entities=[fstore.Entity(\"ticker\")])\nquotes_set.add_aggregation(\"bid\", [\"min\", \"max\"], [\"1h\"], \"10m\")\nOnce this is executed, the feature-set has new features introduced, with their names produced from the aggregate\nparameters, using this format: {column}_{operation}_{window}. Thus, the example above generates two new features:\nbid_min_1h and bid_max_1h. If the function gets an optional name parameter, features are produced in {name}_{operation}_{window} format.\nIf the name parameter is not specified, features are produced in {column_name}_{operation}_{window} format.\nThese features can then be fed into predictive models or be used for additional\nprocessing and feature generation.\nNotes\nInternally, the graph step that is created to perform these aggregations is named \"Aggregates\". If more than one\naggregation steps are needed, a unique name must be provided to each, using the state_name parameter.\nThe timestamp column must be part of the feature set definition (for aggregation).\nAggregations that are supported using this function are:\ncount\nsum\nsqr (sum of squares)\nmax\nmin\nfirst\nlast\navg\nstdvar\nstddev\nFor a full documentation of this function, see the add_aggregation()\ndocumentation.\nBuilt-in transformations#\nMLRun, and the associated storey package, have a built-in library of transformation functions that can be\napplied as steps in the feature-set’s internal execution graph. In order to add steps to the graph, it should be\nreferenced from the FeatureSet object by using the\ngraph property. Then, new steps can be added to the graph using the\nfunctions in storey.transformations (follow the link to browse the documentation and the\nlist of existing functions). The transformations are also accessible directly from the storey module.\nSee the built-in steps.\nNote\nInternally, MLRun makes use of functions defined in the storey package for various purposes. When creating a\nfeature-set and configuring it with sources and targets, what MLRun does behind the scenes is to add steps to the\nexecution graph that wraps methods and classes, which perform the actions. When defining an async execution graph,\nstorey classes are used. For example, when defining a Parquet data-target in MLRun, a graph step is created that\nwraps storey’s WriteToParquet() function.\nTo use a function:\nAccess the graph from the feature-set object, using the graph property.\nAdd steps to the graph using the various graph functions, such as to().\nThe function object passed to the step should point at the transformation function being used.\nThe following is an example for adding a simple filter to the graph, that drops any bid which is lower than\n50USD:\nquotes_set.graph.to(\"storey.Filter\", \"filter\", _fn=\"(event['bid'] > 50)\")\nIn the example above, the parameter _fn denotes a callable expression that is passed to the storey.Filter\nclass as the parameter fn. The callable parameter can also be a Python function, in which case there’s no need for\nparentheses around it. This call generates a step in the graph called filter that calls the expression provided\nwith the event being propagated through the graph as data is fed to the feature-set.\nCustom transformations#\nWhen a transformation is needed that is not provided by the built-in functions, new classes that implement\ntransformations can be created and added to the execution graph. Such classes should extend the\nMapClass class, and the actual transformation should be implemented within their do()\nfunction, which receives an event and returns the event after performing transformations and manipulations on it.\nFor example, consider the following code:\nclass MyMap(MapClass):\ndef __init__(self, multiplier=1, **kwargs):\nsuper().__init__(**kwargs)\nself._multiplier = multiplier\ndef do(self, event):\nevent[\"multi\"] = event[\"bid\"] * self._multiplier\nreturn event\nThe MyMap class can then be used to construct graph steps, in the same way as shown above for built-in functions:\nquotes_set.graph.add_step(\"MyMap\", \"multi\", after=\"filter\", multiplier=3)\nThis uses the add_step function of the graph to add a step called multi utilizing MyMap after the filter step\nthat was added previously. The class is initialized with a multiplier of 3."}
{"text": "Ingest features with Spark#\nThe feature store supports using Spark for ingesting, transforming, and writing results to data targets. When\nusing Spark, the internal execution graph is executed synchronously by utilizing a Spark session to perform read and\nwrite operations, as well as potential transformations on the data. Executing synchronously means that the\nsource data is fully read into a data-frame that is processed, writing the output to the targets defined.\nTo use Spark as the transformation engine in ingestion, follow these steps:\nWhen constructing the FeatureSet object, pass an engine parameter and set it\nto spark. For example:\nfeature_set = fstore.FeatureSet(\"stocks\", entities=[fstore.Entity(\"ticker\")], engine=\"spark\")\nTo use a remote execution engine, pass a RunConfig object as the run_config parameter for\nthe ingest API. The actual remote function to execute depends on the object passed:\nA default RunConfig, in which case the ingestion code either generates a new MLRun function runtime\nof type remote-spark, or utilizes the function specified in feature_set.spec.function (in which case,\nit has to be of runtime type remote-spark or spark).\nA RunConfig that has a function configured within it. As mentioned, the function runtime must be of\ntype remote-spark or spark.\nSpark execution can be done locally, utilizing a local Spark session provided to the ingestion call. To use a local Spark session, pass a\nSpark session context when calling the ingest() function, as the\nspark_context parameter. This session is used for data operations and transformations.\nSee code examples in:\nLocal Spark ingestion example\nRemote Spark ingestion example\nSpark operator ingestion example\nSpark dataframe ingestion example\nSpark over S3 full flow example\nSpark ingestion from Snowflake example\nSpark ingestion from Azure example\nLocal Spark ingestion example#\nA local Spark session is a session running in the Jupyter service.\nThe following code executes data ingestion using a local Spark session.\nWhen using a local Spark session, the ingest API would wait for its completion.\nimport mlrun\nfrom mlrun.datastore.sources import CSVSource\nimport mlrun.feature_store as fstore\nfrom pyspark.sql import SparkSession\nmlrun.get_or_create_project(name=\"stocks\")\nfeature_set = fstore.FeatureSet(\"stocks\", entities=[fstore.Entity(\"ticker\")], engine=\"spark\")\n# add_aggregation can be used in conjunction with Spark\nfeature_set.add_aggregation(\"price\", [\"min\", \"max\"], [\"1h\"], \"10m\")\nsource = CSVSource(\"mycsv\", path=\"v3io:///projects/stocks.csv\")\n# Execution using a local Spark session\nspark = SparkSession.builder.appName(\"Spark function\").getOrCreate()\nfstore.ingest(feature_set, source, spark_context=spark)\nRemote Spark ingestion example#\nRemote Spark refers to  a session running from another service, for example, the Spark standalone service or the Spark operator service.\nWhen using remote execution the MLRun run execution details are returned, allowing tracking of its status and results.\nThe following code should be executed only once to build the remote spark image before running the first ingest.\nIt may take a few minutes to prepare the image.\nfrom mlrun.runtimes import RemoteSparkRuntime\nRemoteSparkRuntime.deploy_default_image()\nRemote ingestion:\n# mlrun: start-code\nfrom mlrun.feature_store.api import ingest\ndef ingest_handler(context):\ningest(mlrun_context=context) # The handler function must call ingest with the mlrun_context\nYou can run your PySpark code for ingesting data into the feature store by adding:\ndef my_spark_func(df, context=None):\nreturn df.filter(\"bid>55\") # PySpark code\n# mlrun: end-code\nfrom mlrun.datastore.sources import CSVSource\nfrom mlrun import code_to_function\nimport mlrun.feature_store as fstore\nfeature_set = fstore.FeatureSet(\"stock-quotes\", entities=[fstore.Entity(\"ticker\")], engine=\"spark\")\nsource = CSVSource(\"mycsv\", path=\"v3io:///projects/quotes.csv\")\nspark_service_name = \"iguazio-spark-service\" # As configured & shown in the Iguazio dashboard\nfeature_set.graph.to(name=\"s1\", handler=\"my_spark_func\")\nmy_func = code_to_function(\"func\", kind=\"remote-spark\")\nconfig = fstore.RunConfig(local=False, function=my_func, handler=\"ingest_handler\")\nfstore.ingest(feature_set, source, run_config=config, spark_context=spark_service_name)\nSpark operator ingestion example#\nWhen running with a Spark operator, the MLRun execution details are returned, allowing tracking of the job’s status and results. Spark operator ingestion is always executed remotely.\nThe following code should be executed only once to build the spark job image before running the first ingest.\nIt may take a few minutes to prepare the image.\nfrom mlrun.runtimes import Spark3Runtime\nSpark3Runtime.deploy_default_image()\nSpark operator ingestion:\n# mlrun: start-code\nfrom mlrun.feature_store.api import ingest\ndef ingest_handler(context):\ningest(mlrun_context=context) # The handler function must call ingest with the mlrun_context\n# You can add your own PySpark code as a graph step:\ndef my_spark_func(df, context=None):\nreturn df.filter(\"bid>55\") # PySpark code\n# mlrun: end-code\nfrom mlrun.datastore.sources import CSVSource\nfrom mlrun import code_to_function\nimport mlrun.feature_store as fstore\nfeature_set = fstore.FeatureSet(\"stock-quotes\", entities=[fstore.Entity(\"ticker\")], engine=\"spark\")\nsource = CSVSource(\"mycsv\", path=\"v3io:///projects/quotes.csv\")\nfeature_set.graph.to(name=\"s1\", handler=\"my_spark_func\")\nmy_func = code_to_function(\"func\", kind=\"spark\")\nmy_func.with_driver_requests(cpu=\"200m\", mem=\"1G\")\nmy_func.with_executor_requests(cpu=\"200m\", mem=\"1G\")\nmy_func.with_igz_spark()\n# Enables using the default image (can be replace with specifying a specific image with .spec.image)\nmy_func.spec.use_default_image = True\n# Not a must - default: 1\nmy_func.spec.replicas = 2\n# If needed, sparkConf can be modified like this:\n# my_func.spec.spark_conf['spark.specific.config.key'] = 'value'\nconfig = fstore.RunConfig(local=False, function=my_func, handler=\"ingest_handler\")\nfstore.ingest(feature_set, source, run_config=config)\nSpark dataframe ingestion example#\nThe following code executes local data ingestion from a spark dataframe (Spark dataframe Ingestion cannot be executed remotely.)\nThe specified dataframe should be associated with spark_context.\nfrom pyspark.sql import SparkSession\nimport mlrun.feature_store as fstore\ncolumns = [\"id\", \"count\"]\ndata = [(\"a\", \"12\"), (\"b\", \"14\"), (\"c\", \"88\")]\nspark = SparkSession.builder.appName('example').getOrCreate()\ndf = spark.createDataFrame(data).toDF(*columns)\nfset = fstore.FeatureSet(\"myset\", entities=[fstore.Entity(\"id\")], engine=\"spark\")\nfstore.ingest(fset, df, spark_context=spark)\nspark.stop()\nSpark over S3 - full flow example#\nFor Spark to work with S3, it requires several properties to be set. Spark over S3 can be executed both remotely and locally, as long as access credentials to the S3\nobjects are available to it. The following example writes a\nfeature set to S3 in the parquet format in a remote k8s job:\nOne-time setup:\nDeploy the default image for your job (this takes several minutes but should be executed only once per cluster for any MLRun/Iguazio upgrade):\nfrom mlrun.runtimes import RemoteSparkRuntime\nRemoteSparkRuntime.deploy_default_image()\nStore your S3 credentials in a k8s secret:\nimport mlrun\nsecrets = {'s3_access_key': AWS_ACCESS_KEY, 's3_secret_key': AWS_SECRET_KEY}\nmlrun.get_run_db().create_project_secrets(\nproject = \"uhuh-proj\",\nprovider=mlrun.api.schemas.SecretProviderName.kubernetes,\nsecrets=secrets\n)\nIngestion job code (to be executed in the remote pod):\n# mlrun: start-code\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\nfrom mlrun.feature_store.api import ingest\ndef ingest_handler(context):\nconf = (SparkConf()\n.set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n.set(\"spark.hadoop.fs.s3a.access.key\", context.get_secret('s3_access_key'))\n.set(\"spark.hadoop.fs.s3a.secret.key\", context.get_secret('s3_secret_key'))\n.set(\"spark.hadoop.fs.s3a.endpoint\", context.get_param(\"s3_endpoint\"))\n.set(\"spark.hadoop.fs.s3a.region\", context.get_param(\"s3_region\"))\n.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n.set(\"com.amazonaws.services.s3.enableV4\", True)\n.set(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\"))\nspark = (\nSparkSession.builder.config(conf=conf).appName(\"S3 app\").getOrCreate()\n)\ningest(mlrun_context=context, spark_context=spark)\n# mlrun: end-code\nIngestion invocation:\nfrom mlrun.datastore.sources import CSVSource\nfrom mlrun.datastore.targets import ParquetTarget\nfrom mlrun import code_to_function\nimport mlrun.feature_store as fstore\nfeature_set = fstore.FeatureSet(\"stock-quotes\", entities=[fstore.Entity(\"ticker\")], engine=\"spark\")\nsource = CSVSource(\"mycsv\", path=\"v3io:///projects/quotes.csv\")\nspark_service_name = \"spark\" # As configured & shown in the Iguazio dashboard\nfn = code_to_function(kind='remote-spark',  name='func')\nrun_config = fstore.RunConfig(local=False, function=fn, handler=\"ingest_handler\")\nrun_config.with_secret('kubernetes', ['s3_access_key', 's3_secret_key'])\nrun_config.parameters = {\n\"s3_endpoint\" : \"s3.us-east-2.amazonaws.com\",\n\"s3_region\" : \"us-east-2\"\n}\ntarget = ParquetTarget(\npath = \"s3://my-s3-bucket/some/path\",\npartitioned = False,\n)\nfstore.ingest(feature_set, source, targets=[target], run_config=run_config, spark_context=spark_service_name)\nSpark ingestion from Snowflake example#\nSpark ingestion from Snowflake can be executed both remotely and locally.\nWhen running aggregations, they actually run on Spark and require Spark compute resources.\nThe queries from the database are “regular” snowflake queries and they use Snowflake compute resources.\nNote\nEntity is case sensitive.\nThe following code executes local data ingestion from Snowflake.\nfrom pyspark.sql import SparkSession\nimport mlrun\nimport mlrun.feature_store as fstore\nfrom mlrun.datastore.sources import SnowflakeSource\nspark = SparkSession.builder.appName(\"snowy\").getOrCreate()\nmlrun.get_or_create_project(\"feature_store\")\nfeature_set = fstore.FeatureSet(\nname=\"customer\", entities=[fstore.Entity(\"C_CUSTKEY\")], engine=\"spark\"\n)\nsource = SnowflakeSource(\n\"customer_sf\",\nquery=\"select * from customer limit 100000\",\nurl=\"<url>\",\nuser=\"<user>\",\npassword=\"<password>\",\ndatabase=\"SNOWFLAKE_SAMPLE_DATA\",\nschema=\"TPCH_SF1\",\nwarehouse=\"compute_wh\",\n)\nfstore.ingest(feature_set, source, spark_context=spark)\nSpark ingestion from Azure example#\nSpark ingestion from Azure can be executed both remotely and locally. The following code executes remote data ingestion from Azure.\nimport mlrun\n# Initialize the MLRun project object\nproject_name = \"spark-azure-test\"\nproject = mlrun.get_or_create_project(project_name, context=\"./\")\nfrom mlrun.runtimes import RemoteSparkRuntime\nRemoteSparkRuntime.deploy_default_image()\nfrom mlrun.datastore.sources import CSVSource\nfrom mlrun.datastore.targets import ParquetTarget\nfrom mlrun import code_to_function\nimport mlrun.feature_store as fstore\nfeature_set = fstore.FeatureSet(\"rides7\", entities=[fstore.Entity(\"ride_id\")], engine=\"spark\", timestamp_key=\"key\")\nsource = CSVSource(\"rides\", path=\"wasbs://warroom@mlrunwarroom.blob.core.windows.net/ny_taxi_train_subset_ride_id.csv\")\nspark_service_name = \"spark-fs\" # As configured & shown in the Iguazio dashboard\nfn = code_to_function(kind='remote-spark',  name='func')\nrun_config = fstore.RunConfig(local=False, function=fn, handler=\"ingest_handler\")\ntarget = ParquetTarget(partitioned = True, time_partitioning_granularity=\"month\")\nfeature_set.set_targets(targets=[target],with_defaults=False)\nfstore.ingest(feature_set, source, run_config=run_config, spark_context=spark_service_name)"}
{"text": "Glossary#\nMLRun terms#\nMLRun terms\nDescription\nFeature set\nA group of features that are ingested together and stored in logical group. See Feature sets.\nFeature vector\nA combination of multiple Features originating from different Feature sets. See Creating and using feature vectors.\nHTTPRunDB\nAPI for wrapper to the internal DB in MLRun. See mlrun.db.httpdb.HTTPRunDB.\nhub\nUsed in code to reference the MLRun Function Hub.\nMLRun function\nAn abstraction over the code, extra packages, runtime configuration and desired resources which allow execution in a local environment and on various serverless engines on top of K8s. See MLRun serverless functions and Creating and using functions.\nMLRun Function Hub\nA collection of pre-built MLRun functions avilable for usage. See MLRun Function Hub.\nMLRun project\nA logical container for all the work on a particular activity/application that include functions, workflow, artifacts, secrets, and more, and can be assigned to a specific group of users. See Projects.\nmpijob\nOne of the MLRun batch runtimes that runs distributed jobs and Horovod over the MPI job operator, used mainly for deep learning jobs. See MLRun MPIJob and Horovod runtime.\nNuclio function\nSubtype of MLRun function that uses the Nuclio runtime for any generic real-time function. See Nuclio real-time functions and Nuclio documentation.\nServing function\nSubtype of MLRun function that uses the Nuclio runtime specifically for serving ML models or real-time pipelines. See Real-time serving pipelines (graphs) and Model serving pipelines.\nstorey\nAsynchronous streaming library for real time event processing and feature extraction. Used in Iguazio’s feature store and real-time pipelines. See storey.transformations - Graph transformations.\nIguazio (V3IO) terms#\nName\nDescription\nConsumer group\nSet of consumers that cooperate to consume data from some topics.\nKey Value (KV) store\nType of storage where data is stored by a specific key, allows for real-time lookups.\nV3IO\nIguazio real-time data layer, supports several formats including KV, Block, File, Streams, and more.\nV3IO shard\nUniquely identified data sets within a V3IO stream. Similar to a Kafka partition.\nV3IO stream\nStreaming mechanism part of Iguazio’s V3IO data layer. Similar to a Kafka stream.\nStandard ML terms#\nName\nDescription\nArtifact\nA versioned output of a data processing or model training jobs, can be used as input for other jobs or pipelines in the project. There are various types of artifacts (file, model, dataset, chart, etc.) that incorporate useful metadata. See Artifacts.\nDAG\nDirected acyclic graph, used to describe workflows/pipelines.\nFeature engineering\nApply domain knowledge and statistical techniques to raw data to extract more information out of data and improve performance of machine. learning models\nEDA\nExploratory data analysis. Used by data scientists to understand dataset via cleaning, visualization, and statistical tests.\nML pipeline\nPipeline of operations for machine learning. It can include loading data, feature engineering, feature selection, model training, hyperparameter tuning, model validation, and model deployment.\nFeature\nData field/vector definition and metadata (name, type, stats, etc.). A dataset is a collection of features.\nMLOps\nSet of practices that reliably and efficiently deploys and maintains machine learning models in production. Combination of Machine Learning and DevOps.\nDataframe\nTabular representation of data, often using tools such as Pandas, Spark, or Dask.\nML libraries / tools#\nName\nDescription\nDask\nFlexible library for parallel computing in Python. Often used for data engineering, data science, and machine learning.\nKeras\nAn open-source software library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.\nKubeFlow pipeline\nPlatform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.\nPyTorch\nAn open source machine learning framework based on the Torch library, used for applications such as computer vision and natural language. processing\nSklearn\nOpen source machine learning Python library. Used for modelling, pipelines, data transformations, feature engineering, and more.\nSpark\nOpen source parallel processing framework for running large-scale data analytics applications across clustered computers. Often used for data engineering, data science, and machine learning.\nTensorFlow\nA Google developed open-source software library for machine learning and deep learning.\nTensorBoard\nTensorFlow’s visualization toolkit, used for tracking metrics like loss and accuracy, visualizing the model graph, viewing histograms of weights, biases, or other tensors as they change over time, etc.\nXGBoost\nOptimized distributed gradient boosting library designed to be highly efficient, flexible and portable. Implements machine learning algorithms under the Gradient Boosting framework.\nBack to top"}
{"text": "Hyperparameter tuning optimization#\nMLRun supports iterative tasks for automatic and distributed execution of many tasks with variable parameters (hyperparams). Iterative tasks can be distributed across multiple containers. They can be used for:\nParallel loading and preparation of many data objects\nModel training with different parameter sets and/or algorithms\nParallel testing with many test vector options\nAutoML\nMLRun iterations can be viewed as child runs under the main task/run. Each child run gets a set of parameters that are computed/selected from the input hyperparameters based on the chosen strategy (Grid, List, Random or Custom).\nThe different iterations can run in parallel over multiple containers (using Dask or Nuclio runtimes, which manage the workers). Read more in Parallel execution over containers.\nThe hyperparameters and options are specified in the task or the run() command\nthrough the hyperparams (for hyperparam values) and hyper_param_options (for\nHyperParamOptions) properties. See the examples below. Hyperparameters can also be loaded directly\nfrom a CSV or Json file (by setting the param_file hyper option).\nThe hyperparams are specified as a struct of key: list values for example: {\"p1\": [1,2,3], \"p2\": [10,20]}. The values\ncan be of any type (int, string, float, …). The lists are used to compute the parameter combinations using one of the\nfollowing strategies:\nGrid search (grid) — running all the parameter combinations.\nRandom (random) — running a sampled set from all the parameter combinations.\nList (list) — running the first parameter from each list followed by the seco2nd from each list and so on. All the lists must be of equal size.\nCustom (custom) — determine the parameter combination per run programmatically.\nYou can specify a selection criteria to select the best run among the different child runs by setting the selector option. This marks the selected result as the parent (iteration 0) result, and marks the best result in the user interface.\nYou can also specify the stop_condition to stop the execution of child runs when some criteria, based on the returned results, is met (for example stop_condition=\"accuracy>=0.9\").\nIn this section\nBasic code\nReview the results\nExamples\nParallel execution over containers\nBasic code#\nHere’s a basic example of running multiple jobs in parallel for hyperparameters tuning, selecting the best run with respect to the max accuracy.\nRun the hyperparameters tuning job by using the keywords arguments:\nhyperparams for the hyperparameters options and values of choice.\nselector for specifying how to select the best model.\nhp_tuning_run = project.run_function(\n\"trainer\",\ninputs={\"dataset\": gen_data_run.outputs[\"dataset\"]},\nhyperparams={\n\"n_estimators\": [100, 500, 1000],\n\"max_depth\": [5, 15, 30]\n},\nselector=\"max.accuracy\",\nlocal=True\n)\nThe returned run object in this case represents the parent (and the best result). You can also access the\nindividual child runs (called iterations) in the MLRun UI.\nReview the results#\nWhen running a hyperparam job, the job results tab shows the list and marks the best run:\nYou can also view results by printing the artifact iteration_results:\nhp_tuning_run.artifact(\"iteration_results\").as_df()\nMLRun also generates a parallel coordinates plot for the run, you can view it in the MLRun UI.\nExamples#\nBase dummy function:\nimport mlrun\n> 2021-10-23 12:47:39,982 [warning] Failed resolving version info. Ignoring and using defaults\n> 2021-10-23 12:47:43,488 [warning] Unable to parse server or client version. Assuming compatible: {'server_version': '0.8.0-rc7', 'client_version': 'unstable'}\ndef hyper_func(context, p1, p2):\nprint(f\"p1={p1}, p2={p2}, result={p1 * p2}\")\ncontext.log_result(\"multiplier\", p1 * p2)\nGrid search (default)#\ngrid_params = {\"p1\": [2,4,1], \"p2\": [10,20]}\ntask = mlrun.new_task(\"grid-demo\").with_hyper_params(grid_params, selector=\"max.multiplier\")\nrun = mlrun.new_function().run(task, handler=hyper_func)\n> 2021-10-23 12:47:43,505 [info] starting run grid-demo uid=29c9083db6774e5096a97c9b6b6c8e93 DB=http://mlrun-api:8080\np1=2, p2=10, result=20\np1=4, p2=10, result=40\np1=1, p2=10, result=10\np1=2, p2=20, result=40\np1=4, p2=20, result=80\np1=1, p2=20, result=20\n> 2021-10-23 12:47:44,851 [info] best iteration=5, used criteria max.multiplier\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...6b6c8e93\n0\nOct 23 12:47:43\ncompleted\ngrid-demo\nv3io_user=adminkind=handlerowner=admin\nbest_iteration=5multiplier=80\niteration_results\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-10-23 12:47:45,071 [info] run executed, status=completed\nUI Screenshot:\nRandom Search#\nMLRun chooses random parameter combinations. Limit the number of combinations using the max_iterations attribute.\ngrid_params = {\"p1\": [2,4,1,3], \"p2\": [10,20,30]}\ntask = mlrun.new_task(\"random-demo\")\ntask.with_hyper_params(grid_params, selector=\"max.multiplier\", strategy=\"random\", max_iterations=4)\nrun = mlrun.new_function().run(task, handler=hyper_func)\n> 2021-10-23 12:47:45,077 [info] starting run random-demo uid=cac368c7fc33455f97ca806e5c7abf2f DB=http://mlrun-api:8080\np1=2, p2=20, result=40\np1=4, p2=10, result=40\np1=3, p2=10, result=30\np1=3, p2=20, result=60\n> 2021-10-23 12:47:45,966 [info] best iteration=4, used criteria max.multiplier\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...5c7abf2f\n0\nOct 23 12:47:45\ncompleted\nrandom-demo\nv3io_user=adminkind=handlerowner=admin\nbest_iteration=4multiplier=60\niteration_results\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-10-23 12:47:46,177 [info] run executed, status=completed\nList search#\nThis example also shows how to use the stop_condition option.\nlist_params = {\"p1\": [2,3,7,4,5], \"p2\": [15,10,10,20,30]}\ntask = mlrun.new_task(\"list-demo\").with_hyper_params(\nlist_params, selector=\"max.multiplier\", strategy=\"list\", stop_condition=\"multiplier>=70\")\nrun = mlrun.new_function().run(task, handler=hyper_func)\n> 2021-10-23 12:47:46,184 [info] starting run list-demo uid=136edfb9c9404a61933c73bbbd35b18b DB=http://mlrun-api:8080\np1=2, p2=15, result=30\np1=3, p2=10, result=30\np1=7, p2=10, result=70\n> 2021-10-23 12:47:47,193 [info] reached early stop condition (multiplier>=70), stopping iterations!\n> 2021-10-23 12:47:47,195 [info] best iteration=3, used criteria max.multiplier\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...bd35b18b\n0\nOct 23 12:47:46\ncompleted\nlist-demo\nv3io_user=adminkind=handlerowner=admin\nbest_iteration=3multiplier=70\niteration_results\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-10-23 12:47:47,385 [info] run executed, status=completed\nCustom iterator#\nYou can define a child iteration context under the parent/main run. The child run is logged independently.\ndef handler(context: mlrun.MLClientCtx, param_list):\nbest_multiplier = total = 0\nfor param in param_list:\nwith context.get_child_context(**param) as child:\nhyper_func(child, **child.parameters)\nmultiplier = child.results['multiplier']\ntotal += multiplier\nif multiplier > best_multiplier:\nchild.mark_as_best()\nbest_multiplier = multiplier\n# log result at the parent\ncontext.log_result('avg_multiplier', total / len(param_list))\nparam_list = [{\"p1\":2, \"p2\":10}, {\"p1\":3, \"p2\":30}, {\"p1\":4, \"p2\":7}]\nrun = mlrun.new_function().run(handler=handler, params={\"param_list\": param_list})\n> 2021-10-23 12:47:47,403 [info] starting run mlrun-a79c5c-handler uid=c3eb08ebae02464ca4025c77b12e3c39 DB=http://mlrun-api:8080\np1=2, p2=10, result=20\np1=3, p2=30, result=90\np1=4, p2=7, result=28\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...b12e3c39\n0\nOct 23 12:47:47\ncompleted\nmlrun-a79c5c-handler\nv3io_user=adminkind=handlerowner=adminhost=jupyter-6476bb5f85-bjc4m\nparam_list=[{'p1': 2, 'p2': 10}, {'p1': 3, 'p2': 30}, {'p1': 4, 'p2': 7}]\nbest_iteration=2multiplier=90avg_multiplier=46.0\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-10-23 12:47:48,734 [info] run executed, status=completed\nParallel execution over containers#\nWhen working with compute intensive or long running tasks you’ll want to run your iterations over a cluster of containers. At the same time, you don’t want to bring up too many containers, and you want to limit the number of parallel tasks.\nMLRun supports distribution of the child runs over Dask or Nuclio clusters. This is handled automatically by MLRun. You only need to deploy the Dask or Nuclio function used by the workers, and set the level of parallelism in the task. The execution can be controlled from the client/notebook, or can have a job (immediate or scheduled) that controls the execution.\nCode example (single task)#\n# mark the start of a code section that will be sent to the job\n# mlrun: start-code\nimport socket\nimport pandas as pd\ndef hyper_func2(context, data, p1, p2, p3):\nprint(data.as_df().head())\ncontext.logger.info(f\"p2={p2}, p3={p3}, r1={p2 * p3} at {socket.gethostname()}\")\ncontext.log_result(\"r1\", p2 * p3)\nraw_data = {\n\"first_name\": [\"Jason\", \"Molly\", \"Tina\", \"Jake\", \"Amy\"],\n\"age\": [42, 52, 36, 24, 73],\n\"testScore\": [25, 94, 57, 62, 70],\n}\ndf = pd.DataFrame(raw_data, columns=[\"first_name\", \"age\", \"testScore\"])\ncontext.log_dataset(\"mydf\", df=df, stats=True)\n# mlrun: end-code\nRunning the workers using Dask#\nThis example creates a new function and executes the parent/controller as an MLRun job and the different child runs over a Dask cluster (MLRun Dask function).\nDefine a Dask cluster (using MLRun serverless Dask)#\ndask_cluster = mlrun.new_function(\"dask-cluster\", kind='dask', image='mlrun/ml-models')\ndask_cluster.apply(mlrun.mount_v3io())        # add volume mounts\ndask_cluster.spec.service_type = \"NodePort\"   # open interface to the dask UI dashboard\ndask_cluster.spec.replicas = 2                # define two containers\nuri = dask_cluster.save()\nuri\n'db://default/dask-cluster'\n# initialize the dask cluster and get its dashboard url\ndask_cluster.client\n> 2021-10-23 12:48:49,020 [info] trying dask client at: tcp://mlrun-dask-cluster-eea516ff-5.default-tenant:8786\n> 2021-10-23 12:48:49,049 [info] using remote dask scheduler (mlrun-dask-cluster-eea516ff-5) at: tcp://mlrun-dask-cluster-eea516ff-5.default-tenant:8786\nMismatched versions found\n+-------------+--------+-----------+---------+\n| Package     | client | scheduler | workers |\n+-------------+--------+-----------+---------+\n| blosc       | 1.7.0  | 1.10.6    | None    |\n| cloudpickle | 1.6.0  | 2.0.0     | None    |\n| distributed | 2.30.0 | 2.30.1    | None    |\n| lz4         | 3.1.0  | 3.1.3     | None    |\n| msgpack     | 1.0.0  | 1.0.2     | None    |\n| tornado     | 6.0.4  | 6.1       | None    |\n+-------------+--------+-----------+---------+\nNotes:\n-  msgpack: Variation is ok, as long as everything is above 0.6\ndashboard link: default-tenant.app.yh38.iguazio-cd2.com:31350\nClient\nScheduler: tcp://mlrun-dask-cluster-eea516ff-5.default-tenant:8786\nDashboard: http://mlrun-dask-cluster-eea516ff-5.default-tenant:8787/status\nCluster\nWorkers: 0\nCores: 0\nMemory: 0 B\nDefine the parallel work#\nSet the parallel_runs attribute to indicate how many child tasks to run in parallel. Set the dask_cluster_uri to point\nto the dask cluster (if it’s not set the cluster uri uses dask local). You can also set the teardown_dask flag to free up\nall the dask resources after completion.\ngrid_params = {\"p2\": [2,1,4,1], \"p3\": [10,20]}\ntask = mlrun.new_task(params={\"p1\": 8}, inputs={'data': 'https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv'})\ntask.with_hyper_params(\ngrid_params, selector=\"r1\", strategy=\"grid\", parallel_runs=4, dask_cluster_uri=uri, teardown_dask=True\n)\n<mlrun.model.RunTemplate at 0x7f673d7b1910>\nDefine a job that will take the code (using code_to_function) and run it over the cluster\nfn = mlrun.code_to_function(name='hyper-tst', kind='job', image='mlrun/ml-models')\nrun = fn.run(task, handler=hyper_func2)\n> 2021-10-23 12:49:56,388 [info] starting run hyper-tst-hyper_func2 uid=50eb72f5b0734954b8b1c57494f325bc DB=http://mlrun-api:8080\n> 2021-10-23 12:49:56,565 [info] Job is running in the background, pod: hyper-tst-hyper-func2-9g6z8\n> 2021-10-23 12:49:59,813 [info] trying dask client at: tcp://mlrun-dask-cluster-eea516ff-5.default-tenant:8786\n> 2021-10-23 12:50:09,828 [warning] remote scheduler at tcp://mlrun-dask-cluster-eea516ff-5.default-tenant:8786 not ready, will try to restart Timed out trying to connect to tcp://mlrun-dask-cluster-eea516ff-5.default-tenant:8786 after 10 s\n> 2021-10-23 12:50:15,733 [info] using remote dask scheduler (mlrun-dask-cluster-04574796-5) at: tcp://mlrun-dask-cluster-04574796-5.default-tenant:8786\nremote dashboard: default-tenant.app.yh38.iguazio-cd2.com:32577\n> --------------- Iteration: (1) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:21,353 [info] p2=2, p3=10, r1=20 at mlrun-dask-cluster-04574796-5k5lhq\n> --------------- Iteration: (3) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:21,459 [info] p2=4, p3=10, r1=40 at mlrun-dask-cluster-04574796-5k5lhq\n> --------------- Iteration: (4) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:21,542 [info] p2=1, p3=10, r1=10 at mlrun-dask-cluster-04574796-5k5lhq\n> --------------- Iteration: (6) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:21,629 [info] p2=1, p3=20, r1=20 at mlrun-dask-cluster-04574796-5k5lhq\n> --------------- Iteration: (7) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:21,792 [info] p2=4, p3=20, r1=80 at mlrun-dask-cluster-04574796-5k5lhq\n> --------------- Iteration: (8) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:22,052 [info] p2=1, p3=20, r1=20 at mlrun-dask-cluster-04574796-5k5lhq\n> --------------- Iteration: (2) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:23,134 [info] p2=1, p3=10, r1=10 at mlrun-dask-cluster-04574796-5j6v59\n> --------------- Iteration: (5) ---------------\nsepal length (cm)  sepal width (cm)  ...  petal width (cm)  label\n0                5.1               3.5  ...               0.2      0\n1                4.9               3.0  ...               0.2      0\n2                4.7               3.2  ...               0.2      0\n3                4.6               3.1  ...               0.2      0\n4                5.0               3.6  ...               0.2      0\n[5 rows x 5 columns]\n> 2021-10-23 12:50:23,219 [info] p2=2, p3=20, r1=40 at mlrun-dask-cluster-04574796-5k5lhq\n> 2021-10-23 12:50:23,261 [info] tearing down the dask cluster..\n> 2021-10-23 12:50:43,363 [info] best iteration=7, used criteria r1\n> 2021-10-23 12:50:43,626 [info] run executed, status=completed\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...94f325bc\n0\nOct 23 12:49:59\ncompleted\nhyper-tst-hyper_func2\nv3io_user=adminkind=jobowner=admin\ndata\np1=8\nbest_iteration=7r1=80\nmydfiteration_results\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-10-23 12:50:53,303 [info] run executed, status=completed\nRunning the workers using Nuclio#\nNuclio is a high-performance serverless engine that can process many events in parallel. It can also separate initialization from execution. Certain parts of the code (imports, loading data, etc.) can be done once per worker vs. in any run.\nNuclio, by default, process events (http, stream, …). There is a special Nuclio kind that runs MLRun jobs (nuclio:mlrun).\nNotes\nNuclio tasks are relatively short (preferably under 5 minutes), use it for running many iterations where each individual run is less than 5 min.\nUse context.logger to drive text outputs (vs print()).\nCreate a nuclio:mlrun function#\nfn = mlrun.code_to_function(name='hyper-tst2', kind='nuclio:mlrun', image='mlrun/mlrun')\n# replicas * workers need to match or exceed parallel_runs\nfn.spec.replicas = 2\nfn.with_http(workers=2)\nfn.deploy()\n> 2021-10-23 12:51:10,152 [info] Starting remote function deploy\n2021-10-23 12:51:10  (info) Deploying function\n2021-10-23 12:51:10  (info) Building\n2021-10-23 12:51:10  (info) Staging files and preparing base images\n2021-10-23 12:51:10  (info) Building processor image\n2021-10-23 12:51:11  (info) Build complete\n2021-10-23 12:51:19  (info) Function deploy complete\n> 2021-10-23 12:51:22,296 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-default-hyper-tst2.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['default-tenant.app.yh38.iguazio-cd2.com:32760']}\n'http://default-tenant.app.yh38.iguazio-cd2.com:32760'\nRun the parallel task over the function#\n# this is required to fix Jupyter issue with asyncio (not required outside of Jupyter)\n# run it only once\nimport nest_asyncio\nnest_asyncio.apply()\ngrid_params = {\"p2\": [2,1,4,1], \"p3\": [10,20]}\ntask = mlrun.new_task(params={\"p1\": 8}, inputs={'data': 'https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv'})\ntask.with_hyper_params(\ngrid_params, selector=\"r1\", strategy=\"grid\", parallel_runs=4, max_errors=3\n)\nrun = fn.run(task, handler=hyper_func2)\n> 2021-10-23 12:51:31,618 [info] starting run hyper-tst2-hyper_func2 uid=97cc3e255f3c4c93822b0154d63f47f5 DB=http://mlrun-api:8080\n> --------------- Iteration: (4) ---------------\n2021-10-23 12:51:32.130812  info   logging run results to: http://mlrun-api:8080  worker_id=1\n2021-10-23 12:51:32.401258  info   p2=1, p3=10, r1=10 at nuclio-default-hyper-tst2-5d4976b685-47dh6  worker_id=1\n> --------------- Iteration: (2) ---------------\n2021-10-23 12:51:32.130713  info   logging run results to: http://mlrun-api:8080  worker_id=0\n2021-10-23 12:51:32.409468  info   p2=1, p3=10, r1=10 at nuclio-default-hyper-tst2-5d4976b685-47dh6  worker_id=0\n> --------------- Iteration: (1) ---------------\n2021-10-23 12:51:32.130765  info   logging run results to: http://mlrun-api:8080  worker_id=0\n2021-10-23 12:51:32.432121  info   p2=2, p3=10, r1=20 at nuclio-default-hyper-tst2-5d4976b685-2gdtc  worker_id=0\n> --------------- Iteration: (5) ---------------\n2021-10-23 12:51:32.568848  info   logging run results to: http://mlrun-api:8080  worker_id=0\n2021-10-23 12:51:32.716415  info   p2=2, p3=20, r1=40 at nuclio-default-hyper-tst2-5d4976b685-47dh6  worker_id=0\n> --------------- Iteration: (7) ---------------\n2021-10-23 12:51:32.855399  info   logging run results to: http://mlrun-api:8080  worker_id=1\n2021-10-23 12:51:33.054417  info   p2=4, p3=20, r1=80 at nuclio-default-hyper-tst2-5d4976b685-2gdtc  worker_id=1\n> --------------- Iteration: (6) ---------------\n2021-10-23 12:51:32.970002  info   logging run results to: http://mlrun-api:8080  worker_id=0\n2021-10-23 12:51:33.136621  info   p2=1, p3=20, r1=20 at nuclio-default-hyper-tst2-5d4976b685-47dh6  worker_id=0\n> --------------- Iteration: (3) ---------------\n2021-10-23 12:51:32.541187  info   logging run results to: http://mlrun-api:8080  worker_id=1\n2021-10-23 12:51:33.301200  info   p2=4, p3=10, r1=40 at nuclio-default-hyper-tst2-5d4976b685-47dh6  worker_id=1\n> --------------- Iteration: (8) ---------------\n2021-10-23 12:51:33.419442  info   logging run results to: http://mlrun-api:8080  worker_id=0\n2021-10-23 12:51:33.672165  info   p2=1, p3=20, r1=20 at nuclio-default-hyper-tst2-5d4976b685-47dh6  worker_id=0\n> 2021-10-23 12:51:34,153 [info] best iteration=7, used criteria r1\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...d63f47f5\n0\nOct 23 12:51:31\ncompleted\nhyper-tst2-hyper_func2\nv3io_user=adminkind=remoteowner=admin\ndata\np1=8\nbest_iteration=7r1=80\nmydfiteration_results\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-10-23 12:51:34,420 [info] run executed, status=completed"}
{"text": "Installation and setup guide #\nThis guide outlines the steps for installing and running MLRun.\nMLRun has two main components, the service and the client (SDK):\nMLRun service runs over Kubernetes (can also be deployed using local Docker for demo and test purposes). It can orchestrate and integrate with other open source open source frameworks, as shown in the following diagram.\nMLRun client SDK is installed in your development environment and interacts with the service using REST API calls.\nIn this section\nDeployment options\nSet up your client\nSecurity\nDeployment options#\nThere are several deployment options:\nLocal deployment: Deploy a Docker on your laptop or on a single server.\nThis option is good for testing the waters or when working in a small scale environment. It’s limited in terms of computing resources and scale, but simpler for deployment.\nKubernetes cluster: Deploy an MLRun server on Kubernetes.\nThis option deploys MLRun on a Kubernetes cluster, which supports elastic scaling. Yet, it is more complex to install as it requires you to install Kubernetes on your own.\nAmazon Web Services (AWS): Deploy an MLRun server on AWS.\nThis option is the easiest way to install MLRun cluster and use cloud-based services. The MLRun software is free of charge, however, there is a cost for the AWS infrastructure services.\nIguazio’s Managed  Service: A commercial offering by Iguazio. This is the fastest way to explore the full set of MLRun functionalities.\nNote that Iguazio provides a 14 day free trial.\nSet up your client#\nYou can work with your favorite IDE (e.g. Pycharm, VScode, Jupyter, Colab etc…). Read how to configure your client against the deployed\nMLRun server in How to configure your client.\nOnce you have installed and configured MLRun, follow the Quick Start tutorial and additional Tutorials and Examples to learn how to use MLRun to develop and deploy machine learning applications to production.\nMLRun client backward compatibility#\nStarting from MLRun 0.10.0, the MLRun client and images are compatible with minor MLRun releases that are released during the following 6 months. When you upgrade to 0.11.0, for example, you can continue to use your 0.10-based images.\nImportant\nImages from 0.9.0 are not compatible with 0.10.0. Backward compatibility starts from 0.10.0.\nWhen you upgrade the MLRun major version, for example 0.10.x to 1.0.x, there is no backward compatibility.\nThe feature store is not backward compatible.\nWhen you upgrade the platform, for example from 3.2 to 3.3, the clients should be upgraded. There is no guaranteed compatibility with an older MLRun client after a platform upgrade.\nSee also Images and their usage in MLRun.\nSecurity#\nNon-root user support#\nBy default, MLRun assigns the root user to MLRun runtimes and pods. You can improve the security context by changing the security mode,\nwhich is implemented by Iguazio during installation, and applied system-wide:\nOverride: Use the user id of the user that triggered the current run or use the nogroupid for group id. Requires Iguazio v3.5.1.\nDisabled: Security context is not auto applied (the system applies the root user). (default)\nSecurity context#\nIf your system is configured in disabled mode, you can apply the security context to individual runtimes/pods by using function.with_security_context, and the job is assigned to the user or to the user’s group that ran the job.\n(You cannot override the user of individual jobs if the system is configured in override mode.) The options are:\nfrom kubernetes import client as k8s_client\nsecurity_context = k8s_client.V1SecurityContext(\nrun_as_user=1000,\nrun_as_group=3000,\n)\nfunction.with_security_context(security_context)\nSee the full definition of the V1SecurityContext object.\nSome services do not support security context yet:\nInfrastructure services\nKubeflow pipelines core services\nServices created by MLRun\nKaniko, used for building images. (To avoid using Kaniko, use prebuilt images that contain all the requirements.)\nSpark services"}
{"text": "Install MLRun on AWS#\nFor AWS users, the easiest way to install MLRun is to use a native AWS deployment. This option deploys MLRun on an AWS EKS service using a CloudFormation stack.\nPrerequisites#\nAWS account with permissions that include the ability to:\nRun a CloudFormation stack\nCreate an EKS cluster\nCreate EC2 instances\nCreate VPC\nCreate S3 buckets\nDeploy and pull images from ECR.\nFor the full set of required permissions, download the IAM policy or expand & copy the IAM policy below:\nshow the IAM policy\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"BasicServices\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"autoscaling:*\",\n\"cloudwatch:*\",\n\"elasticloadbalancing:*\",\n\"sns:*\",\n\"ec2:*\",\n\"s3:*\",\n\"s3-object-lambda:*\",\n\"eks:*\",\n\"elasticfilesystem:*\",\n\"cloudformation:*\",\n\"acm:*\",\n\"route53:*\"\n],\n\"Resource\": \"*\"\n},\n{\n\"Sid\": \"ServiceLinkedRoles\",\n\"Effect\": \"Allow\",\n\"Action\": \"iam:CreateServiceLinkedRole\",\n\"Resource\": \"*\",\n\"Condition\": {\n\"StringEquals\": {\n\"iam:AWSServiceName\": [\n\"autoscaling.amazonaws.com\",\n\"ec2scheduled.amazonaws.com\",\n\"elasticloadbalancing.amazonaws.com\",\n\"spot.amazonaws.com\",\n\"spotfleet.amazonaws.com\",\n\"transitgateway.amazonaws.com\"\n]\n}\n}\n},\n{\n\"Sid\": \"IAMPermissions\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"iam:AddRoleToInstanceProfile\",\n\"iam:AttachRolePolicy\",\n\"iam:TagOpenIDConnectProvider\",\n\"iam:CreateInstanceProfile\",\n\"iam:CreateOpenIDConnectProvider\",\n\"iam:CreateRole\",\n\"iam:CreateServiceLinkedRole\",\n\"iam:DeleteInstanceProfile\",\n\"iam:DeleteOpenIDConnectProvider\",\n\"iam:DeleteRole\",\n\"iam:DeleteRolePolicy\",\n\"iam:DetachRolePolicy\",\n\"iam:GenerateServiceLastAccessedDetails\",\n\"iam:GetAccessKeyLastUsed\",\n\"iam:GetAccountPasswordPolicy\",\n\"iam:GetAccountSummary\",\n\"iam:GetGroup\",\n\"iam:GetInstanceProfile\",\n\"iam:GetLoginProfile\",\n\"iam:GetOpenIDConnectProvider\",\n\"iam:GetPolicy\",\n\"iam:GetPolicyVersion\",\n\"iam:GetRole\",\n\"iam:GetRolePolicy\",\n\"iam:GetServiceLastAccessedDetails\",\n\"iam:GetUser\",\n\"iam:ListAccessKeys\",\n\"iam:ListAccountAliases\",\n\"iam:ListAttachedGroupPolicies\",\n\"iam:ListAttachedRolePolicies\",\n\"iam:ListAttachedUserPolicies\",\n\"iam:ListGroupPolicies\",\n\"iam:ListGroups\",\n\"iam:ListGroupsForUser\",\n\"iam:ListInstanceProfilesForRole\",\n\"iam:ListMFADevices\",\n\"iam:ListOpenIDConnectProviders\",\n\"iam:ListPolicies\",\n\"iam:ListPoliciesGrantingServiceAccess\",\n\"iam:ListRolePolicies\",\n\"iam:ListRoles\",\n\"iam:ListRoleTags\",\n\"iam:ListSAMLProviders\",\n\"iam:ListSigningCertificates\",\n\"iam:ListUserPolicies\",\n\"iam:ListUsers\",\n\"iam:ListUserTags\",\n\"iam:PassRole\",\n\"iam:PutRolePolicy\",\n\"iam:RemoveRoleFromInstanceProfile\",\n\"kms:CreateGrant\",\n\"kms:CreateKey\",\n\"kms:Decrypt\",\n\"kms:DescribeKey\",\n\"kms:Encrypt\",\n\"kms:GenerateDataKeyWithoutPlaintext\",\n\"kms:GetKeyPolicy\",\n\"kms:GetKeyRotationStatus\",\n\"kms:ListResourceTags\",\n\"kms:PutKeyPolicy\",\n\"kms:ScheduleKeyDeletion\",\n\"kms:TagResource\"\n],\n\"Resource\": \"*\"\n},\n{\n\"Sid\": \"AllowLanbda\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"lambda:CreateAlias\",\n\"lambda:CreateCodeSigningConfig\",\n\"lambda:CreateEventSourceMapping\",\n\"lambda:CreateFunction\",\n\"lambda:CreateFunctionUrlConfig\",\n\"lambda:Delete*\",\n\"lambda:Get*\",\n\"lambda:InvokeAsync\",\n\"lambda:InvokeFunction\",\n\"lambda:InvokeFunctionUrl\",\n\"lambda:List*\",\n\"lambda:PublishLayerVersion\",\n\"lambda:PublishVersion\",\n\"lambda:PutFunctionCodeSigningConfig\",\n\"lambda:PutFunctionConcurrency\",\n\"lambda:PutFunctionEventInvokeConfig\",\n\"lambda:PutProvisionedConcurrencyConfig\",\n\"lambda:TagResource\",\n\"lambda:UntagResource\",\n\"lambda:UpdateAlias\",\n\"lambda:UpdateCodeSigningConfig\",\n\"lambda:UpdateEventSourceMapping\",\n\"lambda:UpdateFunctionCode\",\n\"lambda:UpdateFunctionCodeSigningConfig\",\n\"lambda:UpdateFunctionConfiguration\",\n\"lambda:UpdateFunctionEventInvokeConfig\",\n\"lambda:UpdateFunctionUrlConfig\"\n],\n\"Resource\": \"*\"\n},\n{\n\"Sid\": \"CertificateService\",\n\"Effect\": \"Allow\",\n\"Action\": \"iam:CreateServiceLinkedRole\",\n\"Resource\": \"arn:aws:iam::*:role/aws-service-role/acm.amazonaws.com/AWSServiceRoleForCertificateManager*\",\n\"Condition\": {\n\"StringEquals\": {\n\"iam:AWSServiceName\": \"acm.amazonaws.com\"\n}\n}\n},\n{\n\"Sid\": \"DeleteRole\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"iam:DeleteServiceLinkedRole\",\n\"iam:GetServiceLinkedRoleDeletionStatus\",\n\"iam:GetRole\"\n],\n\"Resource\": \"arn:aws:iam::*:role/aws-service-role/acm.amazonaws.com/AWSServiceRoleForCertificateManager*\"\n},\n{\n\"Sid\": \"SSM\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"logs:*\",\n\"ssm:AddTagsToResource\",\n\"ssm:GetParameter\",\n\"ssm:DeleteParameter\",\n\"ssm:PutParameter\",\n\"cloudtrail:GetTrail\",\n\"cloudtrail:ListTrails\"\n],\n\"Resource\": \"*\"\n}\n]\n}\nFor more information, see how to create a new AWS account and policies and permissions in IAM.\nNotes:\nTo access the instances, you will need to have at least one key pair for SSH keys. For more information see Amazon EC2 key pairs and Linux instances.\nYou need to have a Route53 domain configured. External domain registration is currently not supported. For more information see What is Amazon Route 53.\nThe MLRun software is free of charge, however, there is a cost for the AWS infrastructure services such as EKS, EC2, S3 and ECR. The actual pricing depends on a large set of factors including, for example, the region, the number of EC2 instances, the amount of storage consumed, and the data transfer costs. Other factors include, for example, reserved instance configuration, saving plan, and AWS credits you have associated with your account. It is recommended to use the AWS pricing calculator to calculate the expected cost, as well as the AWS Cost Explorer to manage the cost, monitor and set-up alerts.\nPost deployment expectations#\nThe key components deployed on your EKS cluster are:\nMLRun server (including the feature store and the MLRun graph)\nMLRun UI\nKubeflow pipeline\nReal time serverless framework (Nuclio)\nSpark operator\nJupyter lab\nGrafana\nConfiguration settings#\nMake sure you are logged in to the correct AWS account.\nClick the AWS icon to deploy MLRun.\nAfter clicking the icon, the browser directs you to the CloudFormation stack page in your AWS account, or redirects you to the AWS login page if you are not currently logged in.\nNote: You must fill in fields marked as mandatory (m) for the configuration to complete. Fields marked as optional (o) can be left blank.\nStack name (m)—the name of the stack. You cannot continue if left blank. This field becomes the logical id of the stack. Stack name can include letters (A-Z and a-z), numbers (0-9), and dashes (-).\nParameters\nEKS cluster name (m)—the name of EKS cluster created. The EKS cluster is used to run the MLRun services. For example, https://jupyter.<eks_cluster_name>.<route53_domain_name>\nVPC network Configuration\nNumber of Availability Zones (m)—number of availability zones. The default is set to 3. Choose from the dropdown to change the number. The minimum is 2.\nAvailability zones (m)—select a zone from the dropdown. The list is based on the region of the instance. The number of zones must match the number of zones Number of Availability Zones.\nAllowed external access CIDR (m)—range of IP address allowed to access the cluster. Addresses that are not in this range are will not be able to access the cluster.\nAmazon EKS configuration\nAdditional EKS admin ARN (IAM user) (o)—add an additional admin user to the instance. Users can be added after the stack has been created. For more information see Create a kubeconfig for Amazon EKS.\nInstance type (m)—select from the dropdown list. The default is m5.2xlarge. For size considerations see Amazon EC2 Instance Types.\nNumber of Nodes (m)—number of nodes in the cluster. The minimum must match the number of Availability Zones. The number of nodes combined with the Instance type determines the AWS infrastructure cost.\nAmazon EC2 configuration\nSSH key name (m)—select from the stored keys in the dropdown. The list is based on the SSH keys that are in your account. For more information about SSH Keys see Amazon EC2 key pairs and Linux instances.\nProvision bastion host (m)—create a bastion host for SSH access to the Kubernetes nodes. The default is enabled.\nIguazio MLRun configuration\nRoute 53 hosted DNS domain (m)—enter the name of your registered Route53 domain. Note: Only route53 domains are acceptable.\nThe URL of your REDIS database (o)—the URL of your Redis database. This is only required if you’re using Redis with the online feature store. See how to configure the online feature store for more details.\nOther parameters\nMLrunCeVersion (m)—the MLRun Community Edition version to install. Leave the default value for the latest CE release.\nCapabilities\nCheck all the capabilities boxes (m).\nPress Create Stack to continue the deployment.\nThe stack creates a VPC with an EKS cluster and deploys all the services on top of it.\nNote: It could take up to 2 hours for your stack to be created.\nGetting started#\nWhen the stack is complete, go to the output tab for the stack you created. There are links for the MLRun UI, Jupyter and the Kubeconfig command.\nIt’s recommended to go through the quick-start and the other tutorials as shown in the documentation. These tutorials and demos come built-in with Jupyter under the root folder of Jupyter.\nStorage resources#\nWhen installing the MLRun Community Edition via Cloud Formation, several storage resources are created:\nPVs via AWS storage provider: Used to hold the file system of the stacks pods, including the MySQL database of MLRun. These are deleted when the stack is uninstalled.\nS3 Bucket: A bucket named <EKS cluster name>-<Random string> is created in the AWS account that installs the stack (where <EKS cluster name> is the name of the EKS cluster you chose and <Random string> is part of the CloudFormation stack ID). You can see the bucket name in the output tab of the stack. The bucket is used for MLRun’s artifact storage, and is not deleted when uninstalling the stack. The user must empty the bucket and delete it.\nContainer Images in ECR: When building and deploying MLRun and Nuclio functions via the MLRun Community Edition, the function images are stored in an ECR belonging to the AWS account that installs the stack. These images persist in the account’s ECR and are not deleted either.\nHow to configure the online feature store#\nThe feature store can store data on a fast key value database table for quick serving. This online feature store capability requires an external key value database.\nCurrently the MLRun feature store supports the following options:\nRedis\nIguazio key value database\nTo use Redis, you must install Redis separately and provide the Redis URL when configuring the AWS CloudFormation stack. Refer to the Redis getting-started page for information about Redis installation.\nStreaming support#\nFor online serving, it is often convenient to use MLRun graph with a streaming engine. This allows managing queues between steps and functions.\nMLRun supports Kafka streams as well as Iguazio V3IO streams.\nSee the examples on how to configure the MLRun serving graph with kafka and V3IO.\nCleanup#\nTo free up the resources used by MLRun:\nDelete the stack. See instructions for deleting a stack on the AWS CloudFormation console for more details.\nDelete the S3 bucket that begins with the same name as your EKS cluster. The S3 bucket name is available in the CloudFormation stack output tab.\nDelete any remaining images in ECR.\nYou may also need to check any external storage that you used."}
{"text": "Install MLRun on Kubernetes#\nIn this section\nPrerequisites\nCommunity Edition Flavors\nInstalling on Docker Desktop\nInstalling the Lite Version\nInstalling the Full Version\nConfiguring Online Feature Store\nStart working\nConfiguring the remote environment\nAdvanced chart configuration\nUninstalling the chart\nUpgrading the chart\nPrerequisites#\nAccess to a Kubernetes cluster. You must have administrator permissions in order to install MLRun on your cluster. For local installation\non Windows or Mac, Docker Desktop is recommended. MLRun fully supports k8s releases 1.22 and 1.23.\nThe Kubernetes command-line tool (kubectl) compatible with your Kubernetes cluster is installed. Refer to the kubectl installation\ninstructions for more information.\nHelm 3.6 CLI is installed. Refer to the Helm installation instructions for more information.\nAn accessible docker-registry (such as Docker Hub). The registry’s URL and credentials are consumed by the applications via a pre-created secret.\nStorage: 7Gi\nRAM: A minimum of 8Gi is required for running all the initial MLRun components. The amount of RAM required for running MLRun jobs depends on the job’s requirements.\nNote\nThe MLRun Community Edition resources are configured initially with the default cluster/namespace resources limits. You can modify the resources from outside if needed.\nCommunity Edition flavors#\nThe MLRun CE (Community Edition) chart arrives in 2 flavors - lite and full.\nThe lite version is the default installation and includes the following components:\nMLRun - https://github.com/mlrun/mlrun\nMLRun API\nMLRun UI\nMLRun DB (MySQL)\nNuclio - https://github.com/nuclio/nuclio\nJupyter - https://github.com/jupyter/notebook (+MLRun integrated)\nMPI Operator - https://github.com/kubeflow/mpi-operator\nMinio - https://github.com/minio/minio/tree/master/helm/minio\nThe Full Version also includes:\nSpark Operator - https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\nPipelines - https://github.com/kubeflow/pipelines\nPrometheus stack - https://github.com/prometheus-community/helm-charts\nPrometheus\nGrafana\nInstalling on Docker Desktop#\nDocker Desktop is available for Mac and Windows. For download information, system requirements, and installation instructions, see:\nInstall Docker Desktop on Mac\nInstall Docker Desktop on Windows. Note that WSL 2 backend was tested, Hyper-V was not tested.\nConfiguring Docker Desktop#\nDocker Desktop includes a standalone Kubernetes server and client, as well as Docker CLI integration that runs on your machine. The\nKubernetes server runs locally within your Docker instance. To enable Kubernetes support and install a standalone instance of Kubernetes\nrunning as a Docker container, go to Preferences > Kubernetes and then click Enable Kubernetes. Click Apply & Restart to\nsave the settings and then click Install to confirm. This instantiates the images that are required to run the Kubernetes server as\ncontainers, and installs the /usr/local/bin/kubectl command on your machine. For more information, see the Kubernetes documentation.\nIt’s recommended to limit the amount of memory allocated to Kubernetes. If you’re using Windows and WSL 2, you can configure global WSL options by placing a .wslconfig file into the root directory of your users folder: C:\\Users\\<yourUserName>\\.wslconfig. Keep in mind that you might need to run wsl --shutdown to shut down the WSL 2 VM and then restart your WSL instance for these changes to take effect.\n[wsl2]\nmemory=8GB # Limits VM memory in WSL 2 to 8 GB\nTo learn about the various UI options and their usage, see:\nDocker Desktop for Mac user manual\nDocker Desktop for Windows user manual\nInstalling the Lite Version#\nNote\nThese instructions use mlrun as the namespace (-n parameter). You can choose a different namespace in your kubernetes cluster.\nCreate a namespace for the deployed components:\nkubectl create namespace mlrun\nAdd the Community Edition helm chart repo:\nhelm repo add mlrun-ce https://mlrun.github.io/ce\nUpdate the repo to make sure you’re getting the latest chart:\nhelm repo update\nCreate a secret with your docker-registry named registry-credentials:\nkubectl --namespace mlrun create secret docker-registry registry-credentials \\\n--docker-server <your-registry-server> \\\n--docker-username <your-username> \\\n--docker-password <your-password> \\\n--docker-email <your-email>\nWhere:\n<your-registry-server> is your Private Docker Registry FQDN. (https://index.docker.io/v1/ for Docker Hub).\n<your-username> is your Docker username.\n<your-password> is your Docker password.\n<your-email> is your Docker email.\nNote\nFirst-time MLRun users will experience a relatively longer installation time because all required images\nare being pulled locally for the first time (it will take an average of 10-15 minutes mostly depends on\nyour internet speed).\nTo install the chart with the release name mlrun-ce use the following command.\nNote the reference to the pre-created registry-credentials secret in global.registry.secretName:\nhelm --namespace mlrun \\\ninstall mlrun-ce \\\n--wait \\\n--timeout 960s \\\n--set global.registry.url=<registry-url> \\\n--set global.registry.secretName=registry-credentials \\\n--set global.externalHostAddress=<host-machine-address> \\\nmlrun-ce/mlrun-ce\nWhere:\n<registry-url> is the registry URL that can be authenticated by the registry-credentials secret (e.g., index.docker.io/<your-username> for Docker Hub).\n<host-machine-address> is the IP address of the host machine (or $(minikube ip) if using minikube).\nWhen the installation is complete, the helm command prints the URLs and Ports of all the MLRun CE services.\nInstalling the Full Version#\nTo install the full version of the chart, use the following command:\nhelm --namespace mlrun \\\ninstall mlrun-ce \\\n--wait \\\n--timeout 960s \\\n-f override-full.yaml \\\n--set global.registry.url=<registry-url> \\\n--set global.registry.secretName=registry-credentials \\\n--set global.externalHostAddress=<host-machine-address> \\\nmlrun-ce/mlrun-ce\nConfiguring Online Feature Store#\nThe MLRun Community Edition now supports the online feature store. To enable it, you need to first deploy a REDIS service that is accessible to your MLRun CE cluster.\nTo deploy a REDIS service, refer to the following link.\nWhen you have a REDIS service deployed, you can configure MLRun CE to use it by adding the following helm value configuration to your helm install command:\n--set mlrun.api.extraEnvKeyValue.MLRUN_REDIS__URL=<redis-address>\nUsage#\nYour applications are now available in your local browser:\njupyter-notebook - http://:30040\nnuclio - http://:30050\nmlrun UI - http://:30060\nmlrun API (external) - http://:30070\nminio API - http://:30080\nminio UI - http://:30090\npipeline UI - http://:30100\ngrafana UI - http://:30110\nCheck state\nYou can check current state of installation via command kubectl -n mlrun get pods, where the main information\nis in columns Ready and State. If all images have already been pulled locally, typically it will take\na minute for all services to start.\nNote:\nYou can change the ports by providing values to the helm install command.\nYou can add and configure a k8s ingress-controller for better security and control over external access.\nStart Working#\nOpen the Jupyter notebook on jupyter-notebook UI and run the code in the\nexamples/mlrun_basics.ipynb notebook.\nImportant\nMake sure to save your changes in the data folder within the Jupyter Lab. The root folder and any other folder do not retain the changes when you restart the Jupyter Lab.\nConfiguring the remote environment#\nYou can use your code on a local machine while running your functions on a remote cluster. Refer to Set up your client environment for more information.\nAdvanced chart configuration#\nConfigurable values are documented in the values.yaml, and the values.yaml of all sub charts. Override those in the normal methods.\nUninstalling the Chart#\nhelm --namespace mlrun uninstall mlrun-ce\nNote on terminating pods and hanging resources#\nThis chart generates several persistent volume claims that provide persistency (via PVC) out of the box.\nUpon uninstallation, any hanging / terminating pods will hold the PVCs and PVs respectively, as those prevent their safe removal.\nSince pods that are stuck in terminating state seem to be a never-ending plague in k8s, note this,\nand remember to clean the remaining PVs and PVCs.\nHanding stuck-at-terminating pods:#\nkubectl --namespace mlrun delete pod --force --grace-period=0 <pod-name>\nReclaim dangling persistency resources:#\nWARNING\nThis will result in data loss!\n# To list PVCs\n$ kubectl --namespace mlrun get pvc\n...\n# To remove a PVC\n$ kubectl --namespace mlrun delete pvc <pvc-name>\n...\n# To list PVs\n$ kubectl --namespace mlrun get pv\n...\n# To remove a PVC\n$ kubectl --namespace mlrun delete pvc <pv-name>\n...\nUpgrading the chart#\nIn order to upgrade to the latest version of the chart, first make sure you have the latest helm repo\nhelm repo update\nThen upgrade the chart:\nhelm upgrade --install --reuse-values mlrun-ce mlrun-ce/mlrun-ce"}
{"text": "Install MLRun locally using Docker#\nYou can install and use MLRun and Nuclio locally on your computer. This does not include all the services and elastic\nscaling capabilities, which you can get with the Kubernetes based deployment, but it is much simpler to start with.\nNote\nUsing Docker is limited to local, Nuclio, serving runtimes, and local pipelines.\nPrerequisites#\nMemory: 8GB\nStorage: 7GB\nOverview#\nUse docker compose to install MLRun. It deploys the MLRun service,\nMLRun UI, Nuclio serverless engine, and optionally the Jupyter server.\nThe MLRun service, MLRun UI, Nuclio, and Jupyter, do not have default resources. This means that they are set with the default\ncluster/namespace resources limits. These can be modified.\nThere are two installation options:\nUse MLRun with your own client (PyCharm, VSCode, Jupyter)\nUse MLRun with MLRun Jupyter image (pre loaded with examples/demos)\nIn both cases you need to set the SHARED_DIR environment variable to point to a host path for storing MLRun artifacts and DB,\nfor example export SHARED_DIR=~/mlrun-data (or use set SHARED_DIR=c:\\mlrun-data in windows). Make sure the directory exists.\nYou also need to set the HOST_IP variable with your computer IP address (required for Nuclio dashboard).\nYou can select a specific MLRun version with the TAG variable and Nuclio version with the NUCLIO_TAG variable.\nAdd the -d flag to docker-compose for running in detached mode (in the background).\nNote\nSupport for running as a non-root user was added in 1.0.5, hence the underlying exposed port was changed.\nIf you want to use previous mlrun versions, modify the mlrun-ui port from 8090 back to 80.\nIf you are running more than one instance of MLRun, change the exposed port.\nWatch the installation:\nUse MLRun with your own client#\nThe following commands install MLRun and Nuclio for work with your own IDE or notebook.\n[Download here] the compose.yaml file, save it to the working dir and type:\nshow the compose.yaml file\nservices:\ninit_nuclio:\nimage: alpine:3.16\ncommand:\n- \"/bin/sh\"\n- \"-c\"\n- |\nmkdir -p /etc/nuclio/config/platform; \\\ncat << EOF | tee /etc/nuclio/config/platform/platform.yaml\nruntime:\ncommon:\nenv:\nMLRUN_DBPATH: http://${HOST_IP:?err}:8080\nlocal:\ndefaultFunctionContainerNetworkName: mlrun\ndefaultFunctionRestartPolicy:\nname: always\nmaxRetryCount: 0\ndefaultFunctionVolumes:\n- volume:\nname: mlrun-stuff\nhostPath:\npath: ${SHARED_DIR:?err}\nvolumeMount:\nname: mlrun-stuff\nmountPath: /home/jovyan/data/\nlogger:\nsinks:\nmyStdoutLoggerSink:\nkind: stdout\nsystem:\n- level: debug\nsink: myStdoutLoggerSink\nfunctions:\n- level: debug\nsink: myStdoutLoggerSink\nEOF\nvolumes:\n- nuclio-platform-config:/etc/nuclio/config\nmlrun-api:\nimage: \"mlrun/mlrun-api:${TAG:-1.2.0}\"\nports:\n- \"8080:8080\"\nenvironment:\nMLRUN_ARTIFACT_PATH: \"${SHARED_DIR}/{{project}}\"\n# using local storage, meaning files / artifacts are stored locally, so we want to allow access to them\nMLRUN_HTTPDB__REAL_PATH: /data\nMLRUN_HTTPDB__DATA_VOLUME: \"${SHARED_DIR}\"\nMLRUN_LOG_LEVEL: DEBUG\nMLRUN_NUCLIO_DASHBOARD_URL: http://nuclio:8070\nMLRUN_HTTPDB__DSN: \"sqlite:////data/mlrun.db?check_same_thread=false\"\nMLRUN_UI__URL: http://localhost:8060\n# not running on k8s meaning no need to store secrets\nMLRUN_SECRET_STORES__KUBERNETES__AUTO_ADD_PROJECT_SECRETS: \"false\"\n# let mlrun control nuclio resources\nMLRUN_HTTPDB__PROJECTS__FOLLOWERS: \"nuclio\"\nvolumes:\n- \"${SHARED_DIR:?err}:/data\"\nnetworks:\n- mlrun\nmlrun-ui:\nimage: \"mlrun/mlrun-ui:${TAG:-1.2.0}\"\nports:\n- \"8060:8090\"\nenvironment:\nMLRUN_API_PROXY_URL: http://mlrun-api:8080\nMLRUN_NUCLIO_MODE: enable\nMLRUN_NUCLIO_API_URL: http://nuclio:8070\nMLRUN_NUCLIO_UI_URL: http://localhost:8070\nnetworks:\n- mlrun\nnuclio:\nimage: \"quay.io/nuclio/dashboard:${NUCLIO_TAG:-stable-amd64}\"\nports:\n- \"8070:8070\"\nenvironment:\nNUCLIO_DASHBOARD_EXTERNAL_IP_ADDRESSES: \"${HOST_IP:?err}\"\nvolumes:\n- /var/run/docker.sock:/var/run/docker.sock\n- nuclio-platform-config:/etc/nuclio/config\ndepends_on:\n- init_nuclio\nnetworks:\n- mlrun\nvolumes:\nnuclio-platform-config: {}\nnetworks:\nmlrun:\nname: mlrun\nLinux/Mac\nexport HOST_IP=<your host IP address>\nexport SHARED_DIR=~/mlrun-data\nmkdir $SHARED_DIR -p\ndocker-compose -f compose.yaml up\nYour HOST_IP address can be found using the ip addr or ifconfig commands. It is recommended to select an address that does not change dynamically (for example the IP of the bridge interface).\nWindows (cmd)\nset HOST_IP=<your host IP address>\nset SHARED_DIR=c:\\mlrun-data\nmkdir %SHARED_DIR%\ndocker-compose -f compose.yaml up\nYour HOST_IP address can be found using the ipconfig shell command, it is recommended to select an address that does not change dynamically (for example the IP of the vEthernet interface).\nPowershell\n$Env:HOST_IP=<your host IP address>\n$Env:SHARED_DIR=\"~/mlrun-data\"\nmkdir $Env:SHARED_DIR\ndocker-compose -f compose.yaml up\nYour HOST_IP address can be found using the Get-NetIPConfiguration cmdlet, it is recommended to select an address that does not change dynamically (for example the IP of the vEthernet interface).\nThis creates 3 services:\nMLRun API (in http://localhost:8080)\nMLRun UI (in http://localhost:8060)\nNuclio Dashboard/controller (in http://localhost:8070)\nAfter installing MLRun service, set your client environment to work with the service, by setting the MLRun path env variable to\nMLRUN_DBPATH=http://localhost:8080 or using .env files (see setting client environment).\nUse MLRun with MLRun Jupyter image#\nFor the quickest experience with MLRun you can deploy MLRun with a pre-integrated Jupyter server loaded with various ready-to-use MLRun examples.\n[Download here] the compose.with-jupyter.yaml file, save it to the working dir and type:\nservices:\ninit_nuclio:\nimage: alpine:3.16\ncommand:\n- \"/bin/sh\"\n- \"-c\"\n- |\nmkdir -p /etc/nuclio/config/platform; \\\ncat << EOF | tee /etc/nuclio/config/platform/platform.yaml\nruntime:\ncommon:\nenv:\nMLRUN_DBPATH: http://${HOST_IP:?err}:8080\nlocal:\ndefaultFunctionContainerNetworkName: mlrun\ndefaultFunctionRestartPolicy:\nname: always\nmaxRetryCount: 0\ndefaultFunctionVolumes:\n- volume:\nname: mlrun-stuff\nhostPath:\npath: ${SHARED_DIR:?err}\nvolumeMount:\nname: mlrun-stuff\nmountPath: /home/jovyan/data/\nlogger:\nsinks:\nmyStdoutLoggerSink:\nkind: stdout\nsystem:\n- level: debug\nsink: myStdoutLoggerSink\nfunctions:\n- level: debug\nsink: myStdoutLoggerSink\nEOF\nvolumes:\n- nuclio-platform-config:/etc/nuclio/config\njupyter:\nimage: \"mlrun/jupyter:${TAG:-1.2.0}\"\nports:\n- \"8080:8080\"\n- \"8888:8888\"\nenvironment:\nMLRUN_ARTIFACT_PATH: \"/home/jovyan/data/{{project}}\"\nMLRUN_LOG_LEVEL: DEBUG\nMLRUN_NUCLIO_DASHBOARD_URL: http://nuclio:8070\nMLRUN_HTTPDB__DSN: \"sqlite:////home/jovyan/data/mlrun.db?check_same_thread=false\"\nMLRUN_UI__URL: http://localhost:8060\n# using local storage, meaning files / artifacts are stored locally, so we want to allow access to them\nMLRUN_HTTPDB__REAL_PATH: \"/home/jovyan/data\"\n# not running on k8s meaning no need to store secrets\nMLRUN_SECRET_STORES__KUBERNETES__AUTO_ADD_PROJECT_SECRETS: \"false\"\n# let mlrun control nuclio resources\nMLRUN_HTTPDB__PROJECTS__FOLLOWERS: \"nuclio\"\nvolumes:\n- \"${SHARED_DIR:?err}:/home/jovyan/data\"\nnetworks:\n- mlrun\nmlrun-ui:\nimage: \"mlrun/mlrun-ui:${TAG:-1.2.0}\"\nports:\n- \"8060:8090\"\nenvironment:\nMLRUN_API_PROXY_URL: http://jupyter:8080\nMLRUN_NUCLIO_MODE: enable\nMLRUN_NUCLIO_API_URL: http://nuclio:8070\nMLRUN_NUCLIO_UI_URL: http://localhost:8070\nnetworks:\n- mlrun\nnuclio:\nimage: \"quay.io/nuclio/dashboard:${NUCLIO_TAG:-stable-amd64}\"\nports:\n- \"8070:8070\"\nenvironment:\nNUCLIO_DASHBOARD_EXTERNAL_IP_ADDRESSES: \"${HOST_IP:?err}\"\nvolumes:\n- /var/run/docker.sock:/var/run/docker.sock\n- nuclio-platform-config:/etc/nuclio/config\ndepends_on:\n- init_nuclio\nnetworks:\n- mlrun\nvolumes:\nnuclio-platform-config: {}\nnetworks:\nmlrun:\nname: mlrun\nLinux/Mac\nexport HOST_IP=<your host IP address>\nexport SHARED_DIR=~/mlrun-data\nmkdir -p $SHARED_DIR\ndocker-compose -f compose.with-jupyter.yaml up\nYour HOST_IP address can be found using the ip addr or ifconfig commands. It is recommended to select an address that does not change dynamically (for example the IP of the bridge interface).\nWindows (cmd)\nset HOST_IP=<your host IP address>\nset SHARED_DIR=c:\\mlrun-data\nmkdir %SHARED_DIR%\ndocker-compose -f compose.with-jupyter.yaml up\nYour HOST_IP address can be found using the ipconfig shell command, it is recommended to select an address that does not change dynamically (for example the IP of the vEthernet interface).\nPowershell\n$Env:HOST_IP=<your host IP address>\n$Env:SHARED_DIR=\"~/mlrun-data\"\nmkdir $Env:SHARED_DIR\ndocker-compose -f compose.with-jupyter.yaml up\nYour HOST_IP address can be found using the Get-NetIPConfiguration cmdlet, it is recommended to select an address that does not change dynamically (for example the IP of the vEthernet interface).\nThis creates 4 services:\nJupyter lab (in http://localhost:8888)\nMLRun API (in http://localhost:8080), running on the Jupyter container\nMLRun UI (in http://localhost:8060)\nNuclio Dashboard/controller (in http://localhost:8070)\nAfter the installation, access the Jupyter server (in http://localhost:8888) and run through the quick-start tutorial and demos.\nYou can see the projects, tasks, and artifacts in MLRun UI (in http://localhost:8060).\nThe Jupyter environment is pre-configured to work with the local MLRun and Nuclio services.\nYou can switch to a remote or managed MLRun cluster by editing the mlrun.env file in the Jupyter files tree.\nThe artifacts and DB are stored under /home/jovyan/data (/data in Jupyter tree)."}
{"text": "Set up your client environment #\nYou can write your code on a local machine while running your functions on a remote cluster. This tutorial explains how to set this up.\nIn this section\nPrerequisites\nConfigure remote environment\nSet environment variables\nLoad the configuration and credential environmental variables from file\nLoad the configuration and credential environmental variables from the command line\nIDE configuration\nRemote environment from PyCharm\nRemote environment from VSCode\nCreate environment file\nCreate Python debug configuration\nSet environment file in debug configuration\nPrerequisites#\nBefore you begin, ensure that the following prerequisites are met:\nApplications:\nPython 3.7\nRecommended pip 22.x+\nInstall MLRun locally.\nYou need to install MLRun locally. Make sure the that the MLRun version you install is the same as the MLRun\nservice version. Install a specific version using the following command; replace the <version>  placeholder\nwith the MLRun version number (e.g., 1.0.0):\npip install mlrun==<version>\nThere are a two pip install options:\nTo install the requirements in the requirements.txt, run:\npip install mlrun\nIf you expect to connect to, or work with, cloud providers (Azure/Google Cloud/S3), you can install additional packages.\nThis is not part of the regular requirements since not all users work with those platforms. Using this option reduces the\ndependencies and the size of the installation. The additional packages include:\npip install mlrun[s3] # Install requirements for S3\npip install mlrun[azure-blob-storage] # install requirements for Azure blob storage\npip install mlrun[google-cloud-storage] # install requirements for Google cloud storage\nSee the full list here.\nTo install all extras, run:\npip install mlrun[complete]\nAlternatively, if you already installed a previous version of MLRun, upgrade it by running:\npip install -U mlrun==<version>\nEnsure that you have remote access to your MLRun service (i.e., to the service URL on the remote Kubernetes cluster).\nConfigure remote environment#\nSet environment variables#\nSet environment variables to define your MLRun configuration. As a minimum requirement:\nSet MLRUN_DBPATH to the URL of the remote MLRun database/API service:\nMLRUN_DBPATH=<URL endpoint of the MLRun APIs service endpoint; e.g., \"https://mlrun-api.default-tenant.app.mycluster.iguazio.com\">\nIf the remote service is on an instance of the Iguazio MLOps Platform (“the platform”), set the following environment variables as well:\nV3IO_USERNAME=<username of a platform user with access to the MLRun service>\nV3IO_ACCESS_KEY=<platform access key>\nYou can get the platform access key from the platform dashboard: select the user-profile picture or icon from the top right corner of\nany page, and select Access Keys from the menu. In the Access Keys window, either copy an existing access key or create a new\nkey and copy it. Alternatively, you can get the access key by checking the value of the V3IO_ACCESS_KEY environment variable in a web-shell or Jupyter Notebook service.\nYou can also set the environment using MLRun SDK, for example:\n# Use local service\nmlrun.set_environment(\"http://localhost:8080\", artifact_path=\"./\")\n# Use remote service\nmlrun.set_environment(\"<remote-service-url>\", access_key=\"xyz\", username=\"joe\")\nLoad the configuration and credential environmental variables from file#\nYou can load the env via config file when working from remote (e.g. via PyCharm).\nExample env file:\n# this is an env file\nV3IO_USERNAME=admin\nV3IO_ACCESS_KEY=MYKEY123\nMLRUN_DBPATH=https://mlrun-api.default-tenant.app.xxx.iguazio-cd1.com\nAWS_ACCESS_KEY_ID=XXXX\nAWS_SECRET_ACCESS_KEY=YYYY\nUsage:\nset_env_from_file() for reading .env files, setting the OS environment and reloading MLRun config\nproject.set_secrets() reads dict or secrets env file and stores it in the project secrets\n(note that MLRUN_DBPATH and V3IO_xxx vars are not written to the project secrets)\nfunction.set_envs() set the pod environment variables from key/value dict or .env file\nNote\nThe V3IO API is determined automatically. If you want to connect to a different V3IO service, set the service in the variable,br. V3IO_API=<API endpoint of the webapi service endpoint; e.g., \"https://default-tenant.app.mycluster.iguazio.com:8444\">\n# set the env vars from a file and also return the results as a dict (e.g. for using in a function)\nenv_dict = mlrun.set_env_from_file(env_path, return_dict=True)\n# read env vars from dict or file and set as project secrets (plus set the local env)\nproject.set_secrets({\"SECRET1\": \"value\"})\nproject.set_secrets(file_path=env_file)\n# copy env from file into a function spec\nfunction.set_envs(file_path=env_file)\nLoad the configuration and credential environmental variables from the command line#\nCreate an env file similar to the example, with lines in the form KEY=VALUE, and comment lines starting with “#”.\nUse --env-file <env file path> in mlrun run/build/deploy/project CLI commands to load the config and credential env vars from file.\nSet the MLRUN_ENV_FILE=<env file path> env var to point to a default env file (which will be loaded on import).\nIf the MLRUN_DBPATH points to a remote iguazio cluster and the V3IO_API and/or V3IO_FRAMESD vars are not set, they will be inferred from the DBPATH.\nAdd the default env file template in the Jupyter container ~/env (to allow quick setup of remote demos).\nIDE configuration#\nUse these procedures to access MLRun remotely from your IDE (PyCharm or VSCode).\nRemote environment from PyCharm#\nYou can use PyCharm with MLRun remote by changing the environment variables configuration.\nFrom the main menu, choose Run | Edit Configurations.\nTo set-up default values for all Python configurations, on the left-hand pane of the run/debug configuration dialog, expand the Templates node and select the Python node. The corresponding configuration template appears in the right-hand pane. Alternatively, you can edit a specific file configuration by choosing the corresponding file on the left-hand pane. Choose the Environment Variables edit box and expand it to edit the environment variables.\nAdd the environment variable and value of MLRUN_DBPATH.\nIf the remote service is on an instance of the Iguazio MLOps Platform, also set the environment variables and values of V3IO_USERNAME, and V3IO_ACCESS_KEY.\nRemote environment from VSCode#\nCreate environment file#\nCreate an environment file called mlrun.env in your workspace folder. Copy-paste the configuration below:\n# Remote URL to mlrun service\nMLRUN_DBPATH=<API endpoint of the MLRun APIs service endpoint; e.g., \"https://mlrun-api.default-tenant.app.mycluster.iguazio.com\">\n# Iguazio platform username\nV3IO_USERNAME=<username of a platform user with access to the MLRun service>\n# Iguazio V3IO data layer credentials (copy from your user settings)\nV3IO_ACCESS_KEY=<platform access key>\nNote\nMake sure that you add .env to your .gitignore file. The environment file contains sensitive information that you should not store in your source control.\nCreate Python debug configuration#\nCreate a debug configuration in VSCode. Configurations are defined in a launch.json file that’s stored in a .vscode folder in your workspace.\nTo initialize debug configurations, first select the Run view in the sidebar:\nIf you don’t yet have any configurations defined, you’ll see a button to Run and Debug, as well as a link to create a configuration (launch.json) file:\nTo generate a launch.json file with Python configurations:\nClick the create a launch.json file link (circled in the image above) or use the Run > Open configurations menu command.\nA configuration menu opens from the Command Palette. Select the type of debug configuration you want for the opened file. For now, in the Select a debug configuration menu that appears, select Python File.\nNote\nStarting a debugging session through the Debug Panel, F5 or Run > Start Debugging, when no configuration exists also brings up the debug configuration menu, but does not create a launch.json file.\nThe Python extension then creates and opens a launch.json file that contains a pre-defined configuration based on what you previously selected, in this case Python File. You can modify configurations (to add arguments, for example), and also add custom configurations.\nSet environment file in debug configuration#\nAdd an envFile setting to your configuration with the value of ${workspaceFolder}/mlrun.env\nIf you created a new configuration in the previous step, your launch.json would look as follows:\n{\n// Use IntelliSense to learn about possible attributes.\n// Hover to view descriptions of existing attributes.\n// For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n\"version\": \"0.2.0\",\n\"configurations\": [\n{\n\"name\": \"Python: Current File\",\n\"type\": \"python\",\n\"request\": \"launch\",\n\"program\": \"${file}\",\n\"console\": \"integratedTerminal\",\n\"envFile\": \"${workspaceFolder}/mlrun.env\"\n}\n]\n}"}
{"text": "MLOps development workflow #\nML applications require you to implement the following stages in a scalable and reproducible way:\nIngest and process data\nDevelop and train models\nDeploy models and applications\nMonitor and alert\nMLRun automates the MLOps work. It simplifies & accelerates the time to production\nIngest and process data#\nThere is no ML without data. Before everything else, ML teams need access to historical and/or online data from multiple sources, and they\nmust catalog and organize the data in a way that allows for simple and fast analysis (for example, by storing data in columnar data\nstructures, such as Parquet).\nIn most cases, the raw data cannot be used as-is for machine learning algorithms for various reasons such as:\nThe data is low quality (missing fields, null values, etc.) and requires cleaning and imputing.\nThe data needs to be converted to numerical or categorical values which can be processed by algorithms.\nThe data is unstructured in text, json, image, or audio formats, and needs to be converted to tabular or vector formats.\nThe data needs to be grouped or aggregated to make it meaningful.\nThe data is encoded or requires joins with reference information.\nThe ML process starts with manual exploratory data analysis and feature engineering on small data extractions. In order to bring accurate models into production, ML teams must work on larger datasets and automate the process of collecting and preparing the data.\nFurthermore, batch collection and preparation methodologies such as ETL, SQL queries, and batch analytics don’t work well for operational or\nreal-time pipelines. As a result, ML teams often build separate data pipelines which use stream processing, NoSQL, and containerized micro-\nservices. 80% of data today is unstructured, so an essential part of building operational data pipelines is to convert unstructured textual,\naudio, and visual data into machine learning- or deep learning-friendly data organization.\nMLOps solutions should incorporate a feature store that defines the data collection and transformations\njust once for both batch and real-time scenarios, processes features automatically without manual involvement, and serves the features from\na shared catalog to training, serving, and data governance applications. Feature stores must also extend beyond traditional analytics and\nenable advanced transformations on unstructured data and complex layouts.\nDevelop and train models#\nWhether it’s deep learning or machine learning, MLRun allows you to train your models at scale and capture all the relevant metadata for experiments tracking and lineage.\nWith MLOps, ML teams build machine learning pipelines that automatically collect and prepare data, select optimal features, run training\nusing different parameter sets or algorithms, evaluate models, and run various model and system tests. All the executions, along with their\ndata, metadata, code and results must be versioned and logged, providing quick results visualization, to compare them with past results and\nunderstand which data was used to produce each model.\nPipelines can be more complex—for example, when ML teams need to develop a combination of models, or use Deep Learning or NLP.\nML pipelines can be triggered manually, or preferably triggered automatically when:\nThe code, packages or parameters change\nThe input data or feature engineering logic changes\nConcept drift is detected, and the model needs to be re-trained with fresh data\nML pipelines:\nAre built using micro-services (containers or serverless functions), usually over Kubernetes.\nHave all their inputs (code, package dependencies, data, parameters) and the outputs (logs, metrics, data/features, artifacts, models) tracked for every step in the pipeline, in order to reproduce and/or explain the experiment results.\nUse versioning for all the data and artifacts used throughout the pipeline.\nStore code and configuration in versioned Git repositories.\nUse Continuous Integration (CI) techniques to automate the pipeline initiation, test automation, and for the review and approval process.\nPipelines should be executed over scalable services or functions, which can span elastically over multiple servers or containers. This way,\njobs complete faster, and computation resources are freed up once they complete, saving significant costs.\nThe resulting models are stored in a versioned model repository along with metadata, performance metrics, required parameters, statistical\ninformation, etc. Models can be loaded later into batch or real-time serving micro-services or functions.\nDeploy models and applications#\nWith MLRun, in addition to a batch inference, you can deploy a robust and scalable real-time pipeline for more complex and online scenarios.\nMLRun uses Nuclio, an open source serverless framework for creating real-time pipelines for model deployment.\nOnce an ML model has been built, it needs to be integrated with real-world data and the business application or front-end services. The\nentire application, or parts thereof, need to be deployed without disrupting the service. Deployment can be extremely challenging if the ML\ncomponents aren’t treated as an integral part of the application or production pipeline.\nProduction pipelines usually consist of:\nReal-time data collection, validation, and feature engineering logic\nOne or more model serving services\nAPI services and/or application integration logic\nData and model monitoring services\nResource monitoring and alerting services\nEvent, telemetry, and data/features logging services\nThe different services are interdependent. For example, if the inputs to a model change, the feature engineering logic must be upgraded\nalong with the model serving and model monitoring services. These dependencies require online production pipelines (graphs) to reflect these\nchanges.\nProduction pipelines can be more complex when using unstructured data, deep learning, NLP or model ensembles, so having flexible mechanisms\nto build and wire up the pipeline graphs is critical.\nProduction pipelines are usually interconnected with fast streaming or messaging protocols, so they should be elastic to address traffic and\ndemand fluctuations, and they should allow non-disruptive upgrades to one or more elements of the pipeline. These requirements are best\naddressed with fast serverless technologies.\nProduction pipeline development and deployment flow:\nDevelop production components:\nAPI services and application integration logic\nFeature collection, validation, and transformation\nModel serving graphs\nTest online pipelines with simulated data\nDeploy online pipelines to production\nMonitor models and data and detect drift\nRetrain models and re-engineer data when needed\nUpgrade pipeline components (non-disruptively) when needed\nMonitor and alert#\nOnce the model is deployed, use MLRun to track the operational statistics as well as identify drift.\nWhen drift is identified, MLRun can trigger the training pipeline to train a new model.\nAI services and applications are becoming an essential part of any business. This trend brings with it liabilities, which drive further\ncomplexity. ML teams need to add data, code and experiment tracking, monitor data to detect quality problems, and monitor models to detect concept drift and improve model accuracy through the use of AutoML techniques and ensembles, and so on.\nNothing lasts forever, not even carefully constructed models that have been trained using mountains of well-labeled data. ML teams need to\nreact quickly to adapt to constantly changing patterns in real-world data. Monitoring machine learning models is a core component of MLOps\nto keep deployed models current and predicting with the utmost accuracy, and to ensure they deliver value long-term."}
{"text": "Monitor and alert#\nNote\nMonitoring required at the moment Iguazio’s streaming technology. Open-source integration with Kafka is under development.\nNote\nThis is currently a beta feature.\nThe MLRun’s model monitoring service includes built-in model monitoring and reporting capability. With monitoring you get\nout-of-the-box analysis of:\nModel performance: machine learning models train on data. It is important you know how well they perform in production.\nWhen you analyze the model performance, it is important you monitor not just the overall model performance, but also the\nfeature-level performance. This gives you better insights for the reasons behind a particular result\nData drift: the change in model input data that potentially leads to model performance degradation. There are various\nstatistical metrics and drift metrics that you can use in order to identify data drift.\nConcept drift: applies to the target. Sometimes the statistical properties of the target variable, which the model is\ntrying to predict, change over time in unforeseen ways.\nOperational performance: applies to the overall health of the system. This applies to data (e.g., whether all the\nexpected data arrives to the model) as well as the model (e.g., response time, and throughput).\nYou have the option to set up notifications on various channels once an issue is detection. For example, you can set-up notification\nto your IT via email and slack when operational performance metrics pass a threshold. You can also set-up automated actions, for example,\ncall a CI/CD pipeline when data drift is detected and allow a data scientist to review the model with the revised data.\nRefer to the model monitoring & drift detection tutorial for an end-to-end example.\nIn this section\nModel monitoring overview\nEnable model monitoring"}
{"text": "Enable model monitoring#\nNote\nThis is currently a beta feature.\nTo see tracking results, model monitoring needs to be enabled in each model.\nTo enable model monitoring, include serving_fn.set_tracking() in the model server.\nTo utilize drift measurement, supply the train set in the training step.\nIn this section\nModel monitoring demo\nDeploy model servers\nSimulating requests\nModel monitoring demo#\nUse the following code blocks to test and explore model monitoring.\n# Set project name\nproject_name = \"demo-project\"\nDeploy model servers#\nUse the following code to deploy a model server in the Iguazio instance.\nimport os\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom mlrun import import_function, get_dataitem, get_or_create_project\nfrom mlrun.platforms import auto_mount\nproject = get_or_create_project(project_name, context=\"./\")\nproject.set_model_monitoring_credentials(os.environ.get(\"V3IO_ACCESS_KEY\"))\n# Download the pre-trained Iris model\nget_dataitem(\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\").download(\"model.pkl\")\niris = load_iris()\ntrain_set = pd.DataFrame(iris['data'],\ncolumns=['sepal_length_cm', 'sepal_width_cm',\n'petal_length_cm', 'petal_width_cm'])\n# Import the serving function from the Function Hub\nserving_fn = import_function('hub://v2_model_server', project=project_name).apply(auto_mount())\nmodel_name = \"RandomForestClassifier\"\n# Log the model through the projects API so that it is available through the feature store API\nproject.log_model(model_name, model_file=\"model.pkl\", training_set=train_set)\n# Add the model to the serving function's routing spec\nserving_fn.add_model(model_name, model_path=f\"store://models/{project_name}/{model_name}:latest\")\n# Enable model monitoring\nserving_fn.set_tracking()\n# Deploy the function\nserving_fn.deploy()\nSimulating requests#\nUse the following code to simulate production data.\nimport json\nfrom time import sleep\nfrom random import choice, uniform\niris_data = iris['data'].tolist()\nwhile True:\ndata_point = choice(iris_data)\nserving_fn.invoke(f'v2/models/{model_name}/infer', json.dumps({'inputs': [data_point]}))\nsleep(uniform(0.2, 1.7))"}
{"text": "Model monitoring overview#\nNote\nThis is currently a beta feature.\nIn this section\nArchitecture\nModel monitoring using the Iguazio platform interface\nModel monitoring using Grafana dashboards\nArchitecture#\nThe model monitoring process flow starts with collecting operational data. The operational data are converted to vectors, which are posted to the Model Server.\nThe model server is then wrapped around a machine learning model that uses a function to calculate predictions based on the available vectors.\nNext, the model server creates a log for the input and output of the vectors, and the entries are written to the production data stream (a v3io stream).\nWhile the model server is processing the vectors, a Nuclio operation monitors the log of the data stream and is triggered when a new log entry is detected.\nThe Nuclio function examines the log entry, processes it into statistics which are then written to the statistics databases (parquet file, time series database and key value database).\nThe parquet files are written as a feature set under the model monitoring project. The parquet files can be read either using pandas.read_parquet or feature_set.get_offline_features, like any other feature set.\nIn parallel, a scheduled MLRun job runs reading the parquet files, performing drift analysis. The drift analysis data is stored so that the user can retrieve it in the Iguazio UI or in a Grafana dashboard.\nDrift analysis#\nThe model monitoring feature provides drift analysis monitoring.\nModel drift in machine learning is a situation where the statistical properties of the target variable (what the model is trying to predict) change over time.\nIn other words, the production data has changed significantly over the course of time and no longer matches the input data used to train the model.\nSo, for this new data, accuracy of the model predictions is low. Drift analysis statistics are computed once an hour.\nFor more information see Concept Drift.\nCommon terminology#\nThe following terms are used in all the model monitoring pages:\nTotal Variation Distance (TVD) — The statistical difference between the actual predictions and the model’s trained predictions.\nHellinger Distance — A type of f-divergence that quantifies the similarity between the actual predictions, and the model’s trained predictions.\nKullback–Leibler Divergence (KLD) — The measure of how the probability distribution of actual predictions is different from the second model’s trained reference probability distribution.\nModel Endpoint — A combination of a deployed Nuclio function and the models themselves. One function can run multiple endpoints; however, statistics are saved per endpoint.\nModel monitoring using the Iguazio platform interface#\nIguazio’s model monitoring data is available for viewing through the regular platform interface.\nThe platform provides four information pages with model monitoring data.\nModel endpoint summary list\nModel endpoint overview\nModel drift analysis\nModel features analysis\nSelect a project from the project tiles screen.\nFrom the project dashboard, press the Models tile to view the models currently deployed .\nPress Model Endpoints from the menu to display a list of monitored endpoints.\nIf the Model Monitoring feature is not enabled, the endpoints list is empty.\nModel endpoint summary list#\nThe Model Endpoints summary list provides a quick view of the model monitoring data.\nThe summary page contains the following fields:\nName — the name of the model endpoint\nVersion — user configured version taken from model deployment\nClass — the implementation class that is used by the endpoint\nModel — user defined name for the model\nLabels — user configurable tags that are searchable\nUptime — first request for production data\n**Last Prediction **— most recent request for production data\n**Error Count **— includes prediction process errors such as operational issues (For example, a function in a failed state), as well as data processing errors\n(For example, invalid timestamps, request ids, type mismatches etc.)\nDrift — indication of drift status (no drift (green), possible drift (yellow), drift detected (red))\nAccuracy — a numeric value representing the accuracy of model predictions (N/A)\nNote\nModel Accuracy is currently under development.\nModel endpoint overview#\nThe Model Endpoints overview pane displays general information about the selected model.\nThe Overview page contains the following fields:\nUUID — the ID of the deployed model\nModel Class — the implementation class that is used by the endpoint\nModel Artifact — reference to the model’s file location\nFunction URI — the MLRun function to access the model\nLast Prediction — most recent request for production data\nError Count — includes prediction process errors such as operational issues (For example, a function in a failed state), as well as data processing errors\n(For example, invalid timestamps, request ids, type mismatches etc.)\nAccuracy — a numeric value representing the accuracy of model predictions (N/A)\nStream path — the input and output stream of the selected model\nUse the ellipsis to view the YAML resource file for details about the monitored resource.\nModel drift analysis#\nThe Drift Analysis pane provides performance statistics for the currently selected model.\nEach of the following fields has both sum and mean numbers displayed. For definitions of the terms see Common Terminology.\nTVD\nHellinger\nKLD\nUse the ellipsis to view the YAML resource file for details about the monitored resource.\nModel features analysis#\nThe Features Analysis pane provides details of the drift analysis in a table format with each feature in the selected model on its own line.\nEach field has a pair of columns. The Expected column displays the results from the model training phase, and the Actual column\ndisplays the results from the live production data. The following fields are available:\nMean\nSTD (Standard deviation)\nMin\nMax\nTVD\nHellinger\nKLD\nHistograms—the approximate representation of the distribution of the data. Hover over the bars in the graph for  the details.\nUse the ellipsis to view the YAML resource file for details about the monitored resource.\nModel monitoring using Grafana dashboards#\nYou can deploy a Grafana service in your Iguazio instance and use Grafana Dashboards to view model monitoring details.\nThere are three dashboards available:\nOverview Dashboard\nDetails Dashboard\nPerformance Dashboard\nModel endpoints overview dashboard#\nThe Overview dashboard displays the model endpoint IDs of a specific project. Only deployed models with Model Monitoring enabled are displayed.\nEndpoint IDs are URIs used to provide access to performance data and drift detection statistics of a deployed model.\nThe Overview pane provides details about the performance of all the deployed and monitored models within a project. You can change projects by choosing a new project from the\nProject dropdown. The Overview dashboard displays the number of endpoints in the project, the average predictions per second (using a 5-minute rolling average),\nthe average latency (using a 1-hour rolling average), and the total error count in the project.\nAdditional details include:\nEndpoint ID — the ID of the deployed model. Use this link to drill down to the model performance and details panes.\nFunction — the MLRun function to access the model\nModel — user defined name for the model\nModel Class — the implementation class that is used by the endpoint\nFirst Request — first request for production data\nLast Request — most recent request for production data\nError Count — includes prediction process errors such as operational issues (for example, a function in a failed state), as well as data processing errors\n(for example, invalid timestamps, request ids, type mismatches etc.)\nAccuracy — a numeric value representing the accuracy of model predictions (N/A)\nDrift Status — no drift (green), possible drift (yellow), drift detected (red)\nAt the bottom of the dashboard are heat maps for the Predictions per second, Average Latency and Errors. The heat maps display data based on 15 minute intervals.\nSee How to Read a Heat Map for more details.\nClick an endpoint ID to drill down the performance details of that model.\nHow to read a heat map#\nHeat maps are used to analyze trends and to instantly transform and enhance data through visualizations. This helps to quickly identify areas of interest,\nand empower users to explore the data in order to pinpoint where there may be potential issues. A heat map uses a matrix layout with colour and shading to show the relationship between\ntwo categories of values (x and y axes), so the darker the cell, the higher the value. The values presented along each axis correspond to a cell which is color-coded to represent the relationship between\nthe two categories. The Predictions per second heatmap shows the relationship between time, and the predictions per second, and the Average Latency per hour shows the relationship between\ntime and the latency.\nTo properly read the heap maps, follow the hierarchy of shades from the darkest (the highest values) to the lightest shades (the lowest values).\nNote\nThe exact quantitative values represented by the colors may be difficult to determine. Use the Performance Dashboard to see detailed results.\nModel endpoint details dashboard#\nThe model endpoint details dashboard displays the real time performance data of the selected model in detail.\nModel performance data provided is rich and is used to fine tune or diagnose potential performance issues that may affect business goals.\nThe data in this dashboard changes based on the selection of the project and model.\nThis dashboard has three panes:\nProject and model summary\nAnalysis panes\nOverall drift analysis\nFeatures analysis\nIncoming features graph\nProject and model summary#\nUse the dropdown to change the project and model. The dashboard presents the following information about the project:\nEndpoint ID — the ID of the deployed model\nModel — user defined name for the model\nFunction URI — the MLRun function to access the model\nModel Class — the implementation class that is used by the endpoint\nPrediction/s — the average number of predictions per second over a rolling 5-minute period\nAverage Latency — the average latency over a rolling 1-hour period\nFirst Request — first request for production data\nLast Request — most recent request for production data\nUse the Performance and Overview buttons view those dashboards.\nAnalysis panes#\nThis pane has two sections: Overall Drift Analysis and Features Analysis.\nThe Overall Drift Analysis pane provides performance statistics for the currently selected model.\nTVD (sum and mean)\nHellinger (sum and mean)\nKLD (sum and mean)\nThe Features Analysis pane provides details of the drift analysis for each feature in the selected model.\nThis pane includes five types of statistics:\nActual (min, mean and max) — results based on actual live data stream\nExpected (min, mean and max) — results based on training data\nTVD\nHellinger\nKLD\nIncoming features graph#\nThis graph displays the performance of the features that are in the selected model based on sampled data points from actual feature production data.\nThe graph displays the values of the features in the model over time.\nModel endpoint performance dashboard#\nModel endpoint performance displays performance details in graphical format.\nThis dashboard has five graphs:\nDrift Measures — the overall drift over time for each of the endpoints in the selected model\nAverage Latency — the average latency of the model in 5 minute intervals, for 5 minutes and 1 hour rolling windows\nPredictions/s  — the model predictions per second displayed in 5 second intervals for 5 minutes (rolling)\nPredictions Count — the number of predictions the model makes for 5 minutes and 1 hour rolling windows\nConfiguring Grafana dashboards#\nVerify that you have a Grafana service running in your Iguazio MLOps Platform.\nIf you do not have a Grafana service running,\nsee Adding Grafana Dashboards to create and configure it. When you create the service: In the Custom Parameters tab, Platform data-access user parameter, select a user with access to the /user/pipelines directory.\nFor working with Iguazio 3.0.x:\nMake sure you have the model-monitoring as a Grafana data source configured in your Grafana service. If not,\nadd it by:\nOpen your grafana service.\nNavigate to Configuration | Data Sources.\nPress Add data source.\nSelect the SimpleJson datasource and configure the following parameters.\nName: model-monitoring\nURL: http://mlrun-api:8080/api/grafana-proxy/model-endpoints\nAccess: Server (default)\n## Add a custom header of:\nX-V3io-Session-Key: <YOUR ACCESS KEY>\nPress Save & Test for verification. You’ll receive a confirmation with either a success or a failure message.\nDownload the following monitoring dashboards:\nModel Monitoring - Overview\nModel Monitoring - Details\nModel Monitoring - Performance\nImport the downloaded dashboards to your Grafana service:\nNavigate to your Grafana service in the Services list and press it.\nPress the dashboards icon in left menu.\nIn the Dashboard Management screen, press IMPORT, and select one file to import. Repeat this step for each dashboard.\nFor working with Iguazio 3.2.x and later:\nAdd access keys to your model-monitoring data source:\nOpen your Grafana service.\nNavigate to Configuration | Data Sources.\nPress mlrun-model-monitoring.\nIn Custom HTTP Headers, configure the cookie parameter. Set the value of cookie to:\nsession=j:{\"sid\": \"<YOUR ACCESS KEY>\"}\nThe overview, details, and performance dashboards are in Dashboards | Manage | private\nNote\nYou need to train and deploy a model to see results in the dashboards.\nThe dashboards immediately display data if you already have a model that is trained and running with production data."}
{"text": "Build and run workflows/pipelines#\nThis section shows how to write a batch pipeline so that it can be executed via an MLRun Project.\nWith a batch pipelineyou can use the MLRun Project to execute several Functions in a DAG using the Python SDK or CLI.\nThis example creates a project with three MLRun functions and a single pipeline that orchestrates them. The pipeline steps are:\nget-data — Get iris data from sklearn\ntrain-model — Train model via sklearn\ndeploy-model — Deploy model to HTTP endpoint\nimport mlrun\nproject = mlrun.get_or_create_project(\\\"iguazio-academy\\\", context=\\\"./\\\")\nAdd functions to a project#\nAdd the functions to a project:\nproject.set_function(name='get-data', func='functions/get_data.py', kind='job', image='mlrun/mlrun')\nproject.set_function(name='train-model', func='functions/train.py', kind='job', image='mlrun/mlrun'),\nproject.set_function(name='deploy-model', func='hub://v2_model_server')\nWrite a pipeline#\nNext, define the pipeline that orchestrates the three comoponents. This pipeline is simple, however you can create very complex pipelines with branches, conditions, and more.\n%%writefile pipelines/training_pipeline.py\nfrom kfp import dsl\nimport mlrun\n@dsl.pipeline(\nname=\\\"batch-pipeline-academy\\\",\ndescription=\\\"Example of batch pipeline for Iguazio Academy\\\"\n)\ndef pipeline(label_column: str, test_size=0.2):\n# Ingest the data set\ningest = mlrun.run_function(\n'get-data',\nhandler='prep_data',\nparams={'label_column': label_column},\noutputs=[\\\"iris_dataset\\\"]\n)\n# Train a model\ntrain = mlrun.run_function(\n\\\"train-model\\\",\nhandler=\\\"train_model\\\",\ninputs={\\\"dataset\\\": ingest.outputs[\\\"iris_dataset\\\"]},\nparams={\n\\\"label_column\\\": label_column,\n\\\"test_size\\\" : test_size\n},\noutputs=['model']\n)\n# Deploy the model as a serverless function\ndeploy = mlrun.deploy_function(\n\\\"deploy-model\\\",\nmodels=[{\\\"key\\\": \\\"model\\\", \\\"model_path\\\": train.outputs[\\\"model\\\"]}]\n)\nAdd a pipeline to a project#\nAdd the pipeline to your project:\nproject.set_workflow(name='train', workflow_path=\\\"pipelines/training_pipeline.py\")\nproject.save()"}
{"text": "CI/CD integration#\n+You can run your ML Pipelines using CI frameworks like Github Actions, GitLab CI/CD, etc. MLRun supports a simple and native integration\nwith the CI systems.\nBuild/run complex workflows composed of local/library functions or external cloud services (e.g. AutoML)\nSupport various Pipeline/CI engines (Kubeflow, GitHub, Gitlab, Jenkins)\nTrack & version code, data, params, results with minimal effort\nElastic scaling of each step\nExtensive Function Hub\nMLRun workflows can run inside the CI system. The most common method is to use the CLI command  mlrun project to load the project\nand run a workflow as part of a code update (e.g. pull request, etc.). The pipeline tasks are executed on the Kubernetes cluster, which is orchestrated by MLRun.\nWhen MLRun is executed inside a GitHub Action or GitLab CI/CD\npipeline it detects the environment attributes automatically\n(e.g. repo, commit id, etc.). In addition, a few environment variables and credentials must be set:\nMLRUN_DBPATH — url of the MLRun cluster.\nV3IO_USERNAME — username in the remote Iguazio cluster.\nV3IO_ACCESS_KEY — access key to the remote Iguazio cluster.\nGIT_TOKEN or GITHUB_TOKEN — Github/Gitlab API token (set automatically in Github Actions).\nSLACK_WEBHOOK — optional. Slack API key when using slack notifications.\nWhen the workflow runs inside the Git CI system it reports the pipeline progress and results back into the Git tracking system, similar to:\nContents\nUsing GitHub Actions\nUsing GitLab CI/CD\nUsing Jenkins Pipeline\nUsing GitHub Actions#\nWhen running using GitHub Actions you need to set the credentials/secrets\nand add a script under the .github/workflows/ directory, which is executed when the code is commited/pushed.\nExample script that is invoked when you add the comment “/run” to your pull request:\nname: mlrun-project-workflow\non: [issue_comment]\njobs:\nsubmit-project:\nif: github.event.issue.pull_request != null && startsWith(github.event.comment.body, '/run')\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v3\n- name: Set up Python 3.7\nuses: actions/setup-python@v4\nwith:\npython-version: '3.7'\narchitecture: 'x64'\n- name: Install mlrun\nrun: python -m pip install pip install mlrun\n- name: Submit project\nrun: python -m mlrun project ./ -w -r main ${CMD:5}\nenv:\nV3IO_USERNAME: ${{ secrets.V3IO_USERNAME }}\nV3IO_API: ${{ secrets.V3IO_API }}\nV3IO_ACCESS_KEY: ${{ secrets.V3IO_ACCESS_KEY }}\nMLRUN_DBPATH: ${{ secrets.MLRUN_DBPATH }}\nGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\nSLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}\nCMD: ${{ github.event.comment.body}}\nSee the full example in https://github.com/mlrun/project-demo\nUsing GitLab CI/CD#\nWhen running using GitLab CI/CD you need to set the credentials/secrets\nand update the script .gitlab-ci.yml directory, which is executed when code is commited/pushed.\nExample script that is invoked when you create a pull request (merge requests):\nimage: mlrun/mlrun\nrun:\nscript:\n- python -m mlrun project ./ -w -r ci\nonly:\n- merge_requests\nSee the full example in https://gitlab.com/yhaviv/test2\nUsing Jenkins Pipeline#\nWhen using Jenkins Pipeline you need to set up the credentials/secrets in Jenkins and\nand update the script Jenkinsfile in your codebase. You can trigger the Jenkins pipeline either through Jenkins triggers or through the GitHub webhooks.\nExample Jenkinesfile that is invoked when you start a Jenkins pipeline (via triggers or GitHub webhooks):\npipeline {\nagent any\nenvironment {\nRELEASE='1.0.0'\nPROJECT_NAME='project-demo'\n}\nstages {\nstage('Audit tools') {\nsteps{\nauditTools()\n}\n}\nstage('Build') {\nenvironment {\nMLRUN_DBPATH='https://mlrun-api.default-tenant.app.us-sales-341.iguazio-cd1.com'\nV3IO_ACCESS_KEY=credentials('V3IO_ACCESS_KEY')\nV3IO_USERNAME='xingsheng'\n}\nagent {\ndocker {\nimage 'mlrun/mlrun:1.2.0'\n}\n}\nsteps {\necho \"Building release ${RELEASE} for project ${PROJECT_NAME}...\"\nsh 'chmod +x build.sh'\nwithCredentials([string(credentialsId: 'an-api-key', variable: 'API_KEY')]) {\nsh '''\n./build.sh\n'''\n}\n}\n}\nstage('Test') {\nsteps {\necho \"Testing release ${RELEASE}\"\n}\n}\n}\npost {\nsuccess {\nslackSend channel: '#builds',\ncolor: 'good',\nmessage: \"Project ${env.PROJECT_NAME}, success: ${currentBuild.fullDisplayName}.\"\n}\nfailure {\nslackSend channel: '#builds',\ncolor: 'danger',\nmessage: \"Project ${env.PROJECT_NAME}, FAILED: ${currentBuild.fullDisplayName}.\"\n}\n}\n}\nvoid auditTools() {\nsh '''\ngit version\ndocker version\n'''\n}\nAfter the Jenkins pipeline is complete, you can see the MLRun job in the MLRun UI.\nSee the full example in https://github.com/mlrun/project-demo."}
{"text": "Create, save, and use projects#\nA project is a container for all the assets, configuration, and code of a particular application. It is the starting point for your work. Projects are stored in a versioned source repository (GIT) or archive and can map to IDE projects (in PyCharm, VSCode, etc.).\nIn this section\nCreating a new project\nAdding functions, artifacts, workflow, and config\nPushing the project content into git or an archive\nGet a project from DB or create it\nCreating a new project#\nProject files (code, configuration, etc.) are stored in a directory (the project context path) and can be pushed to, or loaded from, the source repository. See the following project directory example:\nmy-project           # Parent directory of the project (context)\n├── data             # Project data for local tests or outputs (not tracked by version control)\n├── docs             # Project documentation\n├── src              # Project source code (functions, libs, workflows)\n├── tests            # Unit tests (pytest) for the different functions\n├── project.yaml     # MLRun project spec file\n├── README.md        # Project README\n└── requirements.txt # Default Python requirements file (can have function specific requirements as well)\nTo define a new project from scratch, use new_project(). You must specify a name and a\nlocation for the context directory (e.g. \"./\"). There are additional, optional parameters.\nThe context dir holds the configuration, code, and workflow files. File paths in the project are relative to the context root.\nThe user_project flag indicates that the project name is unique per user, and the init_git flag is used to initialize git in the context dir.\nimport mlrun\nproject = mlrun.new_project(\"myproj\", \"./\", user_project=True,\ninit_git=True, description=\"my new project\")\nProjects can also be created from a template (yaml file, zip file, or git repo), allowing users to create reusable skeletons.\nThe content of the zip/tar/git archive is copied into the context dir. The remote attribute can be used to register a remote git repository URL.\nExample of creating a new project from a zip template:\n# create a project from zip, initialize a local git, and register the git remote path\nproject = mlrun.new_project(\"myproj\", \"./\", init_git=True, user_project=True,\nremote=\"git://github.com/myorg/some-project.git\",\nfrom_template=\"http://mysite/proj.zip\")\nAdding functions, artifacts, workflow, and config#\nProjects host functions, workflows, artifacts (files, datasets, models, etc.), features, and configuration (parameters, secrets, source, etc.).\nThis section explains how to add or register different project elements. For details on the feature store and its elements (sets, vectors) see the feature store documentation.\nAdding and registering functions:\nFunctions with basic attributes such as code, requirements, image, etc. can be registered using the set_function() method.\nFunctions can be created from a single code/notebook file or have access to the entire project context directory. (By adding the with_repo=True flag, the project context is cloned into the function runtime environment.) See the examples:\n# register a (single) python file as a function\nproject.set_function('src/data_prep.py', 'data-prep', image='mlrun/mlrun', handler='prep', kind=\"job\")\n# register a notebook file as a function, specify custom image and extra requirements\nproject.set_function('src/mynb.ipynb', name='test-function', image=\"my-org/my-image\",\nhandler=\"run_test\", requirements=\"requirements.txt\", kind=\"job\")\n# register a module.handler as a function (requires defining the default sources/work dir, if it's not root)\nproject.spec.workdir = \"src\"\nproject.set_function(name=\"train\", handler=\"training.train\",  image=\"mlrun/mlrun\", kind=\"job\", with_repo=True)\nSee details and examples on how to create and register functions,\nhow to annotate notebooks (to be used as functions), how to run, build, or deploy functions, and how to use them in workflows.\nRegister artifacts:\nArtifacts are used by functions and workflows and are referenced by a key (name) and optional tag (version).\nUsers can define artifact files or objects in the project spec, which are registered during project load or when calling project.register_artifacts().\nTo register artifacts use the set_artifact()) method. See the examples:\n# register a simple file artifact in the project (point to remote object)\ndata_url = 'https://s3.wasabisys.com/iguazio/data/iris/iris.data.raw.csv'\nproject.set_artifact('data', target_path=data_url)\n# register a model artifact\nproject.set_artifact('model', ModelArtifact(model_file=\"model.pkl\"), target_path=model_dir_url)\n# register local or remote artifact object (yaml or zip), will be imported on project load\n# to generate such a package use `artifact.export(zip_path)`\nproject.set_artifact('model', 'https://mystuff.com/models/mymodel.zip')\nNote\nLocal file paths are relative to the context dir.\nRegistering workflows:\nProjects contain one or more workflows (pipelines). The workflows can be registered using the set_workflow()) method.\nProject workflows are executed using the run()) method. See building and running workflows for details.\n# Add a multi-stage workflow (./myflow.py) to the project with the name 'main' and save the project\nproject.set_workflow('main', \"./src/workflow.py\")\nSet project wide parameters and secrets:\nYou can define global project parameters and secrets and use them in your functions enabling simple configuration and templates.\nSee the examples:\n# Read env vars from dict or file and set as project secrets\nproject.set_secrets({\"SECRET1\": \"value\"})\nproject.set_secrets(file_path=\"secrets.env\")\nproject.spec.params = {\"x\": 5}\nNote\nSecrets are not loaded automatically (not part of the project.yaml); you need to apply set_secrets() methods manually or use the UI.\nProject parameters, secrets and configuration can also be set in the UI, in the relevant project, click the\nbutton at the left bottom corner.\nExample, secrets configuration screen:\nSave the project:\nUse the save() method to store all the definitions (functions, artifacts, workflows, parameters, etc.)\nin the MLRun DB and in the project.yaml file (for automated loading and CI/CD).\nproject.save()\nshow an example project.yaml file\nThe generated project.yaml for the above project looks like:\nind: project\nmetadata:\nname: myproj\nspec:\ndescription: my new project\nparams:\nx: 5\nfunctions:\n- url: src/data_prep.py\nname: data-prep\nimage: mlrun/mlrun\nhandler: prep\n- url: src/mynb.ipynb\nname: test-function\nkind: job\nimage: my-org/my-image\nhandler: run_test\nrequirements: requirements.txt\n- name: train\nkind: job\nimage: mlrun/mlrun\nhandler: training.train\nwith_repo: true\nworkflows:\n- path: ./src/workflow.py\nname: main\nartifacts:\n- kind: artifact\nmetadata:\nproject: myproj\nkey: data\nspec:\ntarget_path: https://s3.wasabisys.com/iguazio/data/iris/iris.data.raw.csv\nsource: ''\nworkdir: src```\nPushing the project content into git or an archive#\nProject code, metadata, and configuration are stored and versioned in source control systems like GIT or archives (zip, tar).\nThis allows loading an entire project (with a specific version) into a development or production environment, or seamlessly integrating with CI/CD frameworks.\nNote\nYou must push the updates before you build functions or run workflows that use code from git,\nsince the builder or containers pull the code from the git repo.\nUse standard Git commands to push the current project tree into a git archive. Make sure you .save() the project before pushing it\ngit remote add origin <server>\ngit commit -m \"Commit message\"\ngit push origin master\nAlternatively you can use MLRun SDK calls:\ncreate_remote() - to register the remote Git path\npush() - save project spec (project.yaml) and commit/push updates to remote repo\n{admonition} Note\nIf you are using containerized Jupyter you might need to first set your Git parameters, e.g. using the following commands:\ngit config --global user.email \"<my@email.com>\"\ngit config --global user.name \"<name>\"\ngit config --global credential.helper store\nYou can also save the project content and metadata into a local or remote .zip archive, for example:\nproject.export(\"../archive1.zip\")\nproject.export(\"s3://my-bucket/archive1.zip\")\nproject.export(f\"v3io://projects/{project.name}/archive1.zip\")\nGet a project from DB or create it#\nIf you already have a project saved in the DB and you need to access/use it (for example, from a different notebook or file),\nuse the get_or_create_project() method. It first tries to read the project from the DB,\nand only if it doesn’t exist in the DB it loads/creates it.\nNote\nIf you update the project object from different files/notebooks/users, make sure you .save() your project after a change,\nand run get_or_create_project to load changes made by others.\nExample:\n# load project from the DB (if exists) or the source repo\nproject = mlrun.get_or_create_project(\"myproj\", \"./\", \"git://github.com/mlrun/demo-xgb-project.git\")\nproject.pull(\"development\")  # pull the latest code from git\nproject.run(\"main\", arguments={'data': data_url})  # run the workflow \"main\""}
{"text": "Load and run projects#\nProject code, metadata, and configuration are stored and versioned in source control systems like Git or archives (zip, tar)\nand can be loaded into your work environment or CI system with a single SDK or CLI command.\nThe project root (context) directory contains the project.yaml file with the required metadata and links to various project files/objects, and is read during the load process.\nIn this section\nLoad projects using the SDK\nLoad projects using the CLI\nSee also details on loading and using projects with CI/CD frameworks.\nLoad projects using the SDK#\nWhen a project is already created and stored in a local dir, git, or archive, you can quickly load and use it with the\nload_project() method. load_project uses a local context directory (with initialized git)\nor clones a remote repo into the local dir and returns a project object.\nYou need to provide the path to the context dir and the git/zip/tar archive url. The name can be specified or taken\nfrom the project object. The project can also specify secrets (dict with repo credentials), init_git flag (initializes Git in the context dir),\nclone flag (project is cloned into the context dir, and the local copy is ignored/deleted), and user_project flag (indicates the project name is unique to the user).\nExample of loading a project from git and running the main workflow:\n# load the project and run the 'main' workflow\nproject = load_project(context=\"./\", name=\"myproj\", url=\"git://github.com/mlrun/project-archive.git\")\nproject.run(\"main\", arguments={'data': data_url})\nNote\nIf the url parameter is not specified it searches for Git repo inside the context dir and uses its metadata,\nor if the flag init_git=True, it initializes a Git repo in the target context directory.\nAfter the project object is loaded use the run() method to execute workflows. See details on building and running workflows),\nand how to run, build, or deploy individual functions.\nYou can edit or add project elements like functions, workflows, artifacts, etc. (See  create and use projects.)\nOnce you make changes use GIT or MLRun commands to push those changes to the archive (See save into git or an archive.)\nLoad projects using the CLI#\nLoading a project from git into ./ :\nmlrun project -n myproj --url \"git://github.com/mlrun/project-demo.git\" .\nRunning a specific workflow (main) from the project stored in . (current dir):\nmlrun project --run main --watch .\nCLI usage details\nUsage: mlrun project [OPTIONS] [CONTEXT]\nOptions:\n-n, --name TEXT           project name\n-u, --url TEXT            remote git or archive url\n-r, --run TEXT            run workflow name of .py file\n-a, --arguments TEXT      Kubeflow pipeline arguments name and value tuples\n(with -r flag), e.g. -a x=6\n-p, --artifact-path TEXT  output artifacts path\n-x, --param TEXT          mlrun project parameter name and value tuples,\ne.g. -p x=37 -p y='text'\n-s, --secrets TEXT        secrets file=<filename> or env=ENV_KEY1,..\n--db TEXT                 api and db service path/url\n--init-git                for new projects init git context\n-c, --clone               force override/clone into the context dir\n--sync                    sync functions into db\n-w, --watch               wait for pipeline completion (with -r flag)\n-d, --dirty               allow run with uncommitted git changes\n--handler TEXT            workflow function handler name\n--engine TEXT             workflow engine (kfp/local/remote)\n--local                   try to run workflow functions locally\n--timeout INTEGER         timeout in seconds to wait for pipeline completion\n(used when watch=True)\n--env-file TEXT           path to .env file to load config/variables from\n--ensure-project          ensure the project exists, if not, create project\n--schedule TEXT           To create a schedule define a standard crontab\nexpression string. For using the\npre-defined workflow's schedule, set --schedule 'true'"}
{"text": "Projects and automation#\nMLRun Project is a container for all your work on a particular ML application. Projects host functions, workflows, artifacts (datasets, models, etc.), features (sets, vectors), and configuration (parameters, secrets\n, source, etc.). Projects have owners and members with role-based access control.\nProjects are stored in a GIT or archive and map to IDE projects (in PyCharm, VSCode, etc.), which enables versioning, collaboration, and CI/CD.\nProjects simplify how you process data, submit jobs, run multi-stage workflows, and deploy real-time pipelines in continious development or production environments.\nIn this section\nCreate, save, and use projects\nLoad and run projects\nRun, build, and deploy functions\nBuild and run workflows/pipelines\nCI/CD integration\nWorking with secrets"}
{"text": "Run, build, and deploy functions#\nIn this section\nOverview\nrun_function\nbuild_function\ndeploy_function\nOverview#\nThere is a set of methods used to deploy and run project functions. They can be used interactively or inside a pipeline (e.g. Kubeflow).\nWhen used inside a pipeline, each method is automatically mapped to the relevant pipeline engine command.\nrun_function() — Run a local or remote task as part of local or remote batch/scheduled task\nbuild_function() — deploy an ML function, build a container with its dependencies for use in runs\ndeploy_function() — deploy real-time/online (nuclio or serving based) functions\nYou can use those methods as project methods, or as global (mlrun.) methods. For example:\n# run the \"train\" function in myproject\nrun = myproject.run_function(\"train\", inputs={\"data\": data_url})\n# run the \"train\" function in the current/active project (or in a pipeline)\nrun = mlrun.run_function(\"train\", inputs={\"data\": data_url})\nThe first parameter in all three methods is either the function name (in the project), or a function object, used if you want to\nspecify functions that you imported/created ad hoc, or to modify a function spec. For example:\n# import a serving function from the Function Hub and deploy a trained model over it\nserving = import_function(\"hub://v2_model_server\", new_name=\"serving\")\nserving.spec.replicas = 2\ndeploy = deploy_function(\nserving,\nmodels=[{\"key\": \"mymodel\", \"model_path\": train.outputs[\"model\"]}],\n)\nYou can use the get_function() method to get the function object and manipulate it, for example:\ntrainer = project.get_function(\"train\")\ntrainer.with_limits(mem=\"2G\", cpu=2, gpus=1)\nrun = project.run_function(\"train\", inputs={\"data\": data_url})\nrun_function#\nUse the run_function() method to run a local or remote batch/scheduled task.\nThe run_function method accepts various parameters such as name, handler, params, inputs, schedule, etc.\nAlternatively, you can pass a Task object (see: new_task()) that holds all of the\nparameters and the advanced options.\nFunctions can host multiple methods (handlers). You can set the default handler per function. You need to specify which handler you intend to call in the run command.\nYou can pass parameters (arguments) or data inputs (such as datasets, feature-vectors, models, or files) to the functions through the run_function method.\nThe run_function() command returns an MLRun RunObject object that you can use to track the job and its results.\nIf you pass the parameter watch=True (default), the command blocks until the job completes.\nMLRun also supports iterative jobs that can run and track multiple child jobs (for hyperparameter tasks, AutoML, etc.).\nSee Hyperparameter tuning optimization for details and examples.\nRead further details on running tasks and getting their results.\nUsage examples:\n# create a project with two functions (local and from Function Hub)\nproject = mlrun.new_project(project_name, \"./proj\")\nproject.set_function(\"mycode.py\", \"prep\", image=\"mlrun/mlrun\")\nproject.set_function(\"hub://auto_trainer\", \"train\")\n# run functions (refer to them by name)\nrun1 = project.run_function(\"prep\", params={\"x\": 7}, inputs={'data': data_url})\nrun2 = project.run_function(\"train\", inputs={\"dataset\": run1.outputs[\"data\"]})\nrun2.artifact('confusion-matrix').show()\nRun/simulate functions locally:\nFunctions can also run and be debugged locally by using the local runtime or by setting the local=True\nparameter in the run() method (for batch functions).\nbuild_function#\nThe build_function() method is used to deploy an ML function and build a container with its dependencies for use in runs.\nExample:\n# build the \"trainer\" function image (based on the specified requirements and code repo)\nproject.build_function(\"trainer\")\nThe build_function() method accepts different parameters that can add to, or override, the function build spec.\nYou can specify the target or base image extra docker commands, builder environment, and source credentials (builder_env), etc.\nSee further details and examples in Build function image.\ndeploy_function#\nThe deploy_function() method is used to deploy real-time/online (nuclio or serving) functions and pipelines.\nRead more about Real-time serving pipelines.\nBasic example:\n# Deploy a real-time nuclio function (\"myapi\")\ndeployment = project.deploy_function(\"myapi\")\n# invoke the deployed function (using HTTP request)\nresp = deployment.function.invoke(\"/do\")\nYou can provide the env dict with: extra environment variables; models list to specify specific models and their attributes\n(in the case of serving functions); builder environment; and source credentials (builder_env).\nExample of using deploy_function inside a pipeline, after the train step, to generate a model:\n# Deploy the trained model (from the \"train\" step) as a serverless serving function\nserving_fn = mlrun.new_function(\"serving\", image=\"mlrun/mlrun\", kind=\"serving\")\nmlrun.deploy_function(\nserving_fn,\nmodels=[\n{\n\"key\": model_name,\n\"model_path\": train.outputs[\"model\"],\n\"class_name\": 'mlrun.frameworks.sklearn.SklearnModelServer',\n}\n],\n)\nNote\nIf you want to create a simulated (mock) function instead of a real Kubernetes service, set the mock flag is set to True. See deploy_function api."}
{"text": "Managing job resources#\nMLRun orchestrates serverless functions over Kubernetes. You can specify the resource requirements (CPU, memory, GPUs),\npreferences, and priorities in the logical function object. These are used during the function deployment.\nConfiguration of job resources is relevant for all supported cloud platforms.\nIn this section\nReplicas\nCPU, GPU, and memory limits for user jobs\nVolumes\nPreemption mode: Spot vs. On-demand nodes\nPod priority for user jobs\nReplicas#\nSome runtimes can scale horizontally, configured either as a number of replicas:\nspec.replicas\nor a range (for auto scaling in Dask or Nuclio:\nspec.min_replicas = 1\nspec.max_replicas = 4\nSee more details in Dask, MPIJob and Horovod, Spark, Nuclio.\nCPU, GPU, and memory limits for user jobs#\nWhen you create a pod in an MLRun job or Nuclio function, the pod has default CPU and memory limits. When the job runs, it can consume\nresources up to the limits defined. The default limits are set at the service level. You can change the default limit for the service, and\nalso overwrite the default when creating a job, or a function.\nSee more about Kubernetes Resource Management for Pods and Containers.\nUI configuration#\nWhen creating a service, set the Memory and CPU in the Common Parameters tab, under User jobs defaults.\nWhen creating a job or a function, overwrite the default Memory, CPU, or GPU in the Configuration tab, under Resources.\nSDK configuration#\nConfigure the limits assigned to a function by using with_limits. For example:\ntraining_function = mlrun.code_to_function(\"training.py\", name=\"training\", handler=\"train\",\nkind=\"mpijob\", image=\"mlrun/ml-models-gpu\")\ntraining_function.spec.replicas = 2\ntraining_function.with_requests(cpu=2)\ntraining_function.gpus(1)\nNote\nWhen specifying GPUs, MLRun uses nvidia.com/gpu as default GPU type. To use a different type of GPU, specify it using the optional gpu_type parameter.\nVolumes#\nWhen you create a pod in an MLRun job or Nuclio function, the pod by default has access to a file-system which is ephemeral, and gets\ndeleted when the pod completes its execution. In many cases, a job requires access to files residing on external storage, or to files\ncontaining configurations and secrets exposed through Kubernetes config-maps or secrets.\nPods can be configured to consume the following types of volumes, and to mount them as local files in the local pod file-system:\nV3IO containers: when running on the Iguazio system, pods have access to the underlying V3IO shared storage. This option mounts a V3IO container or a subpath within it to the pod through the V3IO FUSE driver.\nPVC: Mount a Kubernetes persistent volume claim (PVC) to the pod. The persistent volume and the claim need to be configured beforehand.\nConfig Map: Mount a Kubernetes Config Map as local files to the pod.\nSecret: Mount a Kubernetes secret as local files to the pod.\nFor each of the options, a name needs to be assigned to the volume, as well as a local path to mount the volume at (using a Kubernetes Volume Mount). Depending on the type of the volume, other configuration options may be needed, such as an access-key needed for V3IO volume.\nSee more about Kubernetes Volumes.\nMLRun supports the concept of volume auto-mount which automatically mounts the most commonly used type of volume to all pods, unless disabled. See more about MLRun auto mount.\nUI configuration#\nYou can configure Volumes when creating a job, rerunning an existing job, and creating an ML function.\nModify the Volumes for an ML function by pressing ML functions, then\nof the function, Edit | Resources | Volumes drop-down list.\nSelect the volume mount type: either Auto (using auto-mount), Manual or None. If selecting Manual, fill in the details in the volumes list\nfor each volume to mount to the pod. Multiple volumes can be configured for a single pod.\nSDK configuration#\nConfigure volumes attached to a function by using the apply function modifier on the function.\nFor example, using v3io storage:\n# import the training function from the Function Hub (hub://)\ntrain = mlrun.import_function('hub://sklearn_classifier')# Import the function:\nopen_archive_function = mlrun.import_function(\"hub://open_archive\")\n# use mount_v3io() for iguazio volumes\nopen_archive_function.apply(mount_v3io())\nYou can specify a list of the v3io path to use and how they map inside the container (using volume_mounts). For example:\nmlrun.mount_v3io(name='data',access_key='XYZ123..',volume_mounts=[mlrun.VolumeMount(\"/data\", \"projects/proj1/data\")])\nSee full details in mount_v3io.\nAlternatively, using a PVC volume:\nmount_pvc(pvc_name=\"data-claim\", volume_name=\"data\", volume_mount_path=\"/data\")\nSee full details in mount_pvc.\nPreemption mode: Spot vs. On-demand nodes#\nNode selector is supported for all cloud platforms. It is relevant for MLRun and Nuclio only.\nWhen running ML functions you might want to control whether to run on spot nodes or on-demand nodes. Preemption mode controls whether pods can be scheduled on preemptible (spot) nodes. Preemption mode is supported for all functions.\nPreemption mode uses Kubernets Taints and Toleration to enforce the mode selected. Read more in Kubernetes Taints and Tolerations.\nWhy preemption mode#\nOn-demand instances provide full control over the instance lifecycle. You decide when to launch, stop, hibernate, start,\nreboot, or terminate it. With Spot instances you request capacity from specific available zones, though it is\nsusceptible to spot capacity availability. This is a good choice if you can be flexible about when your applications run\nand if your applications can be interrupted.\nHere are some questions to consider when choosing the type of node:\nIs the function mission critical and must be operational at all times?\nIs the function a stateful function or stateless function?\nCan the function recover from unexpected failure?\nIs this a job that should run only when there are available inexpensive resources?\nImportant\nWhen an MLRun job is running on a spot node and it fails, it won’t get back up again. However, if Nuclio goes down due to a spot issue, it\nis brought up by Kubernetes.\nKuberenetes has a few methods for configuring which nodes to run on. To get a deeper understanding, see Pod Priority and Preemption.\nAlso, you must understand the configuration of the spot nodes as specified by the cloud provider.\nStateless and Stateful Applications#\nWhen deploying your MLRun jobs to specific nodes, take into consideration that on-demand\nnodes are designed to run stateful applications while spot nodes are designed for stateless applications.\nMLRun jobs are more stateful by nature. An MLRun job that is assigned to run on a spot node might be subject to interruption;\nit would have to be designed so that the job/function state will be saved when scaling to zero.\nSupported preemption modes#\nPreemption mode has three values:\nAllow: The function pod can run on a spot node if one is available.\nConstrain: The function pod only runs on spot nodes, and does not run if none is available.\nPrevent: Default. The function pod cannot run on a spot node.\nUI configuration#\nNote\nRelevant when MLRun is executed in the Iguazio platform.\nYou can configure Spot node support when creating a job, rerunning an existing job, and creating an ML function.\nThe Run on Spot nodes drop-down list is in the Resources section of jobs.\nConfigure the Spot node support for individual Nuclio functions when creating a function in the Configuration tab, under Resources.\nSDK configuration#\nConfigure preemption mode by adding the with_preemption_mode parameter in your Jupyter notebook, and specifying a mode from the list of values above.\nThis example illustrates a function that cannot be scheduled on preemptible nodes:\nimport mlrun\nimport os\ntrain_fn = mlrun.code_to_function('training',\nkind='job',\nhandler='my_training_function')\ntrain_fn.with_preemption_mode(mode=\"prevent\")\ntrain_fn.run(inputs={\"dataset\" :my_data})\nSee with_preemption_mode.\nAlternatively, you can specify the preemption using with_priority_class and fn.with_priority_class(name=\"default-priority\")node_selector. This example specifies that the pod/function runs only on non-preemptible nodes:\nimport mlrun\nimport os\ntrain_fn = mlrun.code_to_function('training',\nkind='job',\nhandler='my_training_function')\ntrain_fn.with_preemption_mode(mode=\"prevent\")\ntrain_fn.run(inputs={\"dataset\" :my_data})\nfn.with_priority_class(name=\"default-priority\")\nfn.with_node_selection(node_selector={\"app.iguazio.com/lifecycle\":\"non-preemptible\"})\nSee with_node_selection.\nPod priority for user jobs#\nPods (services, or jobs created by those services) can have priorities, which indicate the relative importance of one pod to the other pods on the node. The priority is used for\nscheduling: a lower priority pod can be evicted to allow scheduling of a higher priority pod. Pod priority is relevant for all pods created\nby the service. For MLRun, it applies to the jobs created by MLRun. For Nuclio it applies to the pods of the Nuclio-created functions.\nEviction uses these values to determine what to evict with conjunction to the pods priority Pod Priority and Preemption.\nPod priority is specified through Priority classes, which map to a priority value. The priority values are: High, Medium, Low. The default is Medium. Pod priority is supported for:\nMLRun jobs: the default priority class for the jobs that MLRun creates.\nNuclio functions: the default priority class for the user-created functions.\nJupyter\nPresto (The pods priority also affects any additional services that are directly affected by Presto, for example like hive and mariadb,\nwhich are created if Enable hive is checked in the Presto service.)\nGrafana\nShell\nUI configuration#\nNote\nRelevant when MLRun is executed in the Iguazio platform.\nConfigure the default priority for a service, which is applied to the service itself or to all subsequently created user-jobs in the\nservice’s Common Parameters tab, User jobs defaults section, Priority class drop-down list.\nModify the priority for an ML function by pressing ML functions, then\nof the function, Edit | Resources | Pods Priority drop-down list.\nSDK configuration#\nConfigure pod priority by adding the priority class parameter in your Jupyter notebook.\nFor example:\nimport mlrun\nimport os\ntrain_fn = mlrun.code_to_function('training',\nkind='job',\nhandler='my_training_function')\ntrain_fn.with_priority_class(name={value})\ntrain_fn.run(inputs={\"dataset\" :my_data})\nSee with_priority_class."}
{"text": "Create and use functions#\nFunctions are the basic building blocks of MLRun. They are essentially Python objects that know how to run locally or on a Kubernetes cluster. This section covers how to create and customize an MLRun function, as well as common parameters across all functions.\nIn this section:\nFunctions overview\nFunctions and projects\nCreating functions\nCustomizing functions\nFunctions overview#\nMLRun functions are used to run jobs, deploy models, create pipelines, and more. There are various kinds of MLRun functions with different capabilities, however, there are commonalities across all functions. In general, an MLRun function looks like the following:\nYou can read more about MLRun Functions here. Each parameter and capability is explained in more detail in the following sections Creating functions and Customizing functions.\nFunctions and projects#\nFunction are members of an MLRun project. Once you register a function within a project, you can execute it in your local environment or at scale on a Kubernetes cluster.\nThe relationship between functions, workflows, and projects is as follows:\nAfter the MLRun functions and workflows are created and registered into the project, they are invoked using the project object. This workflow pairs especially well with Git and CI/CD integration.\nCreating functions#\nThe recommended way to create an MLRun function is by using an MLRun project (see create and use projects).\nThe general flow looks like the following:\nproject = mlrun.get_or_create_project(...)\nfn = project.set_function(...)\nWhen creating a function, there are 3 main scenarios:\nSingle source file — when your code can be contained in a single file\nMultiple source files — when your code requires additional files or dependencies\nImport existing function — when your function already exists elsewhere and you just want to import it\nNote\nUsing the set_function method of an MLRun project allows for each of these scenarios in a transparent way. Depending on the source passed in, the project registers the function using some lower level functions. For specific use cases, you also have access to the lower level functions new_function(), code_to_function(), and import_function().\nUsing set_function#\nThe MLRun project object has a method called set_function(), which is a one-size-fits-all way of creating an MLRun function. This method accepts a variety of sources including Python files, Jupyter Notebooks, Git repos, and more.\nNote\nThe return value of set_function is your MLRun function. You can immediately run it or apply additional configurations like resources, scaling, etc. See Customizing functions for more details.\nWhen using set_function there are a number of common parameters across all function types and creation scenarios. Consider the following example:\nfn = project.set_function(\nname=\"my-function\", tag=\"latest\", func=\"my_function.py\",\nimage=\"mlrun/mlrun\", kind=\"job\", handler=\"train_model\",\nrequirements=[\"pandas==1.3.5\"], with_repo=True\n)\nname: Name of your MLRun function within the given project. This is displayed in the MLRun UI, as well as the Kubernetes pod.\ntag: Tag for your function (much like a Docker image). Omitting this parameter defaults to latest.\nfunc: What to run with the MLRun function. This can be a number of things including files (.py, .ipynb, .yaml, etc.), URIs (hub:// prefixed Function Hub URI, db:// prefixed MLRun DB URI), existing MLRun function objects, or None (for current .ipynb file).\nimage: Docker image to use when containerizing the piece of code. If you also specify the requirements parameter to build a new Docker image, the image parameter is used as the base image.\nkind: Runtime the MLRun function uses. See Kinds of functions (runtimes) forthe list of supported batch and real-time runtimes.\nhandler: Default function handler to invoke (e.g. a Python function within your script). This handler can also be overriden when executing the function.\nrequirements: Additional Python dependencies needed for the function to run. Using this parameter results in a new Docker image (using the image parameter as a base image). This can be a list of Python dependencies or a path to a requirements.txt file.\nwith_repo: Whether a function requires additional files or dependencies within a Git repo or archive file. This Git repo or archive file is specified on a project level via project.set_source(...), which the function consumes. If this parameter is omitted, the default is False.\nBuilding images#\nIf your MLRun function requires additional libraries or files, you might need to build a new Docker image. You can do this by specifying a base image to use as the image, your requirements via requirements, and (optionally) your source code via with_repo=True (where the source is specified by project.set_source(...)). See Build function image for more information on when a build is required.\nNote\nWhen using with_repo, the contents of the Git repo or archive are available in the current working directory of your MLRun function during runtime.\nA good place to start is one of the default MLRun images:\nmlrun/mlrun: Suits most lightweight components (includes sklearn, pandas, numpy and more)\nmlrun/ml-models: Suits most CPU ML/DL workloads (includes Tensorflow, Keras, PyTorch and more)\nmlrun/ml-models-gpu: Suits most GPU ML/DL workloads (includes GPU Tensorflow, Keras, PyTorch and more )\nDockerfiles for the MLRun images can be found here.\nSingle source file#\nThe simplest way to create a function is to use a single file as the source. The code itself is embedded into the MLRun function object. This makes the function quite portable since it does not depend on any external files. You can use any source file supported by MLRun such as Python or Jupyter notebook.\nNote\nMLRun is not limited to Python. Files of type Bash, Go, etc. are also supported.\nPython#\nThis is the simplest way to create a function out of a given piece of code. Simply pass in the path to the Python file relative to your project context directory.\nfn = project.set_function(\nname=\"python\", func=\"job.py\",  kind=\"job\",\nimage=\"mlrun/mlrun\", handler=\"handler\"\n)\nJupyter Notebook#\nThis is a great way to create a function out of a Jupyter Notebook. Just pass in the path to the Jupyter Notebook  relative to your project context directory. You can use MLRun cell tags to specify which parts of the notebook should be included in the function.\nNote\nTo ensure that the latest changes are included, make sure you save your notebook before creating/updating the function.\nfn = project.set_function(\nname=\"notebook\", func=\"nb.ipynb\", kind=\"serving\",\nimage=\"mlrun/ml-models\", requirements=[\"pandas==1.3.5\"]\n)\nYou can also create an MLRun function out of the current Jupyter Notebook you are running in. To do this, simply ommit the func parameter in set_function.\nMultiple source files#\nIf your code requires additional files or external libraries, you need to use a source that supports multiple files such as Git, an archive (zip/tar/etc.), or V3IO file share. This approach (especially using a Git repo) pairs well with MLRun projects.\nTo do this, you must:\nProvide with_repo=True when creating your function via project.set_function(...)\nSet project source via project.set_source(source=...)\nThis instructs MLRun to load source code from the git repo/archive/file share associated with the project. There are two ways to load these additional files:\nLoad code from container#\nThe function is built once. This is the preferred approach for production workloads. For example:\nproject.set_source(source=\"git://github.com/mlrun/project-archive.git\")\nfn = project.set_function(\nname=\"myjob\", handler=\"job_func.job_handler\",\nimage=\"mlrun/mlrun\", kind=\"job\", with_repo=True,\n)\nproject.build_function(fn)\nLoad code at runtime#\nThe function pulls the source code at runtime. This is a simpler approach during development that allows for making code changes without re-building the image each time. For example:\narchive_url = \"https://s3.us-east-1.wasabisys.com/iguazio/project-archive/project-archive.zip\"\nproject.set_source(source=archive_url, pull_at_runtime=True)\nfn = project.set_function(\nname=\"nuclio\", handler=\"nuclio_func:nuclio_handler\",\nimage=\"mlrun/mlrun\", kind=\"nuclio\", with_repo=True,\n)\nImport or use an existing function#\nIf you already have an MLRun function that you want to import, you can do so from multiple locations such as YAML, Function Hub, and MLRun DB.\nYAML#\nMLRun functions can be exported to YAML files via fn.export(). These YAML files can then be imported via the following:\nfn = project.set_function(name=\"import\", func=\"function.yaml\")\nFunction Hub#\nFunctions can also be imported from the MLRun Function Hub: simply import using the name of the function and the hub:// prefix:\nNote\nBy default, the hub:// prefix points to the official Function Hub. You can, however, also substitute your own repo to create your own hub.\nfn = project.set_function(name=\"describe\", func=\"hub://describe\")\nMLRun DB#\nYou can also import functions directly from the MLRun DB. These could be functions that have not been pushed to a git repo, archive, or Function Hub. Import via the name of the function and the db:// prefix:\nfn = project.set_function(name=\"db\", func=\"db://import\")\nMLRun function#\nYou can also directly use an existing MLRun function object. This is usually used when more granular control over function parameters is required (e.g. advanced parameters that are not supported by set_function()).\nThis example uses a real-time serving pipeline (graph).\nfn = mlrun.new_function(\"serving\", kind=\"serving\", image=\"mlrun/mlrun\")\ngraph = serving.set_topology(\"flow\")\ngraph.to(name=\"double\", handler=\"mylib.double\") \\\n.to(name=\"add3\", handler=\"mylib.add3\") \\\n.to(name=\"echo\", handler=\"mylib.echo\").respond()\nproject.set_function(name=\"serving\", func=fn, with_repo=True)\nCustomizing functions#\nOnce you have created your MLRun function, there are many customizations you can add. Some potential customizations include:\nEnvironment variables#\nEnvironment variables can be added individually, from a Python dictionary, or a file:\n# Single variable\nfn.set_env(name=\"MY_ENV\", value=\"MY_VAL\")\n# Multiple variables\nfn.set_envs(env_vars={\"MY_ENV\" : \"MY_VAL\", \"SECOND_ENV\" : \"SECOND_VAL\"})\n# Multiple variables from file\nfn.set_envs(file_path=\"env.txt\")\nMemory, CPU, GPU resources#\nAdding requests and limits to your function specify what compute resources are required. It is best practice to define this for each MLRun function. See CPU, GPU, and memory limits for user jobs for more information on configuring resources.\n# Requests - lower bound\nfn.with_requests(mem=\"1G\", cpu=1)\n# Limits - upper bound\nfn.with_limits(mem=\"2G\", cpu=2, gpus=1)\nScaling and auto-scaling#\nScaling behavior can be added to real-time and distributed runtimes including nuclio, serving, spark, dask, and mpijob. See Replicas to see how to configure scaling behavior per runtime. This example demonstrates setting replicas for nuclio/serving runtimes:\n# Nuclio/serving scaling\nfn.spec.replicas = 2\nfn.spec.min_replicas = 1\nfn.spec.min_replicas = 4\nMount persistent storage#\nIn some instances, you might need to mount a file-system to your container to persist data. This can be done with native K8s PVC’s or the V3IO data layer for Iguazio clusters. See Attach storage to functions for more information on the storage options.\n# Mount persistent storage - V3IO\nfn.apply(mlrun.mount_v3io())\n# Mount persistent storage - PVC\nfn.apply(mlrun.platforms.mount_pvc(pvc_name=\"data-claim\", volume_name=\"data\", volume_mount_path=\"/data\"))\nNode selection#\nNode selection can be used to specify where to run workloads (e.g. specific node groups, instance types, etc.). This is a more advanced parameter mainly used in production deployments to isolate platform services from workloads. See Node affinity for more information on how to configure node selection.\n# Only run on non-spot instances\nfn.with_node_selection(node_selector={\"app.iguazio.com/lifecycle\" : \"non-preemptible\"})"}
{"text": "Running Dask on the cluster with mlrun#\nThe dask frameworks enables users to parallelize their python code and run it as a distributed process on Iguazio cluster and dramatically accelerate their performance.\nIn this notebook you’ll learn how to create a dask cluster and then an mlrun function running as a dask client.\nIt also demonstrates how to run parallelize custom algorithm using Dask Delayed option\nFor more information on dask over kubernetes: https://kubernetes.dask.org/en/latest/\nSet up the environment#\n# set mlrun api path and artifact path for logging\nimport mlrun\nproject_name = \"dask-demo\"\nmlrun.set_environment(project=project_name, artifact_path = './')\n('dask-demo', '/User/dask')\nCreate and Start Dask Cluster#\nDask functions can be local (local workers), or remote (use containers in the cluster), in the case of remote users can specify the number of replica (optional) or leave blank for auto-scale.\nWe use new_function()  to define our Dask cluster and set the desired configuration of that clustered function.\nif the dask workers need to access the shared file system we apply a shared volume mount (e.g. via v3io mount).\nDask function spec have several unique attributes (in addition to the standard job attributes):\n.remote - bool, use local or clustered dask\n.replicas - number of desired replicas, keep 0 for auto-scale\n.min_replicas, .max_replicas - set replicas range for auto-scale\n.scheduler_timeout - cluster will be killed after timeout (inactivity), default is ‘60 minutes’\n.nthreads - number of worker threads\nIf you want to access the dask dashboard or scheduler from remote you need to use NodePort service type (set .service_type to ‘NodePort’), and the external IP need to be specified in mlrun configuration (mlconf.remote_host), this will be set automatically if you are running on an Iguazio cluster.\nWe specify the kind (dask) and the container image\n# create an mlrun function which will init the dask cluster\ndask_cluster_name = \"dask-cluster\"\ndask_cluster = mlrun.new_function(dask_cluster_name, kind='dask', image='mlrun/ml-models')\ndask_cluster.apply(mlrun.mount_v3io())\n<mlrun.runtimes.daskjob.DaskCluster at 0x7f7dbe4166d0>\n# set range for # of replicas with replicas and max_replicas\ndask_cluster.spec.min_replicas = 1\ndask_cluster.spec.max_replicas = 4\n# set the use of dask remote cluster (distributed)\ndask_cluster.spec.remote = True\ndask_cluster.spec.service_type = \"NodePort\"\n# set dask memory and cpu limits\ndask_cluster.with_requests(mem='2G', cpu='2')\nInitialize the Dask Cluster#\nWhen we request the dask cluster client attribute it will verify the cluster is up and running\n# init dask client and use the scheduler address as param in the following cell\ndask_cluster.client\n> 2021-01-24 23:48:54,057 [info] trying dask client at: tcp://mlrun-dask-cluster-b3c6e737-3.default-tenant:8786\n> 2021-01-24 23:48:54,067 [info] using remote dask scheduler (mlrun-dask-cluster-b3c6e737-3) at: tcp://mlrun-dask-cluster-b3c6e737-3.default-tenant:8786\n/User/.pythonlibs/jupyter/lib/python3.7/site-packages/distributed/client.py:1129: VersionMismatchWarning: Mismatched versions found\n+---------+--------+-----------+---------+\n| Package | client | scheduler | workers |\n+---------+--------+-----------+---------+\n| blosc   | 1.7.0  | 1.10.2    | 1.10.2  |\n| lz4     | 3.1.0  | 3.1.3     | 3.1.3   |\n| msgpack | 1.0.0  | 1.0.2     | 1.0.2   |\n| numpy   | 1.19.2 | 1.18.1    | 1.18.1  |\n| toolz   | 0.11.1 | 0.10.0    | 0.10.0  |\n| tornado | 6.0.4  | 6.0.3     | 6.0.3   |\n+---------+--------+-----------+---------+\nNotes:\n-  msgpack: Variation is ok, as long as everything is above 0.6\nwarnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\ndashboard link: default-tenant.app.yh57.iguazio-cd0.com:30433\nClient\nScheduler: tcp://mlrun-dask-cluster-b3c6e737-3.default-tenant:8786\nDashboard: http://mlrun-dask-cluster-b3c6e737-3.default-tenant:8787/status\nCluster\nWorkers: 1\nCores: 1\nMemory: 4.12 GB\nCreating A Function Which Run Over Dask#\n# mlrun: start-code\nImport mlrun and dask. nuclio is used just to convert the code into an mlrun function\nimport mlrun\n%nuclio config kind = \"job\"\n%nuclio config spec.image = \"mlrun/ml-models\"\n%nuclio: setting kind to 'job'\n%nuclio: setting spec.image to 'mlrun/ml-models'\nfrom dask.distributed import Client\nfrom dask import delayed\nfrom dask import dataframe as dd\nimport warnings\nimport numpy as np\nimport os\nimport mlrun\nwarnings.filterwarnings(\"ignore\")\npython function code#\nThis simple function reads a csv file using dask dataframe and run group by and describe function on the dataset and store the results as a dataset artifact\ndef test_dask(context,\ndataset: mlrun.DataItem,\nclient=None,\ndask_function: str=None) -> None:\n# setup dask client from the MLRun dask cluster function\nif dask_function:\nclient = mlrun.import_function(dask_function).client\nelif not client:\nclient = Client()\n# load the dataitem as dask dataframe (dd)\ndf = dataset.as_df(df_module=dd)\n# run describe (get statistics for the dataframe) with dask\ndf_describe = df.describe().compute()\n# run groupby and count using dask\ndf_grpby = df.groupby(\"VendorID\").count().compute()\ncontext.log_dataset(\"describe\",\ndf=df_grpby,\nformat='csv', index=True)\nreturn\n# mlrun: end-code\nTest Our Function Over Dask#\nLoad sample data#\nDATA_URL=\"/User/examples/ytrip.csv\"\n!mkdir -p /User/examples/\n!curl -L \"https://s3.wasabisys.com/iguazio/data/Taxi/yellow_tripdata_2019-01_subset.csv\" > {DATA_URL}\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nDload  Upload   Total   Spent    Left  Speed\n100 84.9M  100 84.9M    0     0  17.3M      0  0:00:04  0:00:04 --:--:-- 19.1M\nConvert the code to MLRun function#\nUse code_to_function to convert the code to MLRun and specify the configuration for the dask process (e.g. replicas, memory etc.)\nNote that the resource configurations are per worker\n# mlrun will transform the code above (up to nuclio: end-code cell) into serverless function\n# which will run in k8s pods\nfn = mlrun.code_to_function(\"test_dask\",  kind='job', handler=\"test_dask\").apply(mlrun.mount_v3io())\nRun the function#\nWhen running the function you would see a link as part of the result. click on this link takes you to the dask monitoring dashboard\n# function URI is db://<project>/<name>\ndask_uri = f'db://{project_name}/{dask_cluster_name}'\nr = fn.run(handler = test_dask,\ninputs={\"dataset\": DATA_URL},\nparams={\"dask_function\": dask_uri})\n> 2021-01-24 23:49:37,858 [info] starting run test-dask-test_dask uid=6410ec27b63e4a12b025696fcabc2dc9 DB=http://mlrun-api:8080\n> 2021-01-24 23:49:38,069 [info] Job is running in the background, pod: test-dask-test-dask-rmgkn\n> 2021-01-24 23:49:41,647 [warning] Unable to parse server or client version. Assuming compatible: {'server_version': 'unstable', 'client_version': 'unstable'}\n> 2021-01-24 23:49:42,112 [info] using in-cluster config.\n> 2021-01-24 23:49:42,113 [info] trying dask client at: tcp://mlrun-dask-cluster-b3c6e737-3.default-tenant:8786\n> 2021-01-24 23:49:42,134 [info] using remote dask scheduler (mlrun-dask-cluster-b3c6e737-3) at: tcp://mlrun-dask-cluster-b3c6e737-3.default-tenant:8786\nremote dashboard: default-tenant.app.yh57.iguazio-cd0.com:30433\n> 2021-01-24 23:49:48,334 [info] run executed, status=completed\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndask-demo\n...cabc2dc9\n0\nJan 24 23:49:42\ncompleted\ntest-dask-test_dask\nv3io_user=adminkind=jobowner=adminhost=test-dask-test-dask-rmgkn\ndataset\ndask_function=db://dask-demo/dask-cluster\ndescribe\nTitle\n×\nto track results use .show() or .logs() or in CLI:\n!mlrun get run 6410ec27b63e4a12b025696fcabc2dc9 --project dask-demo , !mlrun logs 6410ec27b63e4a12b025696fcabc2dc9 --project dask-demo\n> 2021-01-24 23:49:50,284 [info] run executed, status=completed\nTrack the progress in the UI#\nUsers can view the progress and detailed information in the mlrun UI by clicking on the uid above.\nAlso, to track the dask progress in the dask UI click on the “dashboard link” above the “client” section"}
{"text": "Dask distributed runtime#\nQuick Links\nRunning Dask Over MLRun\nPipelines Using Dask, Kubeflow and MLRun\nDask overview#\nSource: Dask docs\nDask is a flexible library for parallel computing in Python.\nDask is composed of two parts:\nDynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\n“Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.\nDask emphasizes the following virtues:\nFamiliar: Provides parallelized NumPy array and Pandas DataFrame objects\nFlexible: Provides a task scheduling interface for more custom workloads and integration with other projects.\nNative: Enables distributed computing in pure Python with access to the PyData stack.\nFast: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\nScales up: Runs resiliently on clusters with 1000s of cores\nScales down: Trivial to set up and run on a laptop in a single process\nResponsive: Designed with interactive computing in mind, it provides rapid feedback and diagnostics to aid humans\nDask collections and schedulers\nDask DataFrame mimics Pandas#\nimport pandas as pd                     import dask.dataframe as dd\ndf = pd.read_csv('2015-01-01.csv')      df = dd.read_csv('2015-*-*.csv')\ndf.groupby(df.user_id).value.mean()     df.groupby(df.user_id).value.mean().compute()\nDask Array mimics NumPy - documentation\nimport numpy as np                       import dask.array as da\nf = h5py.File('myfile.hdf5')             f = h5py.File('myfile.hdf5')\nx = np.array(f['/small-data'])           x = da.from_array(f['/big-data'],\nchunks=(1000, 1000))\nx - x.mean(axis=1)                       x - x.mean(axis=1).compute()\nDask Bag mimics iterators, Toolz, and PySpark - documentation\nimport dask.bag as db\nb = db.read_text('2015-*-*.json.gz').map(json.loads)\nb.pluck('name').frequencies().topk(10, lambda pair: pair[1]).compute()\nDask Delayed mimics for loops and wraps custom code - documentation\nfrom dask import delayed\nL = []\nfor fn in filenames:                  # Use for loops to build up computation\ndata = delayed(load)(fn)          # Delay execution of function\nL.append(delayed(process)(data))  # Build connections between variables\nresult = delayed(summarize)(L)\nresult.compute()\nThe concurrent.futures interface provides general submission of custom tasks: - documentation\nfrom dask.distributed import Client\nclient = Client('scheduler:port')\nfutures = []\nfor fn in filenames:\nfuture = client.submit(load, fn)\nfutures.append(future)\nsummary = client.submit(summarize, futures)\nsummary.result()\nDask.distributed#\nDask.distributed is a lightweight library for distributed computing in Python. It extends both the concurrent.futures and dask APIs to moderate sized clusters.\nMotivation#\nDistributed serves to complement the existing PyData analysis stack. In particular it meets the following needs:\nLow latency: Each task suffers about 1ms of overhead. A small computation and network roundtrip can complete in less than 10ms.\nPeer-to-peer data sharing: Workers communicate with each other to share data. This removes central bottlenecks for data transfer.\nComplex Scheduling: Supports complex workflows (not just map/filter/reduce) which are necessary for sophisticated algorithms used in nd-arrays, machine learning, image processing, and statistics.\nPure Python: Built in Python using well-known technologies. This eases installation, improves efficiency (for Python users), and simplifies debugging.\nData Locality: Scheduling algorithms cleverly execute computations where data lives. This minimizes network traffic and improves efficiency.\nFamiliar APIs: Compatible with the concurrent.futures API in the Python standard library. Compatible with dask API for parallel algorithms\nEasy Setup: As a Pure Python package distributed is pip installable and easy to set up on your own cluster.\nArchitecture#\nDask.distributed is a centrally managed, distributed, dynamic task scheduler. The central dask-scheduler process coordinates the actions of several dask-worker processes spread across multiple machines and the concurrent requests of several clients.\nThe scheduler is asynchronous and event driven, simultaneously responding to requests for computation from multiple clients and tracking the progress of multiple workers. The event-driven and asynchronous nature makes it flexible to concurrently handle a variety of workloads coming from multiple users at the same time while also handling a fluid worker population with failures and additions. Workers communicate amongst each other for bulk data transfer over TCP.\nInternally the scheduler tracks all work as a constantly changing directed acyclic graph of tasks. A task is a Python function operating on Python objects, which can be the results of other tasks. This graph of tasks grows as users submit more computations, fills out as workers complete tasks, and shrinks as users leave or become disinterested in previous results.\nUsers interact by connecting a local Python session to the scheduler and submitting work, either by individual calls to the simple interface client.submit(function, *args, **kwargs) or by using the large data collections and parallel algorithms of the parent dask library. The collections in the dask library like dask.array and dask.dataframe provide easy access to sophisticated algorithms and familiar APIs like NumPy and Pandas, while the simple client.submit interface provides users with custom control when they want to break out of canned “big data” abstractions and submit fully custom workloads.\n~5X Faster with Dask#\nShort example which demonstrates the power of Dask, in this notebook we will preform the following:\nGenerate random text files\nProcess the file by sorting and counting it’s content\nCompare run times\nGenerate random text files#\nimport random\nimport string\nimport os\nfrom collections import Counter\nfrom dask.distributed import Client\nimport warnings\nwarnings.filterwarnings('ignore')\ndef generate_big_random_letters(filename, size):\n\"\"\"\ngenerate big random letters/alphabets to a file\n:param filename: the filename\n:param size: the size in bytes\n:return: void\n\"\"\"\nchars = ''.join([random.choice(string.ascii_letters) for i in range(size)]) #1\nwith open(filename, 'w') as f:\nf.write(chars)\npass\nPATH = '/User/howto/dask/random_files'\nSIZE = 10000000\nfor i in range(100):\ngenerate_big_random_letters(filename = PATH + '/file_' + str(i) + '.txt',\nsize = SIZE)\nSetfunction for benchmark#\ndef count_letters(path):\n\"\"\"\ncount letters in text file\n:param path:  path to file\n\"\"\"\n# open file in read mode\nfile = open(path, \"r\")\n# read the content of file\ndata = file.read()\n# sort file\nsorted_file = sorted(data)\n# count file\nnumber_of_characters = len(sorted_file)\nreturn number_of_characters\ndef process_files(path):\n\"\"\"\nlist file and count letters\n:param path: path to folder with files\n\"\"\"\nnum_list = []\nfiles = os.listdir(path)\nfor file in files:\ncnt = count_letters(os.path.join(path, file))\nnum_list.append(cnt)\nl = num_list\nreturn print(\"done!\")\nSort & count number of letters with Python#\n%%time\nPATH = '/User/howto/dask/random_files/'\nprocess_files(PATH)\ndone!\nCPU times: user 2min 19s, sys: 9.31 s, total: 2min 29s\nWall time: 2min 32s\nSort & count number of letters with Dask#\n# get the dask client address\nclient = Client()\n# list all files in folder\nfiles = [PATH + x for x in os.listdir(PATH)]\n%%time\n# run the count_letter function on a list of files while using multiple workers\na = client.map(count_letters, files)\nCPU times: user 13.2 ms, sys: 983 µs, total: 14.2 ms\nWall time: 12.2 ms\n%%time\n# gather results\nl = client.gather(a)\nCPU times: user 3.39 s, sys: 533 ms, total: 3.92 s\nWall time: 40 s\nAdditional topics#\nRunning Dask on the cluster with mlrun\nPipelines using Dask, Kubeflow and MLRun"}
{"text": "Pipelines using Dask, Kubeflow and MLRun#\nCreate a project to host functions, jobs and artifacts#\nProjects are used to package multiple functions, workflows, and artifacts. Project code and definitions are usually stored in a Git archive.\nThe following code creates a new project in a local dir and initializes git tracking on it.\nimport os\nimport mlrun\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# set project name and dir\nproject_name = 'sk-project-dask'\nproject_dir = './'\n# specify artifacts target location\n_, artifact_path = mlrun.set_environment(project=project_name)\n# set project\nsk_dask_proj = mlrun.get_or_create_project(project_name, project_dir, init_git=True)\n> 2022-09-27 17:26:14,808 [info] loaded project sk-project-dask from MLRun DB\n> 2022-09-27 17:26:14,839 [info] loaded project sk-project-dask from MLRun DB\nInit Dask cluster#\nimport mlrun\n# set up function from local file\ndsf = mlrun.new_function(name=\"mydask\", kind=\"dask\", image=\"mlrun/ml-models\")\n# set up function specs for dask\ndsf.spec.remote = True\ndsf.spec.replicas = 5\ndsf.spec.service_type = 'NodePort'\ndsf.with_limits(mem=\"6G\")\ndsf.spec.nthreads = 5\n# apply mount_v3io over our function so that our k8s pod which run our function\n# will be able to access our data (shared data access)\ndsf.apply(mlrun.mount_v3io())\n<mlrun.runtimes.daskjob.DaskCluster at 0x7f47fce9c850>\ndsf.save()\n'db://sk-project-dask/mydask'\n# init dask cluster\ndsf.client\n> 2022-09-27 17:26:25,134 [info] trying dask client at: tcp://mlrun-mydask-d7df9301-d.default-tenant:8786\n> 2022-09-27 17:26:25,162 [info] using remote dask scheduler (mlrun-mydask-d7df9301-d) at: tcp://mlrun-mydask-d7df9301-d.default-tenant:8786\ndashboard link: default-tenant.app.alexp-edge.lab.iguazeng.com:32472\nClient\nClient-83392da2-3e89-11ed-b7e8-82a5d7054c46\nConnection method: Direct\nDashboard:  http://mlrun-mydask-d7df9301-d.default-tenant:8787/status\nScheduler Info\nScheduler\nScheduler-b8468d53-b900-4041-9982-5e14d5e5eb81\nComm: tcp://10.200.152.178:8786\nWorkers: 0\nDashboard: http://10.200.152.178:8787/status\nTotal threads: 0\nStarted: Just now\nTotal memory: 0 B\nWorkers\nLoad and run a functions#\nLoad the function object from .py .yaml file or Function Hub (marketplace).\n# load function from the Function Hub\nsk_dask_proj.set_function(\"hub://describe\", name=\"describe\")\nsk_dask_proj.set_function(\"hub://sklearn_classifier_dask\", name=\"dask_classifier\")\n<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f48353d5130>\nCreate a fully automated ML pipeline#\nAdd more functions to the project to be used in the pipeline (from the Function Hub)#\nDescribe data, train and eval model with dask.\nDefine and save a pipeline#\nThe following workflow definition will be written into a file. It describes a Kubeflow execution graph (DAG)\nand how functions and data are connected to form an end to end pipeline.\nDescribe data.\nTrain, test and evaluate with dask.\nCheck the code below to see how functions objects are initialized and used (by name) inside the workflow.\nThe workflow.py file has two parts, initialize the function objects and define pipeline dsl (connect the function inputs and outputs).\nNote: The pipeline can include CI steps like building container images and deploying models as illustrated  in the following example.\n%%writefile workflow.py\nimport os\nfrom kfp import dsl\nimport mlrun\n# params\nfuncs = {}\nLABELS = \"label\"\nDROP = \"congestion_surcharge\"\nDATA_URL = mlrun.get_sample_path(\"data/iris/iris_dataset.csv\")\nDASK_CLIENT = \"db://sk-project-dask/mydask\"\n# init functions is used to configure function resources and local settings\ndef init_functions(functions: dict, project=None, secrets=None):\nfor f in functions.values():\nf.apply(mlrun.mount_v3io())\npass\n@dsl.pipeline(name=\"Demo training pipeline\", description=\"Shows how to use mlrun\")\ndef kfpipeline():\n# Describe the data\ndescribe = funcs[\"describe\"].as_step(\ninputs={\"table\": DATA_URL},\nparams={\"dask_function\": DASK_CLIENT},\n)\n# Train, test and evaluate:\ntrain = funcs[\"dask_classifier\"].as_step(\nname=\"train\",\nhandler=\"train_model\",\ninputs={\"dataset\": DATA_URL},\nparams={\n\"label_column\": LABELS,\n\"dask_function\": DASK_CLIENT,\n\"test_size\": 0.10,\n\"model_pkg_class\": \"sklearn.ensemble.RandomForestClassifier\",\n\"drop_cols\": DROP,\n},\noutputs=[\"model\", \"test_set\"],\n)\ntrain.after(describe)\nOverwriting workflow.py\n# register the workflow file as \"main\", embed the workflow code into the project YAML\nsk_dask_proj.set_workflow('main', 'workflow.py', embed=False)\nSave the project definitions to a file (project.yaml). It is recommended to commit all changes to a Git repo.\nsk_dask_proj.save()\n<mlrun.projects.project.MlrunProject at 0x7f48342e4880>\nRun a pipeline workflow#\nUse the run method to execute a workflow. You can provide alternative arguments and specify the default target for workflow artifacts.\nThe workflow ID is returned and can be used to track the progress or you can use the hyperlinks.\nNote: The same command can be issued through CLI commands:\nmlrun project my-proj/ -r main -p \"v3io:///users/admin/mlrun/kfp/{{workflow.uid}}/\"\nThe dirty flag lets you run a project with uncommitted changes (when the notebook is in the same git dir it is always dirty)\nThe watch flag waits for the pipeline to complete and print results.\nartifact_path = os.path.abspath('./pipe/{{workflow.uid}}')\nrun_id = sk_dask_proj.run(\n'main',\narguments={},\nartifact_path=artifact_path,\ndirty=False,\nwatch=True\n)\nPipeline running (id=631ad0a3-19f1-4df0-bfa7-6c38c60275e0), click here to view the details in MLRun UIRun ResultsWorkflow 631ad0a3-19f1-4df0-bfa7-6c38c60275e0 finished, state=Succeededclick the hyper links below to see detailed results\nuid\nstart\nstate\nname\nparameters\nresults\n...25cd95b4\nSep 27 17:27:09\ncompleted\ntrain\nlabel_column=labeldask_function=db://sk-project-dask/mydasktest_size=0.1model_pkg_class=sklearn.ensemble.RandomForestClassifierdrop_cols=congestion_surcharge\nmicro=0.9944598337950138macro=0.9945823158323159precision-0=1.0precision-1=0.9166666666666666precision-2=0.8recall-0=1.0recall-1=0.7857142857142857recall-2=0.9230769230769231f1-0=1.0f1-1=0.8461538461538461f1-2=0.8571428571428571\n...6516a656\nSep 27 17:26:42\ncompleted\ndescribe\ndask_function=db://sk-project-dask/mydask\nback to top"}
{"text": "Attach storage to functions#\nIn the vast majority of cases, an MLRun function requires access to storage. This storage\nmight be used to provide inputs to the function including data-sets to process or data-streams that contain input events.\nTypically, storage is used to store function outputs and result artifacts. For example, trained models or processed\ndata-sets.\nSince MLRun functions can be distributed and executed in Kubernetes pods, the storage used would typically be shared,\nand execution pods would need some added configuration options applied to them so that the function code is able to\naccess the designated storage. These configurations might be k8s volume mounts, specific environment variables that\ncontain configuration and credentials, and other configuration of security settings. These storage\nconfigurations are not applicable to functions running locally in the development environment, since they are executed\nin the local context.\nThe common types of shared storage are:\nv3io storage through API — When running as part of the Iguazio system, MLRun has access to the system’s v3io\nstorage through paths such as v3io:///projects/my_projects/file.csv. To enable this type of access, several\nenvironment variables need to be configured in the pod that provide the v3io API URL and access keys.\nv3io storage through FUSE mount — Some tools cannot utilize the v3io API to access it and need basic filesystem\nsemantics. For that purpose, v3io provides a FUSE (Filesystem in user-space) driver that can be used to mount v3io\ncontainers as specific paths in the pod itself. For example /User. To enable this, several specific volume mount\nconfigurations need to be applied to the pod spec.\nNFS storage access — When MLRun is deployed as open-source, independent of Iguazio, the deployment automatically adds\na pod running NFS storage. To access this NFS storage through pods, a kubernetes pvc mount is needed.\nOthers — As use-cases evolve, other cases of storage access may be needed. This will require various configurations\nto be applied to function execution pods.\nMLRun attempts to offload this storage configuration task from the user by automatically applying the most common\nstorage configuration to functions. As a result, most cases do not require any additional storage configurations\nbefore executing a function as a Kubernetes pod. The configurations applied by MLRun are:\nIn an Iguazio system, apply configurations for v3io access through the API.\nIn an open-source deployment where NFS is configured, apply configurations for pvc access to NFS storage.\nThis MLRun logic is referred to as auto-mount.\nIn this section\nDisabling auto-mount\nModifying the auto-mount default configuration\nDisabling auto-mount#\nIn cases where the default storage configuration does not fit the function needs, MLRun allows for function spec\nmodifiers to be manually applied to functions. These modifiers can add various configurations to the function spec,\nadding environment variables, mounts and additional configurations. MLRun also provides a set of common modifiers\nthat can be used to apply storage configurations.\nThese modifiers can be applied by using the .apply() method on the function and adding the modifier to apply.\nYou can see some examples of this later in this page.\nWhen a different storage configuration is manually applied to a function, MLRun’s auto-mount logic is disabled. This\nprevents conflicts between configurations. The auto-mount logic can also be disabled by setting\nfunc.spec.disable_auto_mount = True\non any MLRun function.\nModifying the auto-mount default configuration#\nThe default auto-mount behavior applied by MLRun is controlled by setting MLRun configuration parameters.\nFor example, the logic can be set to automatically mount the v3io FUSE driver on all functions, or perform pvc\nmount for NFS storage on all functions.\nThe following code demonstrates how to apply the v3io FUSE driver by default:\n# Change MLRun auto-mount configuration\nimport mlrun.mlconf\nmlrun.mlconf.storage.auto_mount_type = \"v3io_fuse\"\nEach of the auto-mount supported methods applies a specific modifier function. The supported methods are:\nv3io_credentials — apply v3io credentials needed for v3io API usage. Applies the\nv3io_cred() modifier.\nv3io_fuse — create Fuse driver mount. Applies the mount_v3io() modifier.\npvc — create a pvc mount. Applies the mount_pvc() modifier.\nauto — the default auto-mount logic as described above (either v3io_credentials or pvc).\nnone — perform no auto-mount (same as using disable_auto_mount = True).\nThe modifier functions executed by auto-mount can be further configured by specifying their parameters. These can be\nprovided in the storage.auto_mount_params configuration parameters. Parameters can be passed as a string made of\nkey=value pairs separated by commas. For example, the following code runs a pvc mount with specific parameters:\nmlrun.mlconf.storage.auto_mount_type = \"pvc\"\npvc_params = {\n\"pvc_name\": \"my_pvc_mount\",\n\"volume_name\": \"pvc_volume\",\n\"volume_mount_path\": \"/mnt/storage/nfs\",\n}\nmlrun.mlconf.storage.auto_mount_params = \",\".join(\n[f\"{key}={value}\" for key, value in pvc_params.items()]\n)\nAlternatively, the parameters can be provided as a base64-encoded JSON object, which can be useful when passing complex\nparameters or strings that contain special characters:\npvc_params_str = base64.b64encode(json.dumps(pvc_params).encode())\nmlrun.mlconf.storage.auto_mount_params = pvc_params_str"}
{"text": "Functions architecture#\nMLRun supports:\nMultiple types of runtimes.\nConfiguring the function resources (replicas, CPU/GPU/memory limits, volumes, Spot vs. On-demand nodes, pod priority, node affinity). See details in Managing job resources.\nIterative tasks for automatic and distributed execution of many tasks with variable parameters (hyperparams). See Hyperparam and iterative jobs.\nHorizontal scaling of functions across multiple containers. See Distributed and Parallel Jobs.\nMLRun has an open public Function Hub that stores many pre-developed functions for\nuse in your projects.\nDistributed functions#\nMany of the runtimes support horizontal scaling. You can specify the number of replicas or the\nmin—max value range (for auto scaling in Dask or Nuclio). When scaling functions, MLRun uses a high-speed\nmessaging protocol and shared storage (volumes, objects, databases, or streams). MLRun runtimes\nhandle the orchestration and monitoring of the distributed task."}
{"text": "Serverless functions#\nAll the executions in MLRun are based on Serverless functions. The functions allow specifying code and\nall the operational aspects (image, required packages, cpu/mem/gpu resources, storage, environment, etc.).\nThe different function runtimes take care of automatically transforming the code and spec to fully\nmanaged and elastic services over Kubernetes, which saves significant operational overhead,\naddresses scalability and reduces infrastructure costs.\nMLRun supports:\nReal-time functions for: serving, APIs, and stream processing (based on the high-performance Nuclio engine).\nBatch functions (based on Kubernetes jobs, Spark, Dask, Horovod, etc.)\nFunction objects are all inclusive (code, spec, API, and metadata definitions), which allows placing them\nin a shared and versioned function market place. This means that different members of the team can produce or\nconsume functions. Each function is versioned and stored in the MLRun database with a unique hash code,\nand gets a new hash code upon changes.\nIn this section\nFunctions architecture\nKinds of functions (runtimes)\nCreate and use functions\nConverting notebooks to function\nAttach storage to functions\nImages and their usage in MLRun\nBuild function image\nNode affinity\nManaging job resources\nFunction Hub"}
{"text": "MPIJob and Horovod runtime#\nRunning distributed workloads#\nTraining a Deep Neural Network is a hard task.  With growing datasets, wider and deeper networks, training our Neural Network can require a lot of resources (CPUs / GPUs / Mem and Time).\nThere are two main reasons why we would like to distribute our Deep Learning workloads:\nModel Parallelism — The Model is too big to fit a single GPU.\nIn this case the model contains too many parameters to hold within a single GPU.\nTo negate this we can use strategies like Parameter Server or slicing the model into slices of consecutive layers which we can fit in a single GPU.\nBoth strategies require Synchronization between the layers held on different GPUs / Parameter Server shards.\nData Parallelism — The Dataset is too big to fit a single GPU.\nUsing methods like Stochastic Gradient Descent we can send batches of data to our models for gradient estimation. This comes at the cost of longer time to converge since the estimated gradient may not fully represent the actual gradient.\nTo increase the likelihood of estimating the actual gradient we could use bigger batches, by sending small batches to different GPUs running the same Neural Network, calculating the batch gradient and then running a Synchronization Step to calculate the average gradient over the batches and update the Neural Networks running on the different GPUs.\nIt is important to understand that the act of distribution adds extra Synchronization Costs which may vary according to your cluster’s configuration.\nAs the gradients and NN needs to be propagated to each GPU in the cluster every epoch (or a number of steps), Networking can become a bottleneck and sometimes different configurations need to be used for optimal performance.\nScaling Efficiency is the metric used to show by how much each additional GPU should benefit the training process with Horovod showing up to 90% (When running with a well written code and good parameters).\nHow can we distribute our training?#\nThere are two different cluster configurations (which can be combined) we need to take into account.\nMulti Node — GPUs are distributed over multiple nodes in the cluster.\nMulti GPU — GPUs are within a single Node.\nIn this demo we show a Multi Node Multi GPU — Data Parallel enabled training using Horovod.\nHowever, you should always try and use the best distribution strategy for your use case (due to the added costs of the distribution itself, ability to run in an optimized way on specific hardware or other considerations that may arise).\nHow Horovod works?#\nHorovod’s primary motivation is to make it easy to take a single-GPU training script and successfully scale it to train across many GPUs in parallel. This has two aspects:\nHow much modification does one have to make to a program to make it distributed, and how easy is it to run it?\nHow much faster would it run in distributed mode?\nHorovod Supports TensorFlow, Keras, PyTorch, and Apache MXNet.\nin MLRun we use Horovod with MPI in order to create cluster resources and allow for optimized networking.\nNote: Horovod and MPI may use NCCL when applicable which may require some specific configuration arguments to run optimally.\nHorovod uses this MPI and NCCL concepts for distributed computation and messaging to quickly and easily synchronize between the different nodes or GPUs.\nHorovod will run your code on all the given nodes (Specific node can be addressed via hvd.rank()) while using an hvd.DistributedOptimizer wrapper to run the synchronization cycles between the copies of your Neural Network running at each node.\nNote: Since all the copies of your Neural Network must be the same, Your workers will adjust themselves to the rate of the slowest worker (simply by waiting for it to finish the epoch and receive its updates). Thus try not to make a specific worker do a lot of additional work on each epoch (Like a lot of saving, extra calculations, etc…) since this can affect the overall training time.\nHow do we integrate TF2 with Horovod?#\nAs it’s one of the main motivations, integration is fairly easy and requires only a few steps: (You can read the full instructions for all the different frameworks on Horovod’s documentation website).\nRun hvd.init().\nPin each GPU to a single process.\nWith the typical setup of one GPU per process, set this to local rank. The first process on the server will be allocated the first GPU, the second process will be allocated the second GPU, and so forth.\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\ntf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\ntf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\nScale the learning rate by the number of workers.\nEffective batch size in synchronous distributed training is scaled by the number of workers. An increase in learning rate compensates for the increased batch size.\nWrap the optimizer in hvd.DistributedOptimizer.\nThe distributed optimizer delegates gradient computation to the original optimizer, averages gradients using allreduce or allgather, and then applies those averaged gradients.\nFor TensorFlow v2, when using a tf.GradientTape, wrap the tape in hvd.DistributedGradientTape instead of wrapping the optimizer.\nBroadcast the initial variable states from rank 0 to all other processes.\nThis is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.\nFor TensorFlow v2, use hvd.broadcast_variables after models and optimizers have been initialized.\nModify your code to save checkpoints only on worker 0 to prevent other workers from corrupting them.\nFor TensorFlow v2, construct a tf.train.Checkpoint and only call checkpoint.save() when hvd.rank() == 0.\nYou can go to Horovod’s Documentation to read more about horovod.\nImage classification use case#\nSee the end to end Image Classification with Distributed Training Demo"}
{"text": "Build function image#\nAs discussed in Images and their usage in MLRun, MLRun provides pre-built images which contain the components necessary to execute\nan MLRun runtime. In some cases, however, custom images need to be created.\nThis page details this process and the available options.\nWhen is a build required?#\nIn many cases an MLRun runtime can be executed without having to build an image. This will be true when\nthe basic MLRun images fulfill all the requirements for the code to execute. It is required to build an image\nif one of the following is true:\nThe code uses additional Python packages, OS packages, scripts or other configurations that need to be applied\nThe code uses different base-images or different versions of MLRun images than provided by default\nExecuted source code has changed, and the image has the code packaged in it - see\nhere for more details on source code, and using\nwith_code() to avoid re-building the image when the code has changed\nThe code runs nuclio functions, which are packaged as images (the build is triggered by MLRun and executed by\nnuclio)\nThe build process in MLRun is based on Kaniko and automated by MLRun -\nMLRun generates the dockerfile for the build process, and configures Kaniko with parameters needed for the build.\nBuilding images is done through functions provided by the MlrunProject class. By using\nproject functions, the same process is used to build and deploy a stand-alone function or functions serving as steps\nin a pipeline.\nAutomatically building images#\nMLRun has the capability to auto-detect when a function image needs to first be built. Following is an example that\nwill require building of the image:\nproject = mlrun.new_project(project_name, \"./proj\")\nproject.set_function(\n\"train_code.py\",\nname=\"trainer\",\nkind=\"job\",\nimage=\"mlrun/mlrun\",\nhandler=\"train_func\",\nrequirements=[\"pandas\"]\n)\n# auto_build will trigger building the image before running,\n# due to the additional requirements.\nproject.run_function(\"trainer\", auto_build=True)\nUsing the auto_build option is only suitable when the build configuration does not change between runs of the\nruntime. For example, if during the development process new requirements were added, the auto_build parameter should\nnot be used, and manual build is needed to re-trigger a build of the image.\nIn the example above, the requirements parameter was used to specify a list of additional Python packages required by\nthe code. This option directly affects the image build process - each requirement is installed using pip as\npart of the docker-build process. The requirements parameter can also contain a path to a requirements file, making\nit easier to reuse an existing configuration rather than specify a list of packages.\nManually building an image#\nTo manually build an image, use the build_function() function, which provides multiple\noptions that control and configure the build process.\nSpecifying base image#\nTo use an existing image as the base image for building the image, set the image name in the base_image parameter.\nNote that this image serves as the base (dockerfile FROM property), and should not to be confused with the\nresulting image name, as specified in the image parameter.\nproject.build_function(\n\"trainer\",\nbase_image=\"myrepo/my_base_image:latest\",\n)\nRunning commands#\nTo run arbitrary commands during the image build, pass them in the commands parameter of\nbuild_function(). For example:\ngithub_repo = \"myusername/myrepo.git@mybranch\"\nproject.build_function(\n\"trainer\",\nbase_image=\"myrepo/base_image:latest\",\ncommands= [\n\"pip install git+https://github.com/\" + github_repo,\n\"mkdir -p /some/path && chmod 0777 /some/path\",\n]\n)\nThese commands are added as RUN operations to the dockerfile generating the image.\nMLRun package deployment#\nThe with_mlrun and mlrun_version_specifier parameters allow control over the inclusion of the MLRun package in the\nbuild process. Depending on the base-image used for the build, the MLRun package may already be available in which\ncase use with_mlrun=False. If not specified, MLRun will attempt to detect this situation - if the image used is one\nof the default MLRun images released with MLRun, with_mlrun is automatically set to False.\nIf the code execution requires a different version of MLRun than the one used to deploy the function,\nset the mlrun_version_specifier to point at the specific version needed. This uses the published MLRun images\nof the specified version instead.\nFor example:\nproject.build_function(\n\"trainer\",\nwith_mlrun=True,\nmlrun_version_specifier=\"1.0.0\"\n)\nWorking with code repository#\nAs the code matures and evolves, the code will usually be stored in a git code repository.\nWhen the MLRun project is associated with a git repo (see Create, save, and use projects for details), functions can be added\nby calling set_function() and setting with_repo=True. This indicates that the\ncode of the function should be retrieved from the project code repository.\nIn this case, the entire code repository will be retrieved from git as part of the image-building process, and cloned\ninto the built image. This is recommended when the function relies on code spread across multiple files and also is\nusually preferred for production code, since it means that the code of the function is stable, and further modifications\nto the code will not cause instability in deployed images.\nDuring the development phase it may be desired to retrieve the code in runtime, rather than re-build the function\nimage every time the code changes. To enable this, use set_source() which\ngets a path to the source (can be a git repository or a tar or zip file) and set pull_at_runtime=True.\nUsing a private Docker registry#\nBy default, images are pushed to the registry configured during MLRun deployment, using the configured registry\ncredentials.\nTo push resulting images to a different registry, specify the registry URL in the image parameter. If\nthe registry requires credentials, create a k8s secret containing these credentials, and pass its name in the\nsecret_name parameter.\nWhen using ECR as registry, MLRun uses Kaniko’s ECR credentials helper, in which case the secret provided should contain\nAWS credentials needed to create ECR repositories, as described here.\nMLRun detects automatically that the registry is an ECR registry based on its URL and configures Kaniko to\nuse the ECR helper. For example:\n# AWS credentials stored in a k8s secret -\n# kubectl create secret generic ecr-credentials --from-file=<path to .aws/credentials>\nproject.build_function(\n\"trainer\",\nimage=\"<aws_account_id>.dkr.ecr.us-east-2.amazonaws.com/myrepo/image:v1\",\nsecret_name=\"ecr-credentials\",\n)\nBuild environment variables#\nIt is possible to pass environment variables that will be set in the Kaniko pod that executes the build. This\nmay be useful to pass important information needed for the build process. The variables are passed as a dictionary in\nthe builder_env parameter, for example:\nproject.build_function(\n...\nbuilder_env={\"GIT_TOKEN\": token},\n)\nDeploying nuclio functions#\nWhen using nuclio functions, the image build process is done by nuclio as part of the deployment of the function.\nMost of the configurations mentioned in this page are available for nuclio functions as well. To deploy a nuclio\nfunction, use deploy_function() instead of using\nbuild_function() and run_function().\nCreating default Spark runtime images#\nWhen using Spark to execute code, either using a Spark service (remote-spark) or the Spark operator, an image is\nrequired that contains both Spark binaries and dependencies, and MLRun code and dependencies.\nThis image is used in the following scenarios:\nFor remote-spark, the image is used to run the initial MLRun code which will submit the Spark job using the\nremote Spark service\nFor Spark operator, the image is used for both the driver and the executor pods used to execute the Spark job\nThis image needs to be created any time a new version of Spark or MLRun is being used, to ensure that jobs are executed\nwith the correct versions of both products.\nTo prepare this image, MLRun provides the following facilities:\n# For remote Spark\nfrom mlrun.runtimes import RemoteSparkRuntime\nRemoteSparkRuntime.deploy_default_image()\n# For Spark operator\nfrom mlrun.runtimes import Spark3Runtime\nSpark3Runtime.deploy_default_image()"}
{"text": "Images and their usage in MLRun#\nEvery release of MLRun includes several images for different usages. The build and the infrastructure images are described, and located, in the README. They are also published to dockerhub and quay.io.\nIn this section\nUsing images\nMLRun images and how to build them\nMLRun images and external docker images\nUsing images#\nSee Kubernetes Jobs & Images.\nMLRun images and how to build them#\nSee README.\nMLRun images and external docker images#\nThere is no difference in the usage between the MLRun images and external docker images. However:\nMLRun images resolve auto tags: If you specify image=\"mlrun/mlrun\" the API fills in the tag by the client version, e.g. changes it to mlrun/mlrun:1.2.0. So, if the client gets upgraded you’ll automatically get a new image tag.\nWhere the data node registry exists, MLRun Appends the registry prefix, so the image loads from the datanode registry. This pulls the image more quickly, and also supports air-gapped sites. When you specify an MLRun image, for example mlrun/mlrun:1.2.0, the actual image used is similar to datanode-registry.iguazio-platform.app.vm/mlrun/mlrun:1.2.0.\nThese characteristics are great when you’re working in a POC or development environment. But MLRun typically upgrades packages as part of the image, and therefore the default MLRun images can break your product flow.\nWorking with images in production#\nFor production you should create your own images to ensure that the image is fixed.\nPin the image tag, e.g. image=\"mlrun/mlrun:1.2.0\". This maintains the image tag at 1.1.0 even when the client is upgraded. Otherwise, an upgrade of the client would also upgrade the image. (If you specify an external (not MLRun images) docker image, like python, the result is the docker/k8s default behavior, which defaults to latest when the tag is not provided.)\nPin the versions of requirements, again to avoid breakages, e.g. pandas==1.4.0. (If you only specify the package name, e.g. pandas, then pip/conda (python’s package managers) just pick up the latest version.)"}
{"text": "Function Hub #\nThis section demonstrates how to import a function from the Hub into your project, and provides some basic instructions on how to run the function and view the results.\nIn this section\nOverview\nFunction Hub\nSearching for functions\nSetting the project configuration\nLoading functions from the hub\nView the function params\nRunning the function\nFunction Hub#\nThe MLRun Function Hub has a wide range of functions that can be used for a variety of use cases.\nThere are functions for ETL, data preparation, training (ML & Deep learning), serving, alerts and notifications and more.\nEach function has a docstring that explains how to use it. In addition, the functions are associated with categories to make it easier for you to find the relevant one.\nFunctions can be easily imported into your project and therefore help you to speed up your development cycle by reusing built-in code.\nSearching for functions#\nThe Function Hub is located here.\nYou can search and filter the categories and kinds to find a function that meets your needs.\nSetting the project configuration#\nThe first step for each project is to set the project name and path:\nfrom os import path, getenv\nfrom mlrun import new_project\nproject_name = 'load-func'\nproject_path = path.abspath('conf')\nproject = new_project(project_name, project_path, init_git=True)\nprint(f'Project path: {project_path}\\nProject name: {project_name}')\nSet the artifacts path  #\nThe artifact path is the default path for saving all the artifacts that the functions generate:\nfrom mlrun import run_local, mlconf, import_function, mount_v3io\n# Target location for storing pipeline artifacts\nartifact_path = path.abspath('jobs')\n# MLRun DB path or API service URL\nmlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\nprint(f'Artifacts path: {artifact_path}\\nMLRun DB path: {mlconf.dbpath}')\nLoading functions from the Hub#\nRun project.set_function to load a functions.\nset_function updates or adds a function object to the project.\nset_function(func, name='', kind='', image=None, with_repo=None)\nParameters:\nfunc — function object or spec/code url.\nname — name of the function (under the project).\nkind — runtime kind e.g. job, nuclio, spark, dask, mpijob. Default: job.\nimage — docker image to be used, can also be specified in the function object/yaml.\nwith_repo — add (clone) the current repo to the build source.\nReturns: project object\nFor more information see the set_function()API documentation.\nLoad function example  #\nThis example loads the describe function. This function analyzes a csv or parquet file for data analysis.\nproject.set_function('hub://describe', 'describe')\nCreate a function object called my_describe:\nmy_describe = project.func('describe')\nView the function params#\nTo view the parameters, run the function with .doc():\nmy_describe.doc()\nfunction: describe\ndescribe and visualizes dataset stats\ndefault handler: summarize\nentry points:\nsummarize: Summarize a table\ncontext(MLClientCtx)  - the function context, default=\ntable(DataItem)  - MLRun input pointing to pandas dataframe (csv/parquet file path), default=\nlabel_column(str)  - ground truth column label, default=None\nclass_labels(List[str])  - label for each class in tables and plots, default=[]\nplot_hist(bool)  - (True) set this to False for large tables, default=True\nplots_dest(str)  - destination folder of summary plots (relative to artifact_path), default=plots\nupdate_dataset  - when the table is a registered dataset update the charts in-place, default=False\nRunning the function#\nUse the run method to to run the function.\nWhen working with functions pay attention to the following:\nInput vs. params — for sending data items to a function, send it via “inputs” and not as params.\nWorking with artifacts — Artifacts from each run are stored in the artifact_path, which can be set globally with the environment variable (MLRUN_ARTIFACT_PATH) or with the config. If it’s not already set you can create a directory and use it in the runs. Using {{run.uid}} in the path creates a unique directory per run. When using pipelines you can use the {{workflow.uid}} template option.\nThis example runs the describe function. This function analyzes a dataset (in this case it’s a csv file) and generates HTML files (e.g. correlation, histogram) and saves them under the artifact path.\nDATA_URL = 'https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv'\nmy_describe.run(name='describe',\ninputs={'table': DATA_URL},\nartifact_path=artifact_path)\nSaving the artifacts in a unique folder for each run  #\nout = mlconf.artifact_path or path.abspath('./data')\nmy_describe.run(name='describe',\ninputs={'table': DATA_URL},\nartifact_path=path.join(out, '{{run.uid}}'))\nViewing the jobs & the artifacts  #\nThere are few options to view the outputs of the jobs we ran:\nIn Jupyter the result of the job is displayed in the Jupyter notebook. When you click on the artifacts it displays its content in Jupyter.\nIn the MLRun UI, under the project name, you can view the job that was running as well as the artifacts it generated."}
{"text": "Working with secrets  #\nWhen executing jobs through MLRun, the code might need access to specific secrets, for example to access data\nresiding on a data-store that requires credentials (such as a private S3 bucket), or many other similar needs.\nMLRun provides some facilities that allow handling secrets and passing those secrets to execution jobs. It’s\nimportant to understand how these facilities work, as this has implications on the level of security they provide\nand how much exposure they create for your secrets.\nIn this section\nOverview\nMLRun-managed secrets\nUsing tasks with secrets\nSecret providers\nKubernetes project secrets\nAzure Vault\nDemo/Development secret providers\nExternally managed secrets\nMapping secrets to environment\nMapping secrets as files\nOverview#\nThere are two main use-cases for providing secrets to an MLRun job. These are:\nUse MLRun-managed secrets. This is a flow that enables the MLRun user (for example a\ndata scientist or engineer) to create and use secrets through interfaces that MLRun implements and manages.\nCreate secrets externally to MLRun using a Kubernetes secret or some other secret\nmanagement framework (such as Azure vault), and utilize these secrets from within MLRun to enrich execution jobs. For\nexample, the secrets are created and managed by an IT admin, and the data-scientist only accesses them.\nThe following sections cover the details of those two use-cases.\nMLRun-managed secrets#\nThe easiest way to pass secrets to MLRun jobs is through the MLRun project secrets mechanism. MLRun jobs automatically\ngain access to all project secrets defined for the same project. More details are available\nlater in this page.\nThe following is an example of using project secrets:\n# Create project secrets for the myproj project\nproject = mlrun.get_or_create_project(\"myproj\", \"./\")\nsecrets = {'AWS_KEY': '111222333'}\nproject.set_secrets(secrets=secrets, provider=\"kubernetes\")\n# Create and run the MLRun job\nfunction = mlrun.code_to_function(\nname=\"secret_func\",\nfilename=\"my_code.py\",\nhandler=\"test_function\",\nkind=\"job\",\nimage=\"mlrun/mlrun\"\n)\nfunction.run()\nThe handler defined in my_code.py accesses the AWS_KEY secret by using the\nget_secret() API:\ndef test_function(context):\ncontext.logger.info(\"running function\")\naws_key = context.get_secret(\"AWS_KEY\")\n# Use aws_key to perform processing.\n...\nUsing tasks with secrets#\nMLRun uses the concept of Tasks to encapsulate runtime parameters. Tasks are used to specify execution context\nsuch as hyper-parameters. They can also be used to pass details about secrets that are going to be used in the\nruntime. This allows for control over specific secrets passed to runtimes, and support for the various MLRun secret\nproviders.\nTo pass secret parameters, use the Task’s with_secrets() function. For example,\nthe following command passes specific project-secrets to the execution context:\nfunction = mlrun.code_to_function(\nname=\"secret_func\",\nfilename=\"my_code.py\",\nhandler=\"test_function\",\nkind=\"job\",\nimage=\"mlrun/mlrun\"\n)\ntask = mlrun.new_task().with_secrets(\"kubernetes\", [\"AWS_KEY\", \"DB_PASSWORD\"])\nrun = function.run(task, ...)\nThe with_secrets() function tells MLRun what secrets the executed code needs to\naccess. The MLRun framework prepares the needed infrastructure to make these secrets available to the runtime,\nand passes information about them to the execution framework by specifying those secrets in the spec of the runtime.\nFor example, if running a kubernetes job, the secret keys are noted in the generated pod’s spec.\nThe actual details of MLRun’s handling of the secrets differ per the secret provider used. The following sections\nprovide more details on these providers and how they handle secrets and their values.\nRegardless of the type of secret provider used, the executed code uses the\nget_secret() API to gain access to the value of the secrets passed to it,\nas shown in the above example.\nSecret providers#\nMLRun provides several secret providers. Each of these providers functions differently and\nhave different traits with respect to what secrets can be passed and how they’re handled. It’s important to understand\nthese parameters to make sure secrets are not compromised and that their secrecy is maintained.\nWarning\nThe Inline, environment and file providers do not guarantee\nconfidentiality of the secret values handled by them, and should only be used for development and demo purposes.\nThe Kubernetes and Azure Vault providers are secure and should be used\nfor any other use-case.\nKubernetes project secrets#\nMLRun can use Kubernetes (k8s) secrets to store and retrieve secret values on a per-project basis. This method\nis supported for all runtimes that generate k8s pods.  MLRun creates a k8s secret per project, and stores\nmultiple secret keys within this secret. Project secrets can be created through the MLRun SDK as well as\nthrough the MLRun UI.\nBy default, all jobs in a project automatically get access to all the associated project secrets. There is\nno need to use with_secrets to provide access to project secrets.\nCreating project secrets#\nTo populate the MLRun k8s project secret with secret values, use the project object’s\nset_secrets() function, which accepts a dictionary of secret values or\na file containing a list of secrets. For example:\n# Create project secrets for the myproj project.\nproject = mlrun.get_or_create_project(\"myproj\", \"./\")\nsecrets = {'password': 'myPassw0rd', 'AWS_KEY': '111222333'}\nproject.set_secrets(secrets=secrets, provider=\"kubernetes\")\nWarning\nThis action should not be part of the code committed to git or part of ongoing execution - it is only a setup\naction, which normally should only be executed once. After the secrets are populated, this code should be removed\nto protect the confidentiality of the secret values.\nThe MLRun API does not allow the user to see project secrets values, but it does allow\nseeing the keys that belong to a given project, assuming the user has permissions on that specific project.\nSee the HTTPRunDB class documentation for additional details.\nWhen MLRun is executed in the Iguazio platform, the secret management APIs are protected by the platform such\nthat only users with permissions to access and modify a specific project can alter its secrets.\nCreating secrets in the Projects UI page#\nThe Settings dialog in the Projects page, accessed with the Settings icon, has a Secrets tab where you can\nadd secrets as key-value pairs. The secrets are automatically available to all jobs belonging to this project.\nUsers with the Editor or Admin role can add, modify, and delete secrets, and assign new secret values.\nViewers can only view the secret keys. The values themselves are not visible to any users.\nAccessing the secrets#\nBy default, any runtime not executed locally (local=False) automatically gains access to all the secrets of the project it\nbelongs to, so no configuration is required to enable that.\nJobs that are executed locally (local=True) do not have access to the project secrets.\nIt is possible to limit access of an executing job to a\nsubset of these secrets by calling the following function with a list of the secrets to be accessed:\ntask.with_secrets('kubernetes', ['password', 'AWS_KEY'])\nWhen the job is executed, the MLRun framework adds environment variables to the pod spec whose value is retrieved\nthrough the k8s valueFrom option, with secretKeyRef pointing at the secret maintained by MLRun.\nAs a result, this method does not expose the secret values at all, except inside the pod executing the code where\nthe secret value is exposed through an environment variable. This means that even a user with kubectl looking at the\npod spec cannot see the secret values.\nUsers, however, can view the secrets using the following methods:\nRun kubectl to view the actual contents of the k8s secret.\nPerform kubectl exec into the running pod, and examine the environment variables.\nTo maintain the confidentiality of secret values, these operations must be strictly limited across the system by using\nk8s RBAC and ensuring that elevated permissions are granted to a very limited number of users (very few users have and\nuse elevated permissions).\nAccessing secrets in nuclio functions#\nNuclio functions do not have the MLRun context available to retrieve secret values. Secret values need to be retrieved\nfrom the environment variable of the same name. For example, to access the AWS_KEY secret in a nuclio function use:\naws_key = os.environ.get(\"AWS_KEY\")\nAzure Vault#\nMLRun can serve secrets from an Azure key Vault.\nNote\nAzure key Vaults support 3 types of entities - keys, secrets and certificates. MLRun only supports accessing\nsecret entities.\nSetting up access to Azure key vault#\nTo enable this functionality, a secret must first be created in the k8s cluster that contains the Azure key Vault\ncredentials. This secret should include credentials providing access to your specific Azure key Vault.\nTo configure this, the following steps are needed:\nSet up a key vault in your Azure subscription.\nCreate a service principal in Azure that will be granted access to the key vault. For creating a service principal\nthrough the Azure portal follow the steps listed in this page.\nAssign a key vault access policy to the service principal, as described in\nthis page.\nCreate a secret access key for the service principal, following the steps listed in this page.\nMake sure you have access to the following three identifiers:\nDirectory (tenant) id\nApplication (client) id\nSecret key\nGenerate a k8s secret with those details. Use the following command:\nkubectl -n <namespace> create secret generic <azure_key_vault_k8s_secret> \\\n--from-literal=secret=<secret key> \\\n--from-literal=tenant_id=<tenant id> \\\n--from-literal=client_id=<client id>\nNote\nThe names of the secret keys must be as shown in the above example, as MLRun queries them by these exact names.\nAccessing Azure key vault secrets#\nOnce these steps are done, use with_secrets in the following manner:\ntask.with_secrets(\n\"azure_vault\",\n{\n\"name\": <azure_key_vault_name>,\n\"k8s_secret\": <azure_key_vault_k8s_secret>,\n\"secrets\": [],\n},\n)\nThe name parameter should point at your Azure key Vault name. The secrets parameter is a list of the secret keys\nto be accessed from that specific vault. If it’s empty (as in the example above) then all secrets in the vault can be\naccessed by their key name.\nFor example, if the Azure Vault has a secret whose name is MY_AZURE_SECRET and using the above example for\nwith_secrets(), the executed code can use the following statement to access\nthis secret:\nazure_secret = context.get_secret(\"MY_AZURE_SECRET\")\nIn terms of confidentiality, the executed pod has the Azure secret provided by the user mounted to it. This means\nthat the access-keys to the vault are visible to a user that execs into the pod in question. The same security\nrules should be followed as described in the Kubernetes section above.\nDemo/Development secret providers#\nThe rest of the MLRun secret providers are not secure by design, and should only be used for demonstration or\ndevelopment purposes.\nExpand here for additional details.\nInline#\nThe inline secrets provider is a very basic framework that should mostly be used for testing and demos. The secrets\npassed by this framework are exposed in the source code creating the MLRun function, as well as in the function spec, and\nin the generated pod specs. To add inline secrets to a job, perform the following:\ntask.with_secrets(\"inline\", {\"MY_SECRET\": \"12345\"})\nAs can be seen, even the client code exposes the secret value. If this is used to pass secrets to a job running in a kubernetes\npod, the secret is also visible in the pod spec. This means that any user that can run kubectl and is permitted\nto view pod specs can also see the secret keys and their values.\nEnvironment#\nEnvironment variables are similar to the inline secrets, but their client-side value is not specified directly in\ncode but rather is extracted from a client-side environment variable. For example, if running MLRun on a Jupyter\nnotebook and there are environment variables named MY_SECRET and ANOTHER_SECRET on Jupyter, the following code\npasses those secrets to the executed runtime:\ntask.with_secrets(\"env\", \"MY_SECRET, ANOTHER_SECRET\")\nWhen generating the runtime execution environment (for example, pod for the job runtime), MLRun retrieves the\nvalue of the environment variable and places it in the pod spec. This means that a user with kubectl capabilities who\ncan see pod specs can still see the secret values passed in this manner.\nFile#\nThe file provider is used to pass secret values that are stored in a local file. The file needs to be made of\nlines, each containing a secret and its value separated by =. For example:\n# secrets.txt\nSECRET1=123456\nSECRET2=abcdef\nUse the following command to add these secrets:\ntask.with_secrets(\"file\", \"/path/to/file/secrets.txt\")\nExternally managed secrets#\nMLRun provides facilities to map k8s secrets that were created externally to jobs that are executed. To enable that,\nthe spec of the runtime that is created should be modified by mounting secrets to it - either as files or as\nenvironment variables containing specific keys from the secret.\nMapping secrets to environment#\nLet’s assume a k8s secret called my-secret was created in the same k8s namespace where MLRun is running, with two\nkeys in it - secret1 and secret2. The following example adds these two secret keys as environment variables\nto an MLRun job:\nfunction = mlrun.code_to_function(\nname=\"secret_func\",\nhandler=\"test_function\",\n...\n)\nfunction.set_env_from_secret(\n\"SECRET_ENV_VAR_1\", secret=\"my-secret\", secret_key=\"secret1\"\n)\nfunction.set_env_from_secret(\n\"SECRET_ENV_VAR_2\", secret=\"my-secret\", secret_key=\"secret2\"\n)\nThis only takes effect for functions executed remotely, as the secret value is injected to the function pod, which does\nnot exist for functions executed locally.\nWithin the function code, the secret values will be exposed as regular environment variables, for example:\n# Function handler\ndef test_function(context):\n# Getting the value in the secret2 key.\nmy_secret_value = os.environ.get(\"SECRET_ENV_VAR_2\")\n...\nMapping secrets as files#\nA k8s secret can be mapped as a filesystem folder to the function pod using the mount_secret()\nfunction:\n# Mount all keys in the secret as files under /mnt/secrets\nfunction.mount_secret(\"my-secret\", \"/mnt/secrets/\")\nThis creates two files in the function pod, called /mnt/secrets/secret1 and /mnt/secrets/secret2. Reading these\nfiles provide the values. It is possible to limit the keys mounted to the function - see the documentation\nof mount_secret() for more details."}
{"text": "Converting notebooks to function#\nMLRun annotations are used to identify the code that needs to be converted into an MLRun function.\nThey provide non-intrusive hints that indicate which parts of your notebook should be considered as the code of the function.\nAnnotations start a code block using # mlrun: start-code and end a code block(s), with # mlrun: end-code.\nUse the #mlrun: ignore to exclude items from the code qualified annotations.\nMake sure that the annotations include anything required for the function to run.\n# mlrun: start-code\ndef sub_handler():\nreturn \"hello world\"\nThe # mlrun: ignore annotation enables you to exclude the cell from the function code.\n# mlrun: ignore\n# the handler in the code section below will not call this sub_handler\ndef sub_handler():\nreturn \"I will be ignored!\"\ndef handler(context, event):\nreturn sub_handler()\n# mlrun: end-code\nConvert the function with mlrun.code_to_function and run the handler. Notice the returned value under results.\nNote\nMake sure to save the notebook before running mlrun.code_to_function so that the lateset changes will be reflected in the function.\nfrom mlrun import code_to_function\nsome_function = code_to_function('some-function-name', kind='job', code_output='.')\nsome_function.run(name='some-function-name', handler='handler', local=True)\n> 2021-11-01 07:42:44,930 [info] starting run some-function-name uid=742e7d6e930c48f3a2f1d6175e971455 DB=http://mlrun-api:8080\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...5e971455\n0\nNov 01 07:42:45\ncompleted\nsome-function-name\nv3io_user=adminkind=owner=adminhost=jupyter-8459699595-z544v\nreturn=hello world\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-11-01 07:42:45,214 [info] run executed, status=completed\n<mlrun.model.RunObject at 0x7f3fc9ed81d0>\nIn this section\nNamed annotations\nMulti section function\nAnnotation’s position in code cell\nGuidelines\nNamed annotations#\nThe # mlrun: start-code and # mlrun: end-code annotations can be used to convert different code sections to different MLRun, functions in the same notebook.\nTo do so add the name of the MLRun function to the end of the annotation as shown in the example below.\n# mlrun: start-code my-function-name\ndef handler(context, event):\nreturn \"hello from my-function\"\n# mlrun: end-code my-function-name\nConvert the function and run the handler. Notice that the handler that is being used and that there is a change in the returned value under results.\nmy_function = code_to_function('my-function-name', kind='job')\nmy_function.run(name='my-function-name', handler='handler', local=True)\n> 2021-11-01 07:42:53,892 [info] starting run my-function-name uid=e4bbc3cae21042439cc1c3cb9631751c DB=http://mlrun-api:8080\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...9631751c\n0\nNov 01 07:42:54\ncompleted\nmy-function-name\nv3io_user=adminkind=owner=adminhost=jupyter-8459699595-z544v\nreturn=hello from my-function\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-11-01 07:42:54,137 [info] run executed, status=completed\n<mlrun.model.RunObject at 0x7f3fc9ac71d0>\nNote\nMake sure to use the name given to the code_to_function parameter (name='my-function-name' in the example above) so that all relevant start-code and end-code annotations are included. If none of the annotations are marked with the function’s name, all annotations without any name are used.\nMulti section function#\nYou can use the # mlrun: start-code and # mlrun: end-code annotations multiple times in a notebook since the whole notebook is scanned.\nThe annotations can be named like the following example, and they can be nameless. If you choose nameless, remember all nameless annotations in the notebook are used.\n# mlrun: start-code multi-section-function-name\nfunction_name = \"multi-section-function-name\"\n# mlrun: end-code multi-section-function-name\nAny code between those sections are not included:\nfunction_name = \"I will be ignored!\"\n# mlrun: start-code multi-section-function-name\ndef handler(context, event):\nreturn f\"hello from {function_name}\"\n# mlrun: end-code multi-section-function-name\nmy_multi_section_function = code_to_function('multi-section-function-name', kind='job')\nmy_multi_section_function.run(name='multi-section-function-name', handler='handler', local=True)\n> 2021-11-01 07:43:05,587 [info] starting run multi-section-function-name uid=9ac6a0e977a54980b657bae067c2242a DB=http://mlrun-api:8080\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...67c2242a\n0\nNov 01 07:43:05\ncompleted\nmulti-section-function-name\nv3io_user=adminkind=owner=adminhost=jupyter-8459699595-z544v\nreturn=hello from multi-section-function-name\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-11-01 07:43:05,834 [info] run executed, status=completed\n<mlrun.model.RunObject at 0x7f3fc9a24e10>\nAnnotation’s position in code cell#\n# mlrun: start-code and # mlrun: end-code annotations are relative to their positions inside the code block. Notice how the assignments to function_name below # mlrun: end-code don’t override the assignment between the annotations in the function’s context.\n# mlrun: start-code part-cell-function\ndef handler(context, event):\nreturn f\"hello from {function_name}\"\nfunction_name = \"part-cell-function\"\n# mlrun: end-code part-cell-function\nfunction_name = \"I will be ignored\"\nmy_multi_section_function = code_to_function('part-cell-function', kind='job')\nmy_multi_section_function.run(name='part-cell-function', handler='handler', local=True)\n> 2021-11-01 07:43:14,347 [info] starting run part-cell-function uid=5426e665c7bc4ba492e0a704c5555fb6 DB=http://mlrun-api:8080\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...c5555fb6\n0\nNov 01 07:43:14\ncompleted\npart-cell-function\nv3io_user=adminkind=owner=adminhost=jupyter-8459699595-z544v\nreturn=hello from part-cell-function\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2021-11-01 07:43:14,628 [info] run executed, status=completed\n<mlrun.model.RunObject at 0x7f3fc9a2bf50>\nGuidelines#\nMake sure that every # mlrun: start-code has a corresponding # mlrun: end-code before the next # mlrun: start-code in the notebook.\nOnly one MLRun function can have a nameless annotation per notebook.\nDo not use multiple # mlrun: start-code nor multiple # mlrun: end-code annotations in a single code cell. Only the first appearance of each is used.\nUsing single annotations:\nUse a # mlrun: start-code alone, and all code blocks from the annotation to the end of the notebook are included.\nUse a # mlrun: end-code alone, and all code blocks from the beginning of the notebook to the annotation are included."}
{"text": "Spark Operator runtime#\nUsing Spark Operator for running Spark jobs over k8s.\nThe spark-on-k8s-operator allows Spark applications to be defined in a declarative manner and supports one-time Spark\napplications with SparkApplication and cron-scheduled applications with ScheduledSparkApplication.\nWhen sending a request with MLRun to the Spark operator, the request contains your full application configuration including the\ncode and dependencies to run (packaged as a docker image or specified via URIs), the infrastructure parameters, (e.g. the\nmemory, CPU, and storage volume specs to allocate to each Spark executor), and the Spark configuration.\nKubernetes takes this request and starts the Spark driver in a Kubernetes pod (a k8s abstraction, just a docker container\nin this case). The Spark driver then communicates directly with the Kubernetes master to request executor pods, scaling them\nup and down at runtime according to the load if dynamic allocation is enabled. Kubernetes takes care of the bin-packing of\nthe pods onto Kubernetes nodes (the physical VMs), and dynamically scales the various node pools to meet the requirements.\nWhen using Spark operator the resources are allocated per task, meaning that it scales down to zero when the task is done.\nimport mlrun\nimport os\n# set up new spark function with spark operator\n# command will use our spark code which needs to be located on our file system\n# the name param can have only non capital letters (k8s convention)\nread_csv_filepath = os.path.join(os.path.abspath('.'), 'spark_read_csv.py')\nsj = mlrun.new_function(kind='spark', command=read_csv_filepath, name='sparkreadcsv')\n# set spark driver config (gpu_type & gpus=<number_of_gpus>  supported too)\nsj.with_driver_limits(cpu=\"1300m\")\nsj.with_driver_requests(cpu=1, mem=\"512m\")\n# set spark executor config (gpu_type & gpus=<number_of_gpus> are supported too)\nsj.with_executor_limits(cpu=\"1400m\")\nsj.with_executor_requests(cpu=1, mem=\"512m\")\n# adds fuse, daemon & iguazio's jars support\nsj.with_igz_spark()\n# Alternately, move volume_mounts to driver and executor-specific fields and leave\n# v3io mounts out of executor mounts if mount_v3io_to_executor=False\n# sj.with_igz_spark(mount_v3io_to_executor=False)\n# set spark driver volume mount\n# sj.function.with_driver_host_path_volume(\"/host/path\", \"/mount/path\")\n# set spark executor volume mount\n# sj.function.with_executor_host_path_volume(\"/host/path\", \"/mount/path\")\n# confs are also supported\nsj.spec.spark_conf['spark.eventLog.enabled'] = True\n# add python module\nsj.spec.build.commands = ['pip install matplotlib']\n# Number of executors\nsj.spec.replicas = 2\n# Rebuilds the image with MLRun - needed in order to support artifactlogging etc\nsj.deploy()\n# Run task while setting the artifact path on which our run artifact (in any) will be saved\nsj.run(artifact_path='/User')\nSpark Code (spark_read_csv.py)#\nfrom pyspark.sql import SparkSession\nfrom mlrun import get_or_create_ctx\ncontext = get_or_create_ctx(\"spark-function\")\n# build spark session\nspark = SparkSession.builder.appName(\"Spark job\").getOrCreate()\n# read csv\ndf = spark.read.load('iris.csv', format=\"csv\",\nsep=\",\", header=\"true\")\n# sample for logging\ndf_to_log = df.describe().toPandas()\n# log final report\ncontext.log_dataset(\"df_sample\",\ndf=df_to_log,\nformat=\"csv\")\nspark.stop()"}
{"text": "Built-in steps#\nMlRun provides you with many built-in steps that you can use when building your graph.\nClick on the step names in the following sections to see the full usage.\nBase Operators\nData Transformations\nExternal IO and data enrichment\nSources\nTargets\nModels\nRouters\nOther\nBase Operators#\nClass name\nDescription\nstorey.transformations.Batch\nBatches events. This step emits a batch every max_events events, or when timeout seconds have passed since the first event in the batch was received.\nstorey.transformations.Choice\nRedirects each input element into one of the multiple downstreams.\nstorey.Extend\nAdds fields to each incoming event.\nstorey.transformations.Filter\nFilters events based on a user-provided function.\nstorey.transformations.FlatMap\nMaps, or transforms, each incoming event into any number of events.\nstorey.steps.Flatten\nFlatten is equivalent to FlatMap(lambda x: x).\nstorey.transformations.ForEach\nApplies the given function on each event in the stream, and passes the original event downstream.\nstorey.transformations.MapClass\nSimilar to Map, but instead of a function argument, this class should be extended and its do() method overridden.\nstorey.transformations.MapWithState\nMaps, or transforms, incoming events using a stateful user-provided function, and an initial state, which can be a database table.\nstorey.transformations.Partition\nPartitions events by calling a predicate function on each event. Each processed event results in a Partitioned namedtuple of (left=Optional[Event], right=Optional[Event]).\nstorey.Reduce\nReduces incoming events into a single value that is returned upon the successful termination of the flow.\nstorey.transformations.SampleWindow\nEmits a single event in a window of window_size events, in accordance with emit_period and emit_before_termination.\nData Transformations#\nClass name\nDescription\nstorey.AggregateByKey\nAggregates the data into the table object provided for later persistence, and outputs an event enriched with the requested aggregation features.\nDateExtractor\nExtract a date-time component.\nmlrun.feature_store.Imputer\nReplace None values with default values.\nmlrun.feature_store.MapValues\nMap column values to new values.\nmlrun.feature_store.OneHotEncoder\nCreate new binary fields, one per category (one hot encoded).\nmlrun.feature_store.SetEventMetadata\nSet the event metadata (id, key, timestamp) from the event body.\nExternal IO and data enrichment#\nClass name\nDescription\nBatchHttpRequests\nA class for calling remote endpoints in parallel.\nmlrun.datastore.DataItem\nData input/output class abstracting access to various local/remote data sources.\nstorey.transformations.JoinWithTable\nJoins each event with data from the given table.\nJoinWithV3IOTable\nJoins each event with a V3IO table. Used for event augmentation.\nQueryByKey\nSimilar to to AggregateByKey, but this step is for serving only and does not aggregate the event.\nRemoteStep\nClass for calling remote endpoints.\nstorey.transformations.SendToHttp\nJoins each event with data from any HTTP source. Used for event augmentation.\nSources#\nClass name\nDescription\nmlrun.datastore.BigQuerySource\nReads Google BigQuery query results as input source for a flow.\nmlrun.datastore.CSVSource\nReads a CSV file as input source for a flow.\nDataframeSource\nReads data frame as input source for a flow.\nmlrun.datastore.HttpSource\nmlrun.datastore.KafkaSource\nSets the kafka source for the flow.\nmlrun.datastore.ParquetSource\nReads the Parquet file/dir as the input source for a flow.\nmlrun.datastore.StreamSource\nSets the stream source for the flow. If the stream doesn’t exist it creates it.\nTargets#\nClass name\nDescription\nmlrun.datastore.CSVTarget\nWrites events to a CSV file.\nmlrun.datastore.NoSqlTarget\nPersists the data in table to its associated storage by key.\nmlrun.datastore.ParquetTarget\nThe Parquet target storage driver, used to materialize feature set/vector data into parquet files.\nmlrun.datastore.StreamTarget\nWrites all incoming events into a V3IO stream.\nstorey.transformations.ToDataFrame\nCreate pandas data frame from events. Can appear in the middle of the flow, as opposed to ReduceToDataFrame.\nmlrun.datastore.TSBDTarget\nModels#\nClass name\nDescription\nmlrun.frameworks.onnx.ONNXModelServer\nA model serving class for serving ONYX Models. A sub-class of the  V2ModelServer class.\nmlrun.frameworks.pytorch.PyTorchModelServer\nA model serving class for serving PyTorch Models. A sub-class of the  V2ModelServer class.\nmlrun.frameworks.sklearn.SklearnModelServer\nA model serving class for serving Sklearn Models. A sub-class of the  V2ModelServer class.\nmlrun.frameworks.tf_keras.TFKerasModelServer\nA model serving class for serving TFKeras Models. A sub-class of the V2ModelServer class.\nmlrun.frameworks.xgboost.XGBModelServer\nA model serving class for serving XGB Models. A sub-class of the  V2ModelServer class.\nRouters#\nClass name\nDescription\nmlrun.serving.EnrichmentModelRouter\nAuto enrich the request with data from the feature store. The router input accepts a list of inference requests (each request can be a dict or a list of incoming features/keys). It enriches the request with data from the specified feature vector (feature_vector_uri).\nmlrun.serving.EnrichmentVotingEnsemble\nAuto enrich the request with data from the feature store. The router input accepts a list of inference requests (each request can be a dict or a list of incoming features/keys). It enriches the request with data from the specified feature vector (feature_vector_uri).\nmlrun.serving.ModelRouter\nBasic model router, for calling different models per each model path.\nmlrun.serving.VotingEnsemble\nAn ensemble machine learning model that combines the prediction of several models.\nOther#\nClass name\nDescription\nmlrun.feature_store.FeaturesetValidator\nValidate feature values according to the feature set validation policy.\nReduceToDataFrame\nBuilds a pandas DataFrame from events and returns that DataFrame on flow termination."}
{"text": "Using built-in model serving classes#\nMLRun includes built-in classes for commonly used frameworks. While you can create your own class, it is often not necessary to write one if you use these standard classes.\nThe following table specifies, for each framework, the relevant pre-integrated image and the corresponding MLRun ModelServer serving class:\nframework\nimage\nserving class\nScikit-learn\nmlrun/mlrun\nmlrun.frameworks.sklearn.SklearnModelServer\nTensorFlow.Keras\nmlrun/ml-models\nmlrun.frameworks.tf_keras.TFKerasModelServer\nONNX\nmlrun/ml-models\nmlrun.frameworks.onnx.ONNXModelServer\nXGBoost\nmlrun/ml-models\nmlrun.frameworks.xgboost.XGBoostModelServer\nLightGBM\nmlrun/ml-models\nmlrun.frameworks.lgbm.LGBMModelServer\nPyTorch\nmlrun/ml-models\nmlrun.frameworks.pytorch.PyTorchModelServer\nFor GPU support, use the mlrun/ml-models-gpu image (adding GPU drivers and support).\nExample#\nThe following code shows how to create a basic serving model using Scikit-learn.\nimport os\nimport urllib.request\nimport mlrun\nmodel_path = os.path.abspath('sklearn.pkl')\n# Download the model file locally\nurllib.request.urlretrieve(mlrun.get_sample_path('models/serving/sklearn.pkl'), model_path)\n# Set the base project name\nproject_name_base = 'serving-test'\n# Initialize the MLRun project object\nproject = mlrun.get_or_create_project(project_name_base, context=\"./\", user_project=True)\nserving_function_image = \"mlrun/mlrun\"\nserving_model_class_name = \"mlrun.frameworks.sklearn.SklearnModelServer\"\n# Create a serving function\nserving_fn = mlrun.new_function(\"serving\", project=project.name, kind=\"serving\", image=serving_function_image)\n# Add a model, the model key can be anything we choose. The class will be the built-in scikit-learn model server class\nmodel_key = \"scikit-learn\"\nserving_fn.add_model(key=model_key,\nmodel_path=model_path,\nclass_name=serving_model_class_name)\nAfter the serving function is created, you can test it:\n# Test data to send\nmy_data = {\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}\n# Create a mock server in order to test the model\nmock_server = serving_fn.to_mock_server()\n# Test the serving function\nmock_server.test(f\"/v2/models/{model_key}/infer\", body=my_data)\nSimilarly, you can deploy the serving function and test it with some data:\nserving_fn.with_code(body=\" \") # Workaround, required only for mlrun <= 1.0.2\n# Deploy the serving function\nserving_fn.apply(mlrun.auto_mount()).deploy()\n# Check the result using the deployed serving function\nserving_fn.invoke(path=f'/v2/models/{model_key}/infer',body=my_data)"}
{"text": "Canary and rolling upgrades#\nNote\nRelevant when MLRun is executed in the Iguazio platform (“the platform”).\nCanary rollout is a known practice to first test a software update on a small number of users before rolling it\nout to all users. In machine learning, the main usage is to test a new model on a small subset of users before\nrolling it out to all users.\nCanary functions are defined using an API gateway. The API gateway is a service that exposes your function as a\nweb service. Essentially, it is a proxy that forwards requests to your functions and returns the response.\nYou can configure authentication on the gateway.\nThe API traffic is randomly directed to the two functions at the percentages you specify. Start with a low\npercentage for the canary function. Verify that the canary function works as expected (or modify it until it does\nwork as desired). Then gradually increase its percentage until you turn it into a production function.\nIn this section\nCreate an API gateway\nCreate and use a canary function\nCreate an API gateway#\nTo create an API gateway in the UI:\nIn your project page, press API Gateways tab, then press NEW API GATEWAY.\nSelect an Authentication Mode:\nNone (default)\nBasic\nAccess key\nOAuth2\nand fill in any required values.\nType in the API Gateway parameters:\nName: The name of the API Gateway. Required\nDescription: A description of the API Gateway.\nHost: The host of the API Gateway. (Relevant for open-source only.)\nPath: The path of the API Gateway.\nIn Primary, type in the function that is triggered via the API Gateway.\nCreate and use a canary function#\nPress Create a canary function and type in the function name.\nLeave the percentages at 5% and 95% to get started, and verify that the canary function works as expected.\nGradually increase the percentage, each time verifying its results.\nWhen the percentage is high and you are fully satisfied, turn it into a production function by pressing   > Promote."}
{"text": "Build your own model serving class#\nModel serving classes implement the full model serving functionality, which includes\nloading models, pre- and post-processing, prediction, explainability, and model monitoring.\nModel serving classes must inherit from mlrun.serving.V2ModelServer, and at the minimum\nimplement the load() (download the model file(s) and load the model into memory)\nand predict() (accept request payload and return prediction/inference results) methods.\nThe class is initialized automatically by the model server and can run locally\nas part of a nuclio serverless function, or as part of a real-time pipeline.\nYou need to implement two mandatory methods:\nload() — download the model file(s) and load the model into memory,\nnote this can be done synchronously or asynchronously.\npredict() — accept request payload and return prediction/inference results.\nYou can override additional methods : preprocess, validate, postprocess, explain.\nYou can add a custom api endpoint by adding the method op_xx(event). Invoke it by\ncalling the /xx (operation = xx).\nIn this section\nMinimal sklearn serving function example\nload() method\npredict() method\nexplain() method\npre/post and validate hooks\nModels, routers and graphs\nCreating a model serving function (service)\nModel monitoring\nMinimal sklearn serving function example#\nfrom cloudpickle import load\nimport numpy as np\nimport mlrun\nclass ClassifierModel(mlrun.serving.V2ModelServer):\ndef load(self):\n\"\"\"load and initialize the model and/or other elements\"\"\"\nmodel_file, extra_data = self.get_model('.pkl')\nself.model = load(open(model_file, 'rb'))\ndef predict(self, body: dict) -> list:\n\"\"\"Generate model predictions from sample\"\"\"\nfeats = np.asarray(body['inputs'])\nresult: np.ndarray = self.model.predict(feats)\nreturn result.tolist()\nTest the function locally using the mock server:\nimport mlrun\nfrom sklearn.datasets import load_iris\nfn = mlrun.new_function('my_server', kind='serving')\n# set the topology/router and add models\ngraph = fn.set_topology(\"router\")\nfn.add_model(\"model1\", class_name=\"ClassifierModel\", model_path=\"<path1>\")\nfn.add_model(\"model2\", class_name=\"ClassifierModel\", model_path=\"<path2>\")\n# create and use the graph simulator\nserver = fn.to_mock_server()\nx = load_iris()['data'].tolist()\nresult = server.test(\"/v2/models/model1/infer\", {\"inputs\": x})\nload() method#\nIn the load method, download the model from external store, run the algorithm/framework\nload() call, and do any other initialization logic.\nThe load runs synchronously (the deploy is stalled until load completes).\nThis can be an issue for large models and cause a readiness timeout. You can increase the\nfunction spec.readiness_timeout, or alternatively choose async loading (load ()\nruns in the background) by setting the function spec.load_mode = \"async\".\nThe function self.get_model() downloads the model metadata object and main file (into model_file path).\nAdditional files can be accessed using the returned extra_data (dict of dataitem objects).\nThe model metadata object is stored in self.model_spec and provides model parameters, metrics, schema, etc.\nParameters can be accessed using self.get_param(key). The parameters can be specified in the model or during\nthe function/model deployment.\npredict() method#\nThe predict method is called when you access the /infer or /predict url suffix (operation).\nThe method accepts the request object (as dict), see Model server API.\nAnd it should return the specified response object.\nexplain() method#\nThe explain method provides a hook for model explainability, and is accessed using the /explain operation.\npre/post and validate hooks#\nYou can overwrite the preprocess, validate, and postprocess methods for additional control\nThe call flow is:\npre-process -> validate -> predict/explain -> post-process\nModels, routers and graphs#\nEvery serving function can host multiple models and logical steps. Multiple functions\ncan connect in a graph to form complex real-time pipelines.\nThe basic serving function has a logical router with routes to multiple child models.\nThe url or the message determines which model is selected, e.g. using the url schema:\n/v2/models/<model>[/versions/<ver>]/operation\nNote\nThe model, version and operation can also be specified in the message body\nto support streaming protocols (e.g. Kafka).\nMore complex routers can be used to support ensembles (send the request to all child models\nand aggregate the result), multi-armed-bandit, etc.\nYou can use a pre-defined Router class, or write your own custom router.\nRoutera can route to models on the same function or access models on a separate function.\nTo specify the topology, router class and class args use .set_topology() with your function.\nCreating a model serving function (service)#\nTo provision a serving function, you need to create an MLRun function of type serving.\nThis can be done by using the code_to_function() call from a notebook. You can also import\nan existing serving function/template from the Function Hub.\nExample (run inside a notebook): this code converts a notebook to a serving function and adding a model to it:\nfrom mlrun import code_to_function\nfn = code_to_function('my-function', kind='serving')\nfn.add_model('m1', model_path=<model-artifact/dir>, class_name='MyClass', x=100)\nSee .add_model() docstring for help and parameters.\nSee the full Model Server example.\nIf you want to use multiple versions for the same model, use : to separate the name from the version.\nFor example, if the name is mymodel:v2 it means model name mymodel version v2.\nYou should specify the model_path (url of the model artifact/dir) and the class_name name\n(or class module.submodule.class). Alternatively, you can set the model_url for calling a\nmodel that is served by another function (can be used for ensembles).\nThe function object(fn) accepts many options. You can specify replicas range (auto-scaling), cpu/gpu/mem resources, add shared\nvolume mounts, secrets, and any other Kubernetes resource through the fn.spec object or fn methods.\nFor example, fn.gpu(1) means each replica uses one GPU.\nTo deploy a model, simply call:\nfn.deploy()\nYou can also deploy a model from within an ML pipeline (check the various demos for details).\nModel monitoring#\nModel activities can be tracked into a real-time stream and time-series DB. The monitoring data\nis used to create real-time dashboards and track model accuracy and drift.\nTo set the tracking stream options, specify the following function spec attributes:\nfn.set_tracking(stream_path, batch, sample)\nstream_path — the v3io stream path (e.g. v3io:///users/..)\nsample — optional, sample every N requests\nbatch — optional, send micro-batches every N requests"}
{"text": "Demos and tutorials#\nRead these tutorials to get an even better understanding of serving graphs.\nDistributed (multi-function) pipeline example\nAdvanced model serving graph - notebook example\nSee the MLRun demos repository for additional use cases and full end-to-end examples, including Fraud Prevention using the Iguazio feature store, a mask detection demo, and converting existing ML code to an MLRun project."}
{"text": "Distributed (multi-function) pipeline example#\nThis example demonstrates how to run a pipeline that consists of multiple serverless functions (connected using streams).\nIn the pipeline example the request contains the a URL of a file. It loads the content of the file and breaks it into paragraphs (using the FlatMap class), and pushes the results to a queue/stream. The second function picks up the paragraphs and runs the NLP flow to extract the entities and push the results to the output stream.\nSetting the stream URLs for the internal queue, the final output and error/exceptions stream:\nstreams_prefix = \"v3io:///users/admin/\"\ninternal_stream = streams_prefix + \"in-stream\"\nout_stream = streams_prefix + \"out-stream\"\nerr_stream = streams_prefix + \"err-stream\"\nAlternatively, using Kafka:\nkafka_prefix = f\"kafka://{broker}/\"\ninternal_topic = kafka_prefix + \"in-topic\"\nout_topic = kafka_prefix + \"out-topic\"\nerr_topic = kafka_prefix + \"err-topic\"\nIn either case, continue with:\n# set up the environment\nimport mlrun\nmlrun.set_environment(project=\"pipe\")\n> 2021-05-03 14:28:39,987 [warning] Failed resolving version info. Ignoring and using defaults\n> 2021-05-03 14:28:43,801 [warning] Unable to parse server or client version. Assuming compatible: {'server_version': '0.6.3-rc4', 'client_version': 'unstable'}\n('pipe', '/v3io/projects/{{run.project}}/artifacts')\n# uncomment to install spacy requirements locally\n# !pip install spacy\n# !python -m spacy download en_core_web_sm\nIn this example\nCreate the pipeline\nTest the pipeline locally\nDeploy to the cluster\nCreate the pipeline#\nThe pipeline consists of two functions: data-prep and NLP. Each one has different package dependencies.\nCreate a file with data-prep graph steps:\n%%writefile data_prep.py\nimport mlrun\nimport json\n# load struct from a json file (event points to the url)\ndef load_url(event):\nurl = event[\"url\"]\ndata = mlrun.get_object(url).decode(\"utf-8\")\nreturn {\"url\": url, \"doc\": json.loads(data)}\ndef to_paragraphs(event):\nparagraphs = []\nurl = event[\"url\"]\nfor i, paragraph in enumerate(event[\"doc\"]):\nparagraphs.append(\n{\"url\": url, \"paragraph_id\": i, \"paragraph\": paragraph}\n)\nreturn paragraphs\nOverwriting data_prep.py\nCreate a file with NLP graph steps (use spacy):\n%%writefile nlp.py\nimport json\nimport spacy\ndef myprint(x):\nprint(x)\nreturn x\nclass ApplyNLP:\ndef __init__(self, context=None, spacy_dict=\"en_core_web_sm\"):\nself.nlp = spacy.load(spacy_dict)\ndef do(self, paragraph: dict):\ntokenized_paragraphs = []\nif isinstance(paragraph, (str, bytes)):\nparagraph = json.loads(paragraph)\ntokenized = {\n\"url\": paragraph[\"url\"],\n\"paragraph_id\": paragraph[\"paragraph_id\"],\n\"tokens\": self.nlp(paragraph[\"paragraph\"]),\n}\ntokenized_paragraphs.append(tokenized)\nreturn tokenized_paragraphs\ndef extract_entities(tokens):\nparagraph_entities = []\nfor token in tokens:\nentities = token[\"tokens\"].ents\nfor entity in entities:\nparagraph_entities.append(\n{\n\"url\": token[\"url\"],\n\"paragraph_id\": token[\"paragraph_id\"],\n\"entity\": entity.ents,\n}\n)\nreturn paragraph_entities\ndef enrich_entities(entities):\nenriched_entities = []\nfor entity in entities:\nenriched_entities.append(\n{\n\"url\": entity[\"url\"],\n\"paragraph_id\": entity[\"paragraph_id\"],\n\"entity_text\": entity[\"entity\"][0].text,\n\"entity_start_char\": entity[\"entity\"][0].start_char,\n\"entity_end_char\": entity[\"entity\"][0].end_char,\n\"entity_label\": entity[\"entity\"][0].label_,\n}\n)\nreturn enriched_entities\nOverwriting nlp.py\nBuild and show the graph:\nCreate the master function (“multi-func”) with the data_prep.py source and an async graph topology.\nAdd a pipeline of steps made of custom python handlers, classes and built-in classes (like storey.FlatMap).\nThe pipeline runs across two functions which are connected by a queue/stream (q1). Use the function= to specify which function runs the specified step.\nEnd the flow with writing to the output stream.\n# define a new real-time serving function (from code) with an async graph\nfn = mlrun.code_to_function(\"multi-func\", filename=\"./data_prep.py\", kind=\"serving\", image='mlrun/mlrun')\ngraph = fn.set_topology(\"flow\", engine=\"async\")\n# define the graph steps (DAG)\ngraph.to(name=\"load_url\", handler=\"load_url\")\\\n.to(name=\"to_paragraphs\", handler=\"to_paragraphs\")\\\n.to(\"storey.FlatMap\", \"flatten_paragraphs\", _fn=\"(event)\")\\\n.to(\">>\", \"q1\", path=internal_stream)\\\n.to(name=\"nlp\", class_name=\"ApplyNLP\", function=\"enrich\")\\\n.to(name=\"extract_entities\", handler=\"extract_entities\", function=\"enrich\")\\\n.to(name=\"enrich_entities\", handler=\"enrich_entities\", function=\"enrich\")\\\n.to(\"storey.FlatMap\", \"flatten_entities\", _fn=\"(event)\", function=\"enrich\")\\\n.to(name=\"printer\", handler=\"myprint\", function=\"enrich\")\\\n.to(\">>\", \"output_stream\", path=out_stream)\n<mlrun.serving.states.QueueState at 0x7f9e618f9910>\n# specify the \"enrich\" child function, add extra package requirements\nchild = fn.add_child_function('enrich', './nlp.py', 'mlrun/mlrun')\nchild.spec.build.commands = [\"python -m pip install spacy\",\n\"python -m spacy download en_core_web_sm\"]\ngraph.plot(rankdir='LR')\nTest the pipeline locally#\nCreate an input file:\n%%writefile in.json\n[\"Born and raised in Queens, New York City, Trump attended Fordham University for two years and received a bachelor's degree in economics from the Wharton School of the University of Pennsylvania. He became president of his father Fred Trump's real estate business in 1971, renamed it The Trump Organization, and expanded its operations to building or renovating skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, mostly by licensing his name. Trump and his businesses have been involved in more than 4,000 state and federal legal actions, including six bankruptcies. He owned the Miss Universe brand of beauty pageants from 1996 to 2015, and produced and hosted the reality television series The Apprentice from 2004 to 2015.\",\n\"Trump's political positions have been described as populist, protectionist, isolationist, and nationalist. He entered the 2016 presidential race as a Republican and was elected in a surprise electoral college victory over Democratic nominee Hillary Clinton while losing the popular vote.[a] He became the oldest first-term U.S. president[b] and the first without prior military or government service. His election and policies have sparked numerous protests. Trump has made many false or misleading statements during his campaign and presidency. The statements have been documented by fact-checkers, and the media have widely described the phenomenon as unprecedented in American politics. Many of his comments and actions have been characterized as racially charged or racist.\"]\nOverwriting in.json\nCreate a mock server (simulator) and test:\n# tuggle verbosity if needed\nfn.verbose = False\nto\n# create a mock server (simulator), specify to simulate all the functions in the pipeline (\"*\")\nserver = fn.to_mock_server(current_function=\"*\")\n# push a sample request into the pipeline and see the results print out (by the printer step)\nresp = server.test(body={\"url\": \"in.json\"})\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'Queens', 'entity_start_char': 19, 'entity_end_char': 25, 'entity_label': 'GPE'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'New York City', 'entity_start_char': 27, 'entity_end_char': 40, 'entity_label': 'GPE'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'Trump', 'entity_start_char': 42, 'entity_end_char': 47, 'entity_label': 'ORG'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'Fordham University', 'entity_start_char': 57, 'entity_end_char': 75, 'entity_label': 'ORG'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'two years', 'entity_start_char': 80, 'entity_end_char': 89, 'entity_label': 'DATE'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'the Wharton School of the University of Pennsylvania', 'entity_start_char': 141, 'entity_end_char': 193, 'entity_label': 'ORG'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'Fred Trump', 'entity_start_char': 229, 'entity_end_char': 239, 'entity_label': 'PERSON'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': '1971', 'entity_start_char': 266, 'entity_end_char': 270, 'entity_label': 'DATE'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'The Trump Organization', 'entity_start_char': 283, 'entity_end_char': 305, 'entity_label': 'ORG'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'more than 4,000', 'entity_start_char': 529, 'entity_end_char': 544, 'entity_label': 'CARDINAL'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'six', 'entity_start_char': 588, 'entity_end_char': 591, 'entity_label': 'CARDINAL'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'Universe', 'entity_start_char': 624, 'entity_end_char': 632, 'entity_label': 'PERSON'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': '1996 to 2015', 'entity_start_char': 663, 'entity_end_char': 675, 'entity_label': 'DATE'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': 'The Apprentice', 'entity_start_char': 731, 'entity_end_char': 745, 'entity_label': 'WORK_OF_ART'}\n{'url': 'in.json', 'paragraph_id': 0, 'entity_text': '2004 to 2015', 'entity_start_char': 751, 'entity_end_char': 763, 'entity_label': 'DATE'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'Trump', 'entity_start_char': 0, 'entity_end_char': 5, 'entity_label': 'ORG'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': '2016', 'entity_start_char': 122, 'entity_end_char': 126, 'entity_label': 'DATE'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'Republican', 'entity_start_char': 150, 'entity_end_char': 160, 'entity_label': 'NORP'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'Democratic', 'entity_start_char': 222, 'entity_end_char': 232, 'entity_label': 'NORP'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'Hillary Clinton', 'entity_start_char': 241, 'entity_end_char': 256, 'entity_label': 'PERSON'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'first', 'entity_start_char': 312, 'entity_end_char': 317, 'entity_label': 'ORDINAL'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'U.S.', 'entity_start_char': 323, 'entity_end_char': 327, 'entity_label': 'GPE'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'first', 'entity_start_char': 349, 'entity_end_char': 354, 'entity_label': 'ORDINAL'}\n{'url': 'in.json', 'paragraph_id': 1, 'entity_text': 'American', 'entity_start_char': 671, 'entity_end_char': 679, 'entity_label': 'NORP'}\nserver.wait_for_completion()\nDeploy to the cluster#\n# add credentials to the data/streams\nfn.apply(mlrun.platforms.v3io_cred())\nchild.apply(mlrun.platforms.v3io_cred())\n# specify the error stream (to store exceptions from the functions)\nfn.spec.error_stream = err_stream\n# deploy as a set of serverless functions\nfn.deploy()\n> 2021-05-03 14:33:55,400 [info] deploy child function enrich ...\n> 2021-05-03 14:33:55,427 [info] Starting remote function deploy\n2021-05-03 14:33:55  (info) Deploying function\n2021-05-03 14:33:55  (info) Building\n2021-05-03 14:33:55  (info) Staging files and preparing base images\n2021-05-03 14:33:55  (info) Building processor image\n2021-05-03 14:34:02  (info) Build complete\n2021-05-03 14:34:08  (info) Function deploy complete\n> 2021-05-03 14:34:09,232 [info] function deployed, address=default-tenant.app.yh30.iguazio-c0.com:32356\n> 2021-05-03 14:34:09,233 [info] deploy root function multi-func ...\n> 2021-05-03 14:34:09,234 [info] Starting remote function deploy\n2021-05-03 14:34:09  (info) Deploying function\n2021-05-03 14:34:09  (info) Building\n2021-05-03 14:34:09  (info) Staging files and preparing base images\n2021-05-03 14:34:09  (info) Building processor image\n2021-05-03 14:34:16  (info) Build complete\n2021-05-03 14:34:22  (info) Function deploy complete\n> 2021-05-03 14:34:22,891 [info] function deployed, address=default-tenant.app.yh30.iguazio-c0.com:32046\n'http://default-tenant.app.yh30.iguazio-c0.com:32046'\nListen on the output stream\nYou can use the SDK or CLI to listen on the output stream. Listening should be done in a separate console/notebook. Run:\nmlrun watch-stream v3io:///users/admin/out-stream -j\nor use the SDK:\nfrom mlrun.platforms import watch_stream\nwatch_stream(\"v3io:///users/admin/out-stream\", is_json=True)\nTest the live function:\nNote\nThe url must be a valid path to the input file.\nfn.invoke('', body={\"url\": \"v3io:///users/admin/pipe/in.json\"})\n{'id': '79354e45-a158-405f-811c-976e9cf4ab5e'}"}
{"text": "Getting started#\nThis example uses a custom class and custom function. See custom steps for more details.\nIn this section\nSteps\nCreate a function\nBuild the graph\nVisualize the graph\nTest the function\nDeploy the function\nTest the deployed function\nSteps#\nThe following code defines basic steps that illustrate building a graph. These steps are:\ninc: increments the value by 1.\nmul: multiplies the value by 2.\nWithState: class that increments an internal counter, prints an output, and adds the input value to the current counter.\n# mlrun: start-code\ndef inc(x):\nreturn x + 1\ndef mul(x):\nreturn x * 2\nclass WithState:\ndef __init__(self, name, context, init_val=0):\nself.name = name\nself.context = context\nself.counter = init_val\ndef do(self, x):\nself.counter += 1\nprint(f\"Echo: {self.name}, x: {x}, counter: {self.counter}\")\nreturn x + self.counter\n# mlrun: end-code\nCreate a function#\nNow take the code above and create an MLRun function called serving-graph.\nimport mlrun\nfn = mlrun.code_to_function(\"simple-graph\", kind=\"serving\", image=\"mlrun/mlrun\")\ngraph = fn.set_topology(\"flow\")\nBuild the graph#\nUse graph.to() to chain steps. Use .respond() to mark that the output of that step is returned to the caller\n(as an http response). By default the graph is async with no response.\ngraph.to(name=\"+1\", handler='inc')\\\n.to(name=\"*2\", handler='mul')\\\n.to(name=\"(X+counter)\", class_name='WithState').respond()\n<mlrun.serving.states.TaskStep at 0x7f821e504450>\nVisualize the graph#\nUsing the plot method, you can visualize the graph.\ngraph.plot(rankdir='LR')\nTest the function#\nCreate a mock server and test the graph locally. Since this graph accepts a numeric value as the input, that value is provided\nin the body parameter.\nserver = fn.to_mock_server()\nserver.test(body=5)\nEcho: (X+counter), x: 12, counter: 1\n13\nRun the function again. This time, the counter should be 2 and the output should be 14.\nserver.test(body=5)\nEcho: (X+counter), x: 12, counter: 2\n14\nDeploy the function#\nUse the deploy method to deploy the function.\nfn.deploy(project='basic-graph-demo')\n> 2021-11-08 07:30:21,571 [info] Starting remote function deploy\n2021-11-08 07:30:21  (info) Deploying function\n2021-11-08 07:30:21  (info) Building\n2021-11-08 07:30:21  (info) Staging files and preparing base images\n2021-11-08 07:30:21  (info) Building processor image\n2021-11-08 07:30:26  (info) Build complete\n2021-11-08 07:30:31  (info) Function deploy complete\n> 2021-11-08 07:30:31,785 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-basic-graph-demo-simple-graph.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['basic-graph-demo-simple-graph-basic-graph-demo.default-tenant.app.aganefaibuzg.iguazio-cd2.com/']}\n'http://basic-graph-demo-simple-graph-basic-graph-demo.default-tenant.app.aganefaibuzg.iguazio-cd2.com/'\nTest the deployed function#\nUse the invoke method to call the function.\nfn.invoke('', body=5)\n> 2021-11-08 07:30:43,241 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-basic-graph-demo-simple-graph.default-tenant.svc.cluster.local:8080/'}\n13\nfn.invoke('', body=5)\n> 2021-11-08 07:30:48,359 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-basic-graph-demo-simple-graph.default-tenant.svc.cluster.local:8080/'}\n14"}
{"text": "Advanced model serving graph - notebook example#\nThis example demonstrates how to use MLRun serving graphs and their advanced functionality including:\nUse of flow, task, model, and ensemble router states\nBuild tasks from custom handlers, classes and storey components\nUse custom error handlers\nTest graphs locally\nDeploy the graph as a real-time serverless functions\nIn this example\nDefine functions and classes used in the graph\nCreate a new serving function and graph\nTest the function locally\nDeploy the graph as a real-time serverless function\nDefine functions and classes used in the graph#\nfrom cloudpickle import load\nfrom typing import List\nfrom sklearn.datasets import load_iris\nimport numpy as np\n# model serving class example\nclass ClassifierModel(mlrun.serving.V2ModelServer):\ndef load(self):\n\"\"\"load and initialize the model and/or other elements\"\"\"\nmodel_file, extra_data = self.get_model('.pkl')\nself.model = load(open(model_file, 'rb'))\ndef predict(self, body: dict) -> List:\n\"\"\"Generate model predictions from sample.\"\"\"\nfeats = np.asarray(body['inputs'])\nresult: np.ndarray = self.model.predict(feats)\nreturn result.tolist()\n# echo class, custom class example\nclass Echo:\ndef __init__(self, context, name=None, **kw):\nself.context = context\nself.name = name\nself.kw = kw\ndef do(self, x):\nprint(\"Echo:\", self.name, x)\nreturn x\n# error echo function, demo catching error and using custom function\ndef error_catcher(x):\nx.body = {\"body\": x.body, \"origin_state\": x.origin_state, \"error\": x.error}\nprint(\"EchoError:\", x)\nreturn None\n# mark the end of the code section, DO NOT REMOVE !\n# mlrun: end-code\nCreate a new serving function and graph#\nUse code_to_function to convert the above code into a serving function object and initialize a graph with async flow topology.\nfunction = mlrun.code_to_function(\"advanced\", kind=\"serving\",\nimage=\"mlrun/mlrun\",\nrequirements=['storey'])\ngraph = function.set_topology(\"flow\", engine=\"async\")\n#function.verbose = True\nSpecify the sklearn models that are used in the ensemble.\nmodels_path = 'https://s3.wasabisys.com/iguazio/models/iris/model.pkl'\npath1 = models_path\npath2 = models_path\nBuild and connect the graph (DAG) using the custom function and classes and plot the result. Add states\nusing the state.to() method (adds a new state after the current one), or using the\ngraph.add_step() method.\nUse the graph.error_handler() (apply to all states) or state.error_handler() (apply to a specific state) if you want the error from the graph or the state to be fed into a specific state (catcher).\nYou can specify which state is the responder (returns the HTTP response) using the state.respond() method.\nIf you don’t specify the responder, the graph is non-blocking.\n# use built-in storey class or our custom Echo class to create and link Task states\ngraph.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})') \\\n.to(class_name=\"Echo\", name=\"pre-process\", some_arg='abc').error_handler(\"catcher\")\n# add an Ensemble router with two child models (routes). The \"*\" prefix mark it is a router class\nrouter = graph.add_step(\"*mlrun.serving.VotingEnsemble\", name=\"ensemble\", after=\"pre-process\")\nrouter.add_route(\"m1\", class_name=\"ClassifierModel\", model_path=path1)\nrouter.add_route(\"m2\", class_name=\"ClassifierModel\", model_path=path2)\n# add the final step (after the router) that handles post processing and responds to the client\ngraph.add_step(class_name=\"Echo\", name=\"final\", after=\"ensemble\").respond()\n# add error handling state, run only when/if the \"pre-process\" state fails (keep after=\"\")\ngraph.add_step(handler=\"error_catcher\", name=\"catcher\", full_event=True, after=\"\")\n# plot the graph (using Graphviz) and run a test\ngraph.plot(rankdir='LR')\nTest the function locally#\nCreate a test set.\nimport random\niris = load_iris()\nx = random.sample(iris['data'].tolist(), 5)\nCreate a mock server (simulator) and test the graph with the test data.\nNote: The model and router objects support a common serving protocol API, see the protocol and API section.\nserver = function.to_mock_server()\nresp = server.test(\"/v2/models/infer\", body={\"inputs\": x})\nserver.wait_for_completion()\nresp\n> 2021-01-09 22:49:26,365 [info] model m1 was loaded\n> 2021-01-09 22:49:26,493 [info] model m2 was loaded\n> 2021-01-09 22:49:26,494 [info] Loaded ['m1', 'm2']\nEcho: pre-process {'inputs': [[6.9, 3.2, 5.7, 2.3], [6.4, 2.7, 5.3, 1.9], [4.9, 3.1, 1.5, 0.1], [7.3, 2.9, 6.3, 1.8], [5.4, 3.7, 1.5, 0.2]], 'tag': 'something'}\nEcho: final {'model_name': 'ensemble', 'outputs': [2, 2, 0, 2, 0], 'id': '0ebcc5f6f4c24d4d83eb36391eaefb98'}\n{'model_name': 'ensemble',\n'outputs': [2, 2, 0, 2, 0],\n'id': '0ebcc5f6f4c24d4d83eb36391eaefb98'}\nDeploy the graph as a real-time serverless function#\nfunction.deploy()\n> 2021-01-09 22:49:40,088 [info] Starting remote function deploy\n2021-01-09 22:49:40  (info) Deploying function\n2021-01-09 22:49:40  (info) Building\n2021-01-09 22:49:40  (info) Staging files and preparing base images\n2021-01-09 22:49:40  (info) Building processor image\n2021-01-09 22:49:41  (info) Build complete\n2021-01-09 22:49:47  (info) Function deploy complete\n> 2021-01-09 22:49:48,422 [info] function deployed, address=default-tenant.app.yh55.iguazio-cd0.com:32222\n'http://default-tenant.app.yh55.iguazio-cd0.com:32222'\nInvoke the remote function using the test data\nfunction.invoke(\"/v2/models/infer\", body={\"inputs\": x})\n{'model_name': 'ensemble',\n'outputs': [1, 2, 0, 0, 0],\n'id': '0ebcc5f6f4c24d4d83eb36391eaefb98'}"}
{"text": "Serving graph high availability configuration#\nThis figure illustrates a simplistic flow of an MLRun serving graph with remote invocation:\nAs explained in Real-time serving pipelines (graphs), the serving graph is based on Nuclio functions.\nIn this section\nUsing Nuclio with stream triggers\nConsumer function configuration\nRemote function retry mechanism\nConfiguration considerations\nUsing Nuclio with stream triggers#\nNuclio can use different trigger types. When used with stream triggers, such as Kafka and V3IO, it uses a consumer group to\ncontinue reading from the last processed offset on function restart. This provides the “at least once” semantics for stateless functions.\nHowever, if the function does have state, such as persisting a batch of events to storage (e.g. parquet files, database) or if the function\nperforms additional processing of events after the function handler returns, then the flow can get into situations where events seem to be\nlost. The mechanism of Window ACK provides a solution for such\nstateful event processing.\nWith Window ACK, the consumer group’s committed offset is delayed by one window, committing the offset at (processed event num – window).\nWhen the function restarts (for any reason including scale-up or scale-down), it starts consuming from this last committed point.\nThe size of the required Window ACK is based on the number of events that could be in processing when the function terminates. You can\ndefine a window ACK per trigger (Kafka, V3IO stream, etc.). When used with a serving graph, the appropriate Window ACK size depends on the\ngraph structure and should be calculated accordingly. The following sections explain the relevant considerations.\nConsumer function configuration#\nA consumer function is essentially a Nuclio function with a stream trigger. As part of the trigger, you can set a consumer group.\nWhen the consumer function is part of a graph then the consumer function’s number of replicas is derived from the number of shards and is\ntherefore nonconfigurable. The same applies to the number of workers in each replica, which is set to 1 and is not configurable.\nThe consumer function has one buffer per worker holding the incoming events that were received by the worker and are waiting to be\nprocessed. Once this buffer is full, events need to be processed so that the function is able to receive more events. The buffer size is\nconfigurable and is key to the overall configuration.\nThe buffer should be as small as possible. There is a trade-off between the buffer size and the latency. A larger buffer has lower latency\nbut increases the recovery time after a failure, due to the high number of records that need to be reprocessed.\nTo set the buffer size:\nfunction.spec.parameters[\"source_args\"] = {\"buffer_size\": 1}\nThe default buffer_size is 8.\nRemote function retry mechanism#\nThe required processing time of a remote function varies, depending on the function. The system assumes a processing\ntime in the order of seconds, which affects the default configurations. However, some functions require a longer processing time.\nYou can configure the timeout on both the caller and on the remote, as appropriate for your functions.\nWhen an event is sent to the remote function, and no response is received by the configured (or default) timeout, or an error 500\n(the remote function failed), or error 502, 503, or 504 (the remote function is too busy to handle the request at this time) is received,\nthe caller retries the request, using the platform’s exponential retry backoff mechanism.\nIf the number of caller retries reaches the configured maximum number of retries, the event is pushed to the exception stream, indicating\nthat this event did not complete successfully. You can look at the exception stream to see the functions that did not complete successfully.\nRemote-function caller configuration#\nIn a simplistic flow these are the consumer function defaults:\nMaximum retries: The default is 6, which is equivalent to about 3-4 minutes if all of the related parameters\nare at their default values. If you expect that some cases will require a higher number, for example, a new node needs to be scaled up\ndepending on your cloud vendor, the instance type, and the zone you are running in, you might want to increase the number of retries.\nRemote step http timeout: The time interval the caller waits for a response from the remote before retrying the request. This value is\naffected by the remote function processing time.\nMax in flight: The maximum number of requests that each caller worker can send in parallel to the remote function.\nIf the caller has more than one worker, each worker has its own Max in flight.\nTo set Max in flight, timeout, and retries:\nRemoteStep(name=”remote_scale”, …, max_in_flight=2, timeout=100, retries=10)\nRemote-function configuration#\nFor the remote function, you can configure the following:\nWorker timeout: The maximum time interval, in seconds, an incoming request waits for an available worker. The worker timeout must be\nshorter than the gateway timeout. The default is 10.\nGateway timeout: The maximum time interval, in seconds, the gateway waits for a response to a request. This determines when the ingress\ntimes out on a request. It must be slightly longer than the expected function processing time. The default is 60.\nTo set the buffer gateway timeout and worker timeout:\nmy_serving_func.with_http(gateway_timeout=125, worker_timeout=60)\nConfiguration considerations#\nThe following figure zooms in on a single consumer and its workers and illustrates the various concepts\nand parameters that provide high availability, using a non-default configuration.\nAssume the processing time of the remote function is Pt, in seconds.\ntimeout: Between <Pt+epsilon> and <Pt+worker_timeout>.\nServing function\ngateway_timeout: Pt+1 second (usually sufficient).\nworker_timeout: The general rule is the greater of Pt/10 or 60 seconds. However, you should adjust the\nvalue according to your needs.\nmax_in_flight: If the processing time is very high then max_in_flight should be low. Otherwise, there will be many retries.\nack_window_size:\nWith 1 worker: The consumer buffer_size+max_in_flight, since it is per each shard and there is a single worker.\nWith >1 worker: The consumer (#workers x buffer_size)+max_in_flight\nMake sure you thoroughly understand your serving graph and its functions before defining the ack_window_size. Its value depends on the\nentire graph flow. You need to understand which steps are parallel (branching) vs. sequential invocation. Another key aspect is that the number of workers affects the window size.\nSee the ack_window_size API.\nFor example:\nIf a graph includes: consumer -> remote r1 -> remote r2\nThe window should be the sum of: consumer’s buffer size + MIF to r1 + MIF to r2.\nIf a graph includes: calling to remote r1 and r2 in parallel\nThe window should be set to: consumer’s buffer size + max (MIF to r1, MIF to r2)."}
{"text": "Model serving API#\nMLRun Serving follows the same REST API defined by Triton and KFServing v2.\nNuclio also supports streaming protocols (Kafka, kinesis, MQTT, etc.). When streaming, the\nmodel name and operation can be encoded inside the message body.\nThe APIs are:\nexplain\nget model health / readiness\nget model metadata\nget server info\ninfer / predict\nlist models\nexplain#\nPOST /v2/models/[/versions/{VERSION}]/explain\nRequest body:\n{\n\"id\" : $string #optional,\n\"model\" : $string #optional\n\"parameters\" : $parameters #optional,\n\"inputs\" : [ $request_input, ... ],\n\"outputs\" : [ $request_output, ... ] #optional\n}\nResponse structure:\n{\n\"model_name\" : $string,\n\"model_version\" : $string #optional,\n\"id\" : $string,\n\"outputs\" : [ $response_output, ... ]\n}\nget model health / readiness#\nGET v2/models/${MODEL_NAME}[/versions/${VERSION}]/ready\nReturns 200 for Ok, 40X for not ready.\nget model metadata#\nGET v2/models/${MODEL_NAME}[/versions/${VERSION}]\nResponse example: {\"name\": \"m3\", \"version\": \"v2\", \"inputs\": [..], \"outputs\": [..]}\nget server info#\nGET /\nGET /v2/health\nResponse example: {'name': 'my-server', 'version': 'v2', 'extensions': []}\ninfer / predict#\nPOST /v2/models/<model>[/versions/{VERSION}]/infer\nRequest body:\n{\n\"id\" : $string #optional,\n\"model\" : $string #optional\n\"data_url\" : $string #optional\n\"parameters\" : $parameters #optional,\n\"inputs\" : [ $request_input, ... ],\n\"outputs\" : [ $request_output, ... ] #optional\n}\nid: Unique Id of the request, if not provided a random value is provided.\nmodel: Model to select (for streaming protocols without URLs).\ndata_url: Option to load the inputs from an external file/s3/v3io/… object.\nparameters: Optional request parameters.\ninputs: List of input elements (numeric values, arrays, or dicts).\noutputs: Optional, requested output values.\nNote\nYou can also send binary data to the function, for example, a JPEG image. The serving engine pre-processor\ndetects it based on the HTTP content-type and converts it to the above request structure, placing the\nimage bytes array in the inputs field.\nResponse structure:\n{\n\"model_name\" : $string,\n\"model_version\" : $string #optional,\n\"id\" : $string,\n\"outputs\" : [ $response_output, ... ]\n}\nlist models#\nGET /v2/models/\nResponse example:  {\"models\": [\"m1\", \"m2\", \"m3:v1\", \"m3:v2\"]}"}
{"text": "Model serving graph#\nIn this section\nServing Functions\nTopology\nRemote execution\nExamples\nServing Functions#\nTo start using a serving graph, you first need a serving function. A serving function contains the serving\nclass code to run the model and all the code necessary to run the tasks. MLRun comes with a wide library of tasks. If you\nuse just those, you don’t have to add any special code to the serving function, you just have to provide\nthe code that runs the model. For more information about serving classes see Build your own model serving class.\nFor example, the following code is a basic model serving class:\n# mlrun: start-code\nfrom cloudpickle import load\nfrom typing import List\nimport numpy as np\nimport mlrun\nclass ClassifierModel(mlrun.serving.V2ModelServer):\ndef load(self):\n\"\"\"load and initialize the model and/or other elements\"\"\"\nmodel_file, extra_data = self.get_model(\".pkl\")\nself.model = load(open(model_file, \"rb\"))\ndef predict(self, body: dict) -> List:\n\"\"\"Generate model predictions from sample.\"\"\"\nfeats = np.asarray(body[\"inputs\"])\nresult: np.ndarray = self.model.predict(feats)\nreturn result.tolist()\n# mlrun: end-code\nTo obtain the serving function, use the code_to_function and specify kind to be serving.\nfn = mlrun.code_to_function(\"serving_example\",\nkind=\"serving\",\nimage=\"mlrun/mlrun\")\nTopology#\nRouter#\nOnce you have a serving function, you need to choose the graph topology. The default is router topology. With the router topology you can specify different machine learning models. Each model has a logical name. This name is used to route to the correct model when calling the serving function.\nfrom sklearn.datasets import load_iris\n# set the topology/router\ngraph = fn.set_topology(\"router\")\n# Add the model\nfn.add_model(\"model1\", class_name=\"ClassifierModel\", model_path=\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\")\n# Add additional models\n#fn.add_model(\"model2\", class_name=\"ClassifierModel\", model_path=\"<path2>\")\n# create and use the graph simulator\nserver = fn.to_mock_server()\nx = load_iris()['data'].tolist()\nresult = server.test(\"/v2/models/model1/infer\", {\"inputs\": x})\nprint(result)\n> 2021-11-02 04:18:36,925 [info] model model1 was loaded\n> 2021-11-02 04:18:36,926 [info] Initializing endpoint records\n> 2021-11-02 04:18:36,965 [info] Loaded ['model1']\n{'id': '6bd11e864805484ea888f58e478d1f91', 'model_name': 'model1', 'outputs': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\nFlow#\nYou can use the flow topology to specify tasks, which typically manipulates the data. The most common scenario is pre-processing of data prior to the model execution.\nNote\nOnce the topology is set, you cannot change an existing function toplogy.\nIn this topology, you build and connect the graph (DAG) by adding steps using the step.to() method, or by using the\ngraph.add_step() method.\nThe step.to() is typically used to chain steps together. graph.add_step can add steps anywhere on the\ngraph and has before and after parameters to specify the location of the step.\nfn2 = mlrun.code_to_function(\"serving_example_flow\",\nkind=\"serving\",\nimage=\"mlrun/mlrun\")\ngraph2 = fn2.set_topology(\"flow\")\ngraph2_enrich = graph2.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})')\n# add an Ensemble router with two child models (routes)\nrouter = graph2.add_step(mlrun.serving.ModelRouter(), name=\"router\", after=\"enrich\")\nrouter.add_route(\"m1\", class_name=\"ClassifierModel\", model_path='https://s3.wasabisys.com/iguazio/models/iris/model.pkl')\nrouter.respond()\n# Add additional models\n#router.add_route(\"m2\", class_name=\"ClassifierModel\", model_path=path2)\n# plot the graph (using Graphviz)\ngraph2.plot(rankdir='LR')\nfn2_server = fn2.to_mock_server()\nresult = fn2_server.test(\"/v2/models/m1/infer\", {\"inputs\": x})\nprint(result)\n> 2021-11-02 04:18:42,142 [info] model m1 was loaded\n> 2021-11-02 04:18:42,142 [info] Initializing endpoint records\n> 2021-11-02 04:18:42,183 [info] Loaded ['m1']\n{'id': 'f713fd7eedeb431eba101b13c53a15b5'}\nRemote execution#\nYou can chain functions together with remote execution. This allows you to:\nCall existing functions from the graph and reuse them from other graphs.\nScale up and down different components individually.\nCalling a remote function can either use HTTP or via a queue (streaming).\nHTTP#\nCalling a function using http uses the special $remote class. First deploy the remote function:\nremote_func_name = \"serving-example-flow\"\nproject_name = \"graph-basic-concepts\"\nfn_remote = mlrun.code_to_function(remote_func_name,\nproject=project_name,\nkind=\"serving\",\nimage=\"mlrun/mlrun\")\nfn_remote.add_model(\"model1\", class_name=\"ClassifierModel\", model_path=\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\")\nremote_addr = fn_remote.deploy()\n> 2022-03-17 08:20:40,674 [info] Starting remote function deploy\n2022-03-17 08:20:40  (info) Deploying function\n2022-03-17 08:20:40  (info) Building\n2022-03-17 08:20:40  (info) Staging files and preparing base images\n2022-03-17 08:20:40  (info) Building processor image\n2022-03-17 08:20:42  (info) Build complete\n2022-03-17 08:20:47  (info) Function deploy complete\n> 2022-03-17 08:20:48,289 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-graph-basic-concepts-serving-example-flow.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['graph-basic-concepts-serving-example-flow-graph-basic-concepts.default-tenant.app.maor-gcp2.iguazio-cd0.com/']}\nCreate a new function with a graph and call the remote function above:\nfn_preprocess = mlrun.new_function(\"preprocess\", kind=\"serving\")\ngraph_preprocessing = fn_preprocess.set_topology(\"flow\")\ngraph_preprocessing.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})').to(\n\"$remote\", \"remote_func\", url=f'{remote_addr}v2/models/model1/infer', method='put').respond()\ngraph_preprocessing.plot(rankdir='LR')\nfn3_server = fn_preprocess.to_mock_server()\nmy_data = '''{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}'''\nresult = fn3_server.test(\"/v2/models/my_model/infer\", body=my_data)\nprint(result)\n> 2022-03-17 08:20:48,374 [warning] run command, file or code were not specified\n{'id': '3a1dd36c-e7de-45af-a0c4-72e3163ba92a', 'model_name': 'model1', 'outputs': [0, 2]}\nQueue (streaming)#\nYou can use queues to send events from one part of the graph to another and to decouple the processing of those parts.\nQueues are better suited to deal with bursts of events, since all the events are stored in the queue until they are processed.\nV3IO stream example#\nThe example below uses a V3IO stream, which is a fast real-time implementation of a stream that allows processing of events at very low latency.\n%%writefile echo.py\ndef echo_handler(x):\nprint(x)\nreturn x\nOverwriting echo.py\nConfigure the streams\nimport os\nstreams_prefix = f\"v3io:///users/{os.getenv('V3IO_USERNAME')}/examples/graph-basic-concepts\"\ninput_stream = streams_prefix + \"/in-stream\"\nout_stream = streams_prefix + \"/out-stream\"\nerr_stream = streams_prefix + \"/err-stream\"\nAlternativey, use Kafka to configure the streams:\nkafka_prefix = f\"kafka://{broker}/\"\ninternal_topic = kafka_prefix + \"in-topic\"\nout_topic = kafka_prefix + \"out-topic\"\nerr_topic = kafka_prefix + \"err-topic\"\nCreate the graph. Note that in the to method the class name is specified to be >> or $queue to specify that this is a queue.\nfn_preprocess2 = mlrun.new_function(\"preprocess\", kind=\"serving\")\nfn_preprocess2.add_child_function('echo_func', './echo.py', 'mlrun/mlrun')\ngraph_preprocess2 = fn_preprocess2.set_topology(\"flow\")\ngraph_preprocess2.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})')\\\n.to(\">>\", \"input_stream\", path=input_stream)\\\n.to(name=\"echo\", handler=\"echo_handler\", function=\"echo_func\")\\\n.to(\">>\", \"output_stream\", path=out_stream, sharding_func=\"partition\")\ngraph_preprocess2.plot(rankdir='LR')\nfrom echo import *\nfn4_server = fn_preprocess2.to_mock_server(current_function=\"*\")\nmy_data = '''{\"inputs\": [[5.1, 3.5, 1.4, 0.2], [7.7, 3.8, 6.7, 2.2]], \"partition\": 0}'''\nresult = fn4_server.test(\"/v2/models/my_model/infer\", body=my_data)\nprint(result)\n> 2022-03-17 08:20:55,182 [warning] run command, file or code were not specified\n{'id': 'a6efe8217b024ec7a7e02cf0b7850b91'}\n{'inputs': [[5.1, 3.5, 1.4, 0.2], [7.7, 3.8, 6.7, 2.2]], 'tag': 'something'}\nKafka stream example#\n%%writefile echo.py\ndef echo_handler(x):\nprint(x)\nreturn x\nOverwriting echo.py\nConfigure the streams\nimport os\ninput_topic = \"in-topic\"\nout_topic = \"out-topic\"\nerr_topic = \"err-topic\"\n# replace this\nbrokers = \"<broker IP>\"\nCreate the graph. Note that in the to method the class name is specified to be >> or $queue to specify that this is a queue.\nimport mlrun\nfn_preprocess2 = mlrun.new_function(\"preprocess\", kind=\"serving\")\nfn_preprocess2.add_child_function('echo_func', './echo.py', 'mlrun/mlrun')\ngraph_preprocess2 = fn_preprocess2.set_topology(\"flow\")\ngraph_preprocess2.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})')\\\n.to(\">>\", \"input_stream\", path=input_topic, kafka_bootstrap_servers=brokers)\\\n.to(name=\"echo\", handler=\"echo_handler\", function=\"echo_func\")\\\n.to(\">>\", \"output_stream\", path=out_topic, kafka_bootstrap_servers=brokers)\ngraph_preprocess2.plot(rankdir='LR')\nfrom echo import *\nfn4_server = fn_preprocess2.to_mock_server(current_function=\"*\")\nfn4_server.set_error_stream(f\"kafka://{brokers}/{err_topic}\")\nmy_data = '''{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}'''\nresult = fn4_server.test(\"/v2/models/my_model/infer\", body=my_data)\nprint(result)\nExamples#\nNLP processing pipeline with real-time streaming#\nIn some cases it’s useful to split your processing to multiple functions and use\nstreaming protocols to connect those functions.\nSee the full notebook example, where the data processing is in the first function/container and the NLP processing is in the second function. And the second function contains the GPU.\nCurrently queues support Iguazio v3io and Kafka streams."}
{"text": "Error handling#\nGraph steps might raise an exception. If you want to have an error handling flow, you can specify an exception handling\nstep/branch that is triggered on error. The error handler step receives the event that entered the failed step,\nwith two extra attributes: event.origin_state indicates the name of the failed step; and event.error holds the error string.\nUse the graph.error_handler() (apply to all steps) or step.error_handler() (apply to a specific step)\nif you want the error from the graph or the step to be fed into a specific step (catcher).\nExample of setting an error catcher per step:\ngraph.add_step(\"MyClass\", name=\"my-class\", after=\"pre-process\").error_handler(\"catcher\")\ngraph.add_step(\"ErrHandler\", name=\"catcher\", full_event=True, after=\"\")\nNote\nAdditional steps can follow the catcher step.\nUsing the example in Model serving graph, you can add an error handler as follows:\ngraph2_enrich.error_handler(\"catcher\")\ngraph2.add_step(\"ErrHandler\", name=\"catcher\", full_event=True, after=\"\")\n<mlrun.serving.states.TaskStep at 0x7fd46e557750>\nNow, display the graph again:\ngraph2.plot(rankdir='LR')\n<mlrun.serving.states.TaskStep at 0x7fd46e557750>\nException stream#\nThe graph errors/exceptions can be pushed into a special error stream. This is very convenient in the case of\ndistributed and production graphs.\nTo set the exception stream address (using v3io streams uri):\nfn_preprocess2.spec.error_stream = err_stream"}
{"text": "Graph concepts and state machine#\nA graph is composed of the following:\nStep: A Step runs a function or class handler or a REST API call. MLRun comes with a list of pre-built steps that include data manipulation, readers, writers and model serving. You can also write your own steps using\nstandard Python functions or custom functions/classes, or can be a external REST API (the special $remote class).\nRouter: A special type of step is a router with routing logic and multiple child routes/models. The basic\nrouting logic is to route to the child routes based on the event.path. More advanced or custom routing can be used,\nfor example, the ensemble router sends the event to all child routes in parallel, aggregates the result and responds.\nQueue: A queue or stream that accepts data from one or more source steps and publishes to one or more output steps.\nQueues are best used to connect independent functions/containers. Queues can run in-memory or be implemented using a stream, which allows it to span processes/containers.\nThe Graph server has two modes of operation (topologies):\nRouter topology (default): A minimal configuration with a single router and child tasks/routes. This can be used for simple model serving or single hop configurations.\nFlow topology: A full graph/DAG. The flow topology is implemented using two engines: async (the default)\nis based on Storey and asynchronous event loop; and sync, which supports a simple\nsequence of steps.\nIn this section\nThe Event object\nThe Context object\nTopology\nBuilding distributed graphs\nError handling\nThe Event object#\nThe Graph state machine accepts an Event object (similar to a Nuclio Event) and passes\nit along the pipeline. An Event object hosts the event body along with other attributes\nsuch as path (http request path), method (GET, POST, …), andid (unique event ID).\nIn some cases the events represent a record with a unique key, which can be read/set\nthrough the event.key. Records have associated event.time that, by default, is\nthe arrival time, but can also be set by a step.\nThe Task steps are called with the event.body by default. If a task step needs to\nread or set other event elements (key, path, time, …) you should set the task full_event\nargument to True.\nTask steps support optional input_path and result_path attributes that allow controlling which portion of\nthe event is sent as input to the step, and where to update the returned result.\nFor example, for an event body {\"req\": {\"body\": \"x\"}}, input_path=\"req.body\" and result_path=\"resp\"\nthe step gets \"x\" as the input. The output after the step is {\"req\": {\"body\": \"x\"}: \"resp\": <step output>}.\nNote that input_path and result_path do not work together with full_event=True.\nThe Context object#\nThe step classes are initialized with a context object (when they have context in their __init__ args).\nThe context is used to pass data and for interfacing with system services. The context object has the\nfollowing attributes and methods.\nAttributes:\nlogger: Central logger (Nuclio logger when running in Nuclio).\nverbose: True if in verbose/debug mode.\nroot: The graph object.\ncurrent_function: When running in a distributed graph, the current child function name.\nMethods:\nget_param(key, default=None): Get the graph parameter by key. Parameters are set at the\nserving function (e.g. function.spec.parameters = {\"param1\": \"x\"}).\nget_secret(key): Get the value of a project/user secret.\nget_store_resource(uri, use_cache=True): Get the mlrun store object (data item, artifact, model, feature set, feature vector).\nget_remote_endpoint(name, external=False): Return the remote nuclio/serving function http(s) endpoint given its [project/]function-name[:tag].\nResponse(headers=None, body=None, content_type=None, status_code=200): Create a nuclio response object, for returning detailed http responses.\nExample, using the context:\nif self.context.verbose:\nself.context.logger.info('my message', some_arg='text')\nx = self.context.get_param('x', 0)\nTopology#\nRouter#\nOnce you have a serving function, you need to choose the graph topology. The default is router topology. With the router topology you can specify different machine learning models. Each model has a logical name. This name is used to route to the correct model when calling the serving function.\nfrom sklearn.datasets import load_iris\n# set the topology/router\ngraph = fn.set_topology(\"router\")\n# Add the model\nfn.add_model(\"model1\", class_name=\"ClassifierModel\", model_path=\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\")\n# Add additional models\n#fn.add_model(\"model2\", class_name=\"ClassifierModel\", model_path=\"<path2>\")\n# create and use the graph simulator\nserver = fn.to_mock_server()\nx = load_iris()['data'].tolist()\nresult = server.test(\"/v2/models/model1/infer\", {\"inputs\": x})\nprint(result)\n> 2021-11-02 04:18:36,925 [info] model model1 was loaded\n> 2021-11-02 04:18:36,926 [info] Initializing endpoint records\n> 2021-11-02 04:18:36,965 [info] Loaded ['model1']\n{'id': '6bd11e864805484ea888f58e478d1f91', 'model_name': 'model1', 'outputs': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\nFlow#\nUsing the flow topology, you can specify tasks, which typically manipulate the data. The most common scenario is pre-processing of data prior to the model execution.\nNote\nOnce the topology is set, you cannot change an existing function topology.\nIn this topology, you build and connect the graph (DAG) by adding steps using the step.to() method, or by using the\ngraph.add_step() method.\nThe step.to() is typically used to chain steps together. graph.add_step can add steps anywhere on the\ngraph and has before and after parameters to specify the location of the step.\nfn2 = mlrun.code_to_function(\"serving_example_flow\",\nkind=\"serving\",\nimage=\"mlrun/mlrun\")\ngraph2 = fn2.set_topology(\"flow\")\ngraph2_enrich = graph2.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})')\n# add an Ensemble router with two child models (routes)\nrouter = graph2.add_step(mlrun.serving.ModelRouter(), name=\"router\", after=\"enrich\")\nrouter.add_route(\"m1\", class_name=\"ClassifierModel\", model_path='https://s3.wasabisys.com/iguazio/models/iris/model.pkl')\nrouter.respond()\n# Add additional models\n#router.add_route(\"m2\", class_name=\"ClassifierModel\", model_path=path2)\n# plot the graph (using Graphviz)\ngraph2.plot(rankdir='LR')\nfn2_server = fn2.to_mock_server()\nresult = fn2_server.test(\"/v2/models/m1/infer\", {\"inputs\": x})\nprint(result)\n> 2021-11-02 04:18:42,142 [info] model m1 was loaded\n> 2021-11-02 04:18:42,142 [info] Initializing endpoint records\n> 2021-11-02 04:18:42,183 [info] Loaded ['m1']\n{'id': 'f713fd7eedeb431eba101b13c53a15b5'}\nBuilding distributed graphs#\nGraphs can be hosted by a single function (using zero to n containers), or span multiple functions\nwhere each function can have its own container image and resources (replicas, GPUs/CPUs, volumes, etc.).\nIt has a root function, which is where you configure triggers (http, incoming stream, cron, …),\nand optional downstream child functions.\nYou can specify the function attribute in Task or Router steps. This indicates where\nthis step should run. When the function attribute is not specified it runs on the root function.\nfunction=\"*\" means the step can run in any of the child functions.\nSteps on different functions should be connected using a Queue step (a stream).\nAdding a child function:\nfn.add_child_function('enrich',\n'./entity_extraction.ipynb',\nimage='mlrun/mlrun',\nrequirements=[\"storey\", \"sklearn\"])\nSee a full example with child functions.\nA distributed graph looks like this:\nError handling#\nGraph steps might raise an exception. If you want to have an error handling flow,\nyou can specify an exception handling step/branch that is triggered on error.\nThe error handler step receives the event that entered the failed step, with two extra\nattributes: event.origin_state indicates the name of the failed step; and event.error\nholds the error string.\nUse the graph.error_handler() (apply to all steps) or step.error_handler()\n(apply to a specific step) if you want the error from the graph or the step to be\nfed into a specific step (catcher).\nExample of setting an error catcher per step:\ngraph.add_step(\"MyClass\", name=\"my-class\", after=\"pre-process\").error_handler(\"catcher\")\ngraph.add_step(\"ErrHandler\", name=\"catcher\", full_event=True, after=\"\")\nNote\nAdditional steps can follow the catcher step.\nUsing the example in Model serving graph, you can add an error handler as follows:\ngraph2_enrich.error_handler(\"catcher\")\ngraph2.add_step(\"ErrHandler\", name=\"catcher\", full_event=True, after=\"\")\nNow, display the graph again:\ngraph2.plot(rankdir='LR')\nException stream#\nThe graph errors/exceptions can be pushed into a special error stream. This is very convenient\nin the case of distributed and production graphs.\nTo set the exception stream address (using v3io streams uri):\nfn_preprocess2.spec.error_stream = err_stream"}
{"text": "Real-time serving pipelines (graphs)#\nMLRun graphs enable building and running DAGs (directed acyclic graph).\nMLRun graph capabilities include:\nEasy to build and deploy distributed real-time computation graphs\nUse the real-time serverless engine (Nuclio) for auto-scaling and optimized resource utilization\nBuilt-in operators to handle data manipulation, IO, machine learning, deep-learning, NLP, etc.\nBuilt-in monitoring for performance, resources, errors, data, model behaviour, and custom metrics\nDebug in the IDE/Notebook\nGraphs are composed of individual steps.\nThe first graph element accepts an Event object, transforms/processes the event and passes the result to the next steps\nin the graph. The final result can be written out to some destination (file, DB, stream, etc.) or returned back to the caller\n(one of the graph steps can be marked with .respond()).\nThe serving graphs can be composed of pre-defined graph steps, block-type elements (model servers, routers, ensembles,\ndata readers and writers, data engineering tasks, validators, etc.), custom steps, or from native python\nclasses/functions. A graph can have data processing steps, model ensembles, model servers, post-processing, etc. (see the Advanced Model Serving Graph Notebook Example). Graphs can auto-scale and span multiple function containers (connected through streaming protocols).\nDifferent steps can run on the same local function, or run on a remote function. You can call existing functions from the graph and reuse\nthem from other graphs, as well as scale up and down the different components individually.\nGraphs can run inside your IDE or Notebook for test and simulation. Serving graphs are built on\ntop of Nuclio (real-time serverless engine), MLRun jobs,\nMLRun Storey (native Python async and stream processing engine),\nand other MLRun facilities.\nThe serving graphs are used by MLRun’s Feature Store to build real-time feature engineering pipelines.\nIn this section\nGetting started\nUse cases\nGraph concepts and state machine\nModel serving graph\nWriting custom steps\nBuilt-in steps\nDemos and tutorials\nServing graph high availability configuration\nError handling"}
{"text": "Real-time deployment#\nMLRun can produce managed real-time serverless pipelines from various tasks, including MLRun models or standard model files.\nThe pipelines use a real-time serverless engine, called Nuclio, which can be deployed anywhere and is capable of delivering intensive data, I/O, and compute workloads.\nServing a model begins by creating a serving function. This function can run one or more models. To load and call a model, one needs to provide a serving class. MLRun has built-in support for commonly used frameworks and therefore it is often convenient to start with built-in classes. You can also create your own custom model serving class. You can also find an example notebook that shows how to build and run a serving class.\nMLRun serving supports advanced real-time data processing and model serving pipelines. For more details and examples, see the MLRun serving pipelines documentation.\nIn this section\nUsing built-in model serving classes\nBuild your own model serving class\nTest and deploy a model server\nModel serving API"}
{"text": "Test and deploy a model server#\nIn this section\nTesting the model\nDeploying the model\nTesting the model#\nMLRun provides a mock server as part of the serving runtime. This gives you the ability to deploy your serving function in your local environment for testing purposes.\nserving_fn = code_to_function(name='myService', kind='serving', image='mlrun/mlrun')\nserving_fn.add_model('my_model', model_path=model_file_path)\nserver = serving_fn.to_mock_server()\nYou can use test data and programmatically invoke the predict() method of mock server. In this example, the model is expecting a python dictionary as input.\nmy_data = '''{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}'''\nserver.test(\"/v2/models/my_model/infer\", body=my_data)\nThe data structure used in the body parameter depends on how the predict() method of the model server is defined. For examples of how to define your own model server class, see here.\nTo review the mock server api, see here.\nDeploying the model#\nDeploying models in MLRun uses a special function type serving. You can create a serving function using the code_to_function() call from a notebook. You can also import an existing serving function/template from the Function Hub.\nThis example converts a notebook to a serving function and adds a model to it:\nfrom mlrun import code_to_function\nfn = code_to_function('my-function', kind='serving')\nfn.add_model('m1', model_path=<model-artifact/dir>, class_name='MyClass', x=100)\nSee .add_model() docstring for help and parameters.\nSee the full Model Server example.\nIf you want to use multiple versions for the same model, use : to separate the name from the version.\nFor example, if the name is mymodel:v2 it means model name mymodel version v2.\nYou should specify the model_path (url of the model artifact/dir) and the class_name name\n(or class module.submodule.class). Alternatively, you can set the model_url for calling a\nmodel that is served by another function (can be used for ensembles).\nThe function object(fn) accepts many options. You can specify replicas range (auto-scaling), cpu/gpu/mem resources, add shared volume mounts, secrets, and any other Kubernetes resource through the fn.spec object or fn methods.\nFor example, fn.gpu(1) means each replica uses one GPU.\nTo deploy a model, simply call:\nfn.deploy()\nYou can also deploy a model from within an ML pipeline (check the various demos for details)."}
{"text": "Use cases#\nIn this section\nData and feature engineering\nExample of Simple model serving router\nExample of Advanced data processing and serving ensemble\nExample of NLP processing pipeline with real-time streaming\nIn addition to the examples in this section, see the:\nDistributed (multi-function) pipeline example that details how to run a pipeline that consists of multiple serverless functions (connected using streams).\nAdvanced Model Serving Graph Notebook Example that illustrates the flow, task, model, and ensemble router states; building tasks from custom handlers; classes and storey components; using custom error handlers; testing graphs locally; deploying a graph as a real-time serverless function.\nMLRun demos repository for additional use cases and full end-to-end examples, including fraud prevention using the Iguazio feature store, a mask detection demo, and converting existing ML code to an MLRun project.\nData and feature engineering (using the feature store)#\nYou can build a feature set transformation using serving graphs.\nHigh-level transformation logic is automatically converted to real-time serverless processing engines that can read\nfrom any online or offline source, handle any type of structures or unstructured data, run complex computation graphs\nand native user code. Iguazio’s solution uses a unique multi-model database, serving the computed features consistently\nthrough many different APIs and formats (like files, SQL queries, pandas, real-time REST APIs, time-series, streaming),\nresulting in better accuracy and simpler integration.\nRead more in Feature store, and Feature set transformations.\nExample of a simple model serving router#\nGraphs are used for serving models with different transformations.\nTo deploy a serving function, you need to import or create the serving function,\nadd models to it, and then deploy it.\nimport mlrun\n# load the sklearn model serving function and add models to it\nfn = mlrun.import_function('hub://v2_model_server')\nfn.add_model(\"model1\", model_path={model1-url})\nfn.add_model(\"model2\", model_path={model2-url})\n# deploy the function to the cluster\nfn.deploy()\n# test the live model endpoint\nfn.invoke('/v2/models/model1/infer', body={\"inputs\": [5]})\nThe Serving function supports the same protocol used in KFServing V2 and Triton Serving framework.\nTo invoke the model, to use following url: <function-host>/v2/models/model1/infer.\nSee the serving protocol specification for details.\nNote\nModel url is either an MLRun model store object (starts with store://) or URL of a model directory\n(in NFS, s3, v3io, azure, for example s3://{bucket}/{model-dir}). Note that credentials might need to\nbe added to the serving function via environment variables or MLRun secrets.\nSee the scikit-learn classifier example,\nwhich explains how to create/log MLRun models.\nWriting your own serving class#\nYou can implement your own model serving or data processing classes. All you need to do is:\nInherit the base model serving class.\nAdd your implementation for model load() (download the model file(s) and load the model into memory).\npredict() (accept the request payload and return the prediction/inference results).\nYou can override additional methods: preprocess, validate, postprocess, explain.\nYou can add custom API endpoints by adding the method op_xx(event) (which can be invoked by\ncalling the <model-url>/xx, where operation = xx). See model class API.\nFor an example of writing the minimal serving functions, see Minimal sklearn serving function example.\nSee the full V2 Model Server (SKLearn) example that\ntests one or more classifier models against a held-out dataset.\nExample of advanced data processing and serving ensemble#\nMLRun Serving graphs can host advanced pipelines that handle event/data processing, ML functionality,\nor any custom task. The following example demonstrates an asynchronous pipeline that pre-processes data,\npasses the data into a model ensemble, and finishes off with post processing.\nFor a complete example, see the Advanced graph example notebook.\nCreate a new function of type serving from code and set the graph topology to async flow.\nimport mlrun\nfunction = mlrun.code_to_function(\"advanced\", filename=\"demo.py\",\nkind=\"serving\", image=\"mlrun/mlrun\",\nrequirements=['storey'])\ngraph = function.set_topology(\"flow\", engine=\"async\")\nBuild and connect the graph (DAG) using the custom function and classes and plot the result.\nAdd steps using the step.to() method (adds a new step after the current one), or using the\ngraph.add_step() method.\nIf you want the error from the graph or the step to be fed into a specific step (catcher),\nuse the graph.error_handler() (apply to all steps) or step.error_handler()\n(apply to a specific step).\nSpecify which step is the responder (returns the HTTP response) using the step.respond() method.\nIf the responder is not specified, the graph is non-blocking.\n# use built-in storey class or our custom Echo class to create and link Task steps\ngraph.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})') \\\n.to(class_name=\"Echo\", name=\"pre-process\", some_arg='abc').error_handler(\"catcher\")\n# add an Ensemble router with two child models (routes), the \"*\" prefix mark it is a router class\nrouter = graph.add_step(\"*mlrun.serving.VotingEnsemble\", name=\"ensemble\", after=\"pre-process\")\nrouter.add_route(\"m1\", class_name=\"ClassifierModel\", model_path=path1)\nrouter.add_route(\"m2\", class_name=\"ClassifierModel\", model_path=path2)\n# add the final step (after the router) which handles post processing and respond to the client\ngraph.add_step(class_name=\"Echo\", name=\"final\", after=\"ensemble\").respond()\n# add error handling step, run only when/if the \"pre-process\" step fail (keep after=\"\")\ngraph.add_step(handler=\"error_catcher\", name=\"catcher\", full_event=True, after=\"\")\n# plot the graph (using Graphviz) and run a test\ngraph.plot(rankdir='LR')\nCreate a mock (test) server, and run a test. Use wait_for_completion()\nto wait for the async event loop to complete.\nserver = function.to_mock_server()\nresp = server.test(\"/v2/models/m2/infer\", body={\"inputs\": data})\nserver.wait_for_completion()\nAnd deploy the graph as a real-time Nuclio serverless function with one command:\nfunction.deploy()\nNote\nIf you test a Nuclio function that has a serving graph with the async engine via the Nuclio UI, the UI might not display the logs in the output.\nExample of an NLP processing pipeline with real-time streaming#\nIn some cases it’s useful to split your processing to multiple functions and use\nstreaming protocols to connect those functions. In this example the data\nprocessing is in the first function/container and the NLP processing is in the second function.\nIn this example the GPU contained in the second function.\nSee the full notebook example.\n# define a new real-time serving function (from code) with an async graph\nfn = mlrun.code_to_function(\"multi-func\", filename=\"./data_prep.py\", kind=\"serving\", image='mlrun/mlrun')\ngraph = fn.set_topology(\"flow\", engine=\"async\")\n# define the graph steps (DAG)\ngraph.to(name=\"load_url\", handler=\"load_url\")\\\n.to(name=\"to_paragraphs\", handler=\"to_paragraphs\")\\\n.to(\"storey.FlatMap\", \"flatten_paragraphs\", _fn=\"(event)\")\\\n.to(\">>\", \"q1\", path=internal_stream)\\\n.to(name=\"nlp\", class_name=\"ApplyNLP\", function=\"enrich\")\\\n.to(name=\"extract_entities\", handler=\"extract_entities\", function=\"enrich\")\\\n.to(name=\"enrich_entities\", handler=\"enrich_entities\", function=\"enrich\")\\\n.to(\"storey.FlatMap\", \"flatten_entities\", _fn=\"(event)\", function=\"enrich\")\\\n.to(name=\"printer\", handler=\"myprint\", function=\"enrich\")\\\n.to(\">>\", \"output_stream\", path=out_stream)\n# specify the \"enrich\" child function, add extra package requirements\nchild = fn.add_child_function('enrich', './nlp.py', 'mlrun/mlrun')\nchild.spec.build.commands = [\"python -m pip install spacy\",\n\"python -m spacy download en_core_web_sm\"]\ngraph.plot()\nCurrently queues support iguazio v3io and Kafka streams."}
{"text": "Writing custom steps#\nThe Graph executes built-in task classes, or task classes and functions that you implement.\nThe task parameters include the following:\nclass_name (str): the relative or absolute class name.\nhandler (str): the function handler (if class_name is not specified it is the function handler).\n**class_args: a set of class __init__ arguments.\nFor example, see the following simple echo class:\nimport mlrun\n# mlrun: start\n# echo class, custom class example\nclass Echo:\ndef __init__(self, context, name=None, **kw):\nself.context = context\nself.name = name\nself.kw = kw\ndef do(self, x):\nprint(\"Echo:\", self.name, x)\nreturn x\n# mlrun: end\nTest the graph: first convert the code to function, and then add the step to the graph:\nfn_echo = mlrun.code_to_function(\"echo_function\", kind=\"serving\", image=\"mlrun/mlrun\")\ngraph_echo = fn_echo.set_topology(\"flow\")\ngraph_echo.to(class_name=\"Echo\", name=\"pre-process\", some_arg='abc')\ngraph_echo.plot(rankdir='LR')\nCreate a mock server to test this locally:\necho_server = fn_echo.to_mock_server(current_function=\"*\")\nresult = echo_server.test(\"\", {\"inputs\": 123})\nprint(result)\n{'id': '97397ea412334afdb5e4cb7d7c2e6dd3'}\nEcho: pre-process {'inputs': 123}\nFor more information, see the Advanced model serving graph notebook example\nYou can use any Python function by specifying the handler name (e.g. handler=json.dumps).\nThe function is triggered with the event.body as the first argument, and its result\nis passed to the next step.\nAlternatively, you can use classes that can also store some step/configuration and separate the\none time init logic from the per event logic. The classes are initialized with the class_args.\nIf the class init args contain context or name, they are initialized with the\ngraph context and the step name.\nBy default, the class_name and handler specify a class/function name in the globals() (i.e. this module).\nAlternatively, those can be full paths to the class (module.submodule.class), e.g. storey.WriteToParquet.\nYou can also pass the module as an argument to functions such as function.to_mock_server(namespace=module).\nIn this case the class or handler names are also searched in the provided module.\nWhen using classes the class event handler is invoked on every event with the event.body.\nIf the Task step full_event parameter is set to True the handler is invoked and returns\nthe full event object. If the class event handler is not specified, it invokes the class do() method.\nIf you need to implement async behavior, then subclass storey.MapClass."}
{"text": "Artifacts#\nAn artifact is any data that is produced and/or consumed by functions, jobs, or pipelines.\nArtifacts metadata is stored in the project’s database. The main types of artifacts are:\nFiles — files, directories, images, figures, and plotlines\nDatasets — any data, such as tables and DataFrames\nModels — all trained models\nFeature Store Objects — Feature sets and feature vectors\nIn this section\nViewing artifacts\nArtifact path\nSaving artifacts in run-specific paths\nArtifact URIs, versioning, and metadata\nSee also\nViewing artifacts#\nArtifacts that are stored in certain paths (see Artifact path) can be viewed and managed in the UI.\nIn the Project page, select the type of artifact you want to view from the left-hand menu:\nFeature Store (for feature-sets, feature-vectors and features), Datasets, Artifacts, or Models.\nExample dataset artifact screen:\nArtifacts that were generated by an MLRun job can also be viewed from the Jobs > Artifacts tab.\nYou can search the artifacts based on time and labels, and you can filter the artifacts by tag type.\nFor each artifact, you can view its content, its location, the artifact type, labels,\nthe producer of the artifact, the artifact owner, last update date, and type-specific information.\nYou can download the artifact. You can also tag and remove tags from artifacts using the UI.\nArtifact path#\nAny path that is supported by MLRun can be used to store artifacts. However, only artifacts that are stored in paths that are system-configured as “allowed” in the MLRun service are visible in the UI. These are:\nMLRun < 1.2: The allowed paths include only v3io paths\nMLRun 1.2 and higher: Allows cloud storage paths — v3io://, s3://, az://, gcs://, gs://\nJobs use the default or job specific artifact_path parameter to determine where the artifacts are stored.\nThe default artifact_path can be specified at the cluster level, client level, project level, or job level\n(at that precedence order), or can be specified as a parameter in the specific log operation.\nYou can set the default artifact_path for your environment using the set_environment() function.\nYou can override the default artifact_path configuration by setting the artifact_path parameter of\nthe set_environment() function. You can use variables in the artifacts path,\nsuch as {{project}} for the name of the running project or {{run.uid}} for the current job/pipeline run UID.\n(The default artifacts path uses {{project}}.) The following example configures the artifacts path to an\nartifacts directory in the current active directory (./artifacts)\nset_environment(project=project_name, artifact_path='./artifacts')\nFor Iguazio MLOps Platform users\nIn the platform, the default artifacts path is a /artifacts directory in the\npredefined “projects” data container: /v3io/projects/<project name>/artifacts\n(for example, /v3io/projects/myproject/artifacts for a “myproject” project).\nSaving artifacts in run-specific paths#\nWhen you specify {{run.uid}}, the artifacts for each job are stored in a dedicated directory for each executed job.\nUnder the artifact path, you should see the source-data file in a new directory whose name is derived from the unique run ID.\nOtherwise, the same artifacts directory is used in all runs, and the artifacts for newer runs override those from the previous runs.\nAs previously explained, set_environment returns a tuple with the project name and artifacts path.\nYou can optionally save your environment’s artifacts path to a variable, as demonstrated in the previous steps.\nYou can then use the artifacts-path variable to extract paths to task-specific artifact subdirectories.\nFor example, the following code extracts the path to the artifacts directory of a training task, and saves the path\nto a training_artifacts variable:\nfrom os import path\ntraining_artifacts = path.join(artifact_path, 'training')\nNote\nThe artifacts path uses data store URLs, which are not necessarily local file paths\n(for example, s3://bucket/path). Be careful not to use such paths with general file utilities.\nArtifact URIs, versioning, and metadata#\nArtifacts have unique URIs in the form store://<type>/<project>/<key/path>[:tag].\nThe URI is automatically generated by log_artifact and can be used as input to jobs, functions, pipelines, etc.\nArtifacts are versioned. Each unique version has a unique IDs (uid) and can have a tag label.\nWhen the tag is not specified, it uses the latest version.\nArtifact metadata and objects can be accessed through the SDK or downloaded from the UI (as YAML files).\nThey host common and object specific metadata such as:\nCommon metadata: name, project, updated, version info\nHow they were produced (user, job, pipeline, etc.)\nLineage data (sources used to produce that artifact)\nInformation about formats, schema, sample data\nLinks to other artifacts (e.g. a model can point to a chart)\nType-specific attributes\nArtifacts can be obtained via the SDK through type specific APIs or using generic artifact APIs such as:\nget_dataitem() - get the DataItem object for reading/downloading the artifact content\nget_store_resource() - get the artifact object\nExample artifact URLs:\nstore://artifacts/default/my-table\nstore://artifacts/sk-project/train-model:e95f757e-7959-4d66-b500-9f6cdb1f0bc7\nstore://feature-sets/stocks/quotes:v2\nstore://feature-vectors/stocks/enriched-ticker\nSee also#\nWorking with data and model artifacts\nModel Artifacts\nLogging datasets\nBack to top"}
{"text": "Data items#\nA data item can be one item or a or collection of items (file, dir, table, etc.).\nWhen running jobs or pipelines, data is passed using the DataItem objects. Data items objects abstract away\nthe data backend implementation, provide a set of convenience methods (.as_df, .get, .show, …), and enable auto logging/versioning\nof data and metadata.\nExample function:\ndef prep_data(context, source_url: mlrun.DataItem, label_column='label'):\n# Convert the DataItem to a Pandas DataFrame\ndf = source_url.as_df()\ndf = df.drop(label_column, axis=1).dropna()\ncontext.log_dataset('cleaned_data', df=df, index=False, format='csv')\nRunning the function:\nprep_data_run = data_prep_func.run(name='prep_data',\nhandler=prep_data,\ninputs={'source_url': source_url},\nparams={'label_column': 'userid'})\nIn order to call the function with an input you can use the inputs dictionary attribute. In order to pass\na simple parameter, use the params dictionary attribute. The input value is the specific item uri\n(per data store schema) as explained in Shared data stores.\nReading the data results from the run, you can easily get a run output artifact as a DataItem (so that you can view/use the artifact) using:\n# read the data locally as a Dataframe\nprep_data_run.artifact('cleaned_data').as_df()\nThe DataItem supports multiple convenience methods such as:\nget(), put() - to read/write data\ndownload(), upload() - to download/upload files\nas_df() - to convert the data to a DataFrame object\nlocal - to get a local file link to the data (that is downloaded locally if needed)\nlistdir(), stat - file system like methods\nmeta - access to the artifact metadata (in case of an artifact uri)\nshow() - visualizes the data in Jupyter (as image, html, etc.)\nSee the DataItem class documentation for details.  mlrun.datastore.DataItem\nIn order to get a DataItem object from a url use get_dataitem() or\nget_object() (returns the DataItem.get()).\nFor example:\ndf = mlrun.get_dataitem('s3://demo-data/mydata.csv').as_df()\nprint(mlrun.get_object('https://my-site/data.json'))"}
{"text": "Data stores#\nA data store defines a storage provider (e.g. file system, S3, Azure blob, Iguazio v3io, etc.).\nIn this section\nShared data stores\nStorage credentials and parameters\nShared data stores#\nMLRun supports multiple data stores. (More can easily added by extending the DataStore class.)\nData stores are referred to using the schema prefix (e.g. s3://my-bucket/path). The currently supported schemas and their urls:\nfiles — local/shared file paths, format: /file-dir/path/to/file\nhttp, https — read data from HTTP sources (read-only), format: https://host/path/to/file\ns3 — S3 objects (AWS or other endpoints), format: s3://<bucket>/path/to/file\nv3io, v3ios — Iguazio v3io data fabric, format: v3io://[<remote-host>]/<data-container>/path/to/file\naz — Azure Blob storage, format: az://<container>/path/to/file\ngs, gcs — Google Cloud Storage objects, format: gs://<bucket>/path/to/file\nstore — MLRun versioned artifacts (see Artifacts), format: store://artifacts/<project>/<artifact-name>[:tag]\nmemory — in memory data registry for passing data within the same process, format memory://key, use mlrun.datastore.set_in_memory_item(key, value) to register in memory data items (byte buffers or DataFrames).\nStorage credentials and parameters#\nData stores might require connection credentials. These can be provided through environment variables\nor project/job context secrets. The exact credentials depend on the type of the data store and are listed in\nthe following table. Each parameter specified can be provided as an environment variable, or as a project-secret that\nhas the same key as the name of the parameter.\nMLRun jobs executed remotely run in independent pods, with their own environment. When setting an environment\nvariable in the development environment (for example Jupyter), this has no effect on the executing pods. Therefore,\nbefore executing jobs that require access to storage credentials, these need to be provided by assigning environment\nvariables to the MLRun runtime itself, assigning secrets to it, or placing the variables in project-secrets.\nWarning\nPassing secrets as environment variables to runtimes is discouraged, as they are exposed in the pod spec.\nRefer to Working with secrets for details on secret handling in MLRun.\nFor example, running a function locally:\n# Access object in AWS S3, in the \"input-data\" bucket\nsource_url = \"s3://input-data/input_data.csv\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"<access key ID>\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<access key>\"\n# Execute a function that reads from the object pointed at by source_url.\n# When running locally, the function can use the local environment variables.\nlocal_run = func.run(name='aws_test', inputs={'source_url': source_url}, local=True)\nRunning the same function remotely:\n# Executing the function remotely using env variables (not recommended!)\nfunc.set_env(\"AWS_ACCESS_KEY_ID\", \"<access key ID>\").set_env(\"AWS_SECRET_ACCESS_KEY\", \"<access key>\")\nremote_run = func.run(name='aws_test', inputs={'source_url': source_url})\n# Using project-secrets (recommended) - project secrets are automatically mounted to project functions\nsecrets = {\"AWS_ACCESS_KEY_ID\": \"<access key ID>\", \"AWS_SECRET_ACCESS_KEY\": \"<access key>\"}\ndb = mlrun.get_run_db()\ndb.create_project_secrets(project=project_name, provider=\"kubernetes\", secrets=secrets)\nremote_run = func.run(name='aws_test', inputs={'source_url': source_url})\nThe following sections list the credentials and configuration parameters applicable to each storage type.\nv3io#\nWhen running in an Iguazio system, MLRun automatically configures executed functions to use v3io storage, and passes\nthe needed parameters (such as access-key) for authentication. Refer to the\nauto-mount section for more details on this process.\nIn some cases, the v3io configuration needs to be overridden. The following parameters can be configured:\nV3IO_API — URL pointing to the v3io web-API service.\nV3IO_ACCESS_KEY — access key used to authenticate with the web API.\nV3IO_USERNAME — the user-name authenticating with v3io. While not strictly required when using an access-key to\nauthenticate, it is used in several use-cases, such as resolving paths to the home-directory.\nS3#\nAWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY — access key\nparameters\nS3_ENDPOINT_URL — the S3 endpoint to use. If not specified, it defaults to AWS. For example, to access\na storage bucket in Wasabi storage, use S3_ENDPOINT_URL = \"https://s3.wasabisys.com\"\nMLRUN_AWS_ROLE_ARN — IAM role to assume.\nConnect to AWS using the secret key and access key, and assume the role whose ARN is provided. The\nARN must be of the format arn:aws:iam::<account-of-role-to-assume>:role/<name-of-role>\nAWS_PROFILE — name of credentials profile from a local AWS credentials file.\nWhen using a profile, the authentication secrets (if defined) are ignored, and credentials are retrieved from the\nfile. This option should be used for local development where AWS credentials already exist (created by aws CLI, for\nexample)\nAzure Blob storage#\nThe Azure Blob storage can utilize several methods of authentication. Each requires a different set of parameters as listed\nhere:\nAuthentication method\nParameters\nConnection string\nAZURE_STORAGE_CONNECTION_STRING\nSAS token\nAZURE_STORAGE_ACCOUNT_NAMEAZURE_STORAGE_SAS_TOKEN\nAccount key\nAZURE_STORAGE_ACCOUNT_NAMEAZURE_STORAGE_KEY\nService principal with a client secret\nAZURE_STORAGE_ACCOUNT_NAMEAZURE_STORAGE_CLIENT_IDAZURE_STORAGE_CLIENT_SECRETAZURE_STORAGE_TENANT_ID\nNote\nThe AZURE_STORAGE_CONNECTION_STRING configuration uses the BlobServiceClient to access objects. This has\nlimited functionality and cannot be used to access Azure Datalake storage objects. In this case use one of the other\nauthentication methods that use the fsspec mechanism.\nGoogle cloud storage#\nGOOGLE_APPLICATION_CREDENTIALS — path to the application credentials to use (in the form of a JSON file). This can\nbe used if this file is located in a location on shared storage, accessible to pods executing MLRun jobs.\nGCP_CREDENTIALS — when the credentials file cannot be mounted to the pod, this environment variable may contain\nthe contents of this file. If configured in the function pod, MLRun dumps its contents to a temporary file\nand points GOOGLE_APPLICATION_CREDENTIALS at it."}
{"text": "Model Artifacts#\nAn essential piece of artifact management and versioning is storing a model version. This allows the users to experiment with different models and compare their performance, without having to worry about losing their previous results.\nThe simplest way to store a model named my_model is with the following code:\nfrom pickle import dumps\nmodel_data = dumps(model)\ncontext.log_model(key='my_model', body=model_data, model_file='my_model.pkl')\nYou can also store any related metrics by providing a dictionary in the metrics parameter, such as metrics={'accuracy': 0.9}. Furthermore, any additional data that you would like to store along with the model can be specified in the extra_data parameter. For example extra_data={'confusion': confusion.target_path}\nA convenient utility method, eval_model_v2, which calculates mode metrics is available in mlrun.utils.\nSee example below for a simple model trained using scikit-learn (normally, you would send the data as input to the function). The last 2 lines evaluate the model and log the model.\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom pickle import dumps\nfrom mlrun.execution import MLClientCtx\nfrom mlrun.mlutils import eval_model_v2\ndef train_iris(context: MLClientCtx):\n# Basic scikit-learn iris SVM model\nX, y = datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42)\nmodel = linear_model.LogisticRegression(max_iter=10000)\nmodel.fit(X_train, y_train)\n# Evaluate model results and get the evaluation metrics\neval_metrics = eval_model_v2(context, X_test, y_test, model)\n# Log model\ncontext.log_model(\"model\",\nbody=dumps(model),\nartifact_path=context.artifact_subpath(\"models\"),\nextra_data=eval_metrics,\nmodel_file=\"model.pkl\",\nmetrics=context.results,\nlabels={\"class\": \"sklearn.linear_model.LogisticRegression\"})\nSave the code above to train_iris.py. The following code loads the function and runs it as a job. See the quick-start page to learn how to create the project and set the artifact path.\nfrom mlrun import code_to_function\ngen_func = code_to_function(name='train_iris',\nfilename='train_iris.py',\nhandler='train_iris',\nkind='job',\nimage='mlrun/ml-models')\ntrain_iris_func = project.set_function(gen_func).apply(auto_mount())\ntrain_iris = train_iris_func.run(name='train_iris',\nhandler='train_iris',\nartifact_path=artifact_path)\nYou can now use get_model to read the model and run it. This function will get the model file, metadata, and extra data. The input can be either the path of the model, or the directory where the model resides. If you provide a directory, the function will search for the model file (by default it searches for .pkl files)\nThe following example gets the model from models_path and test data in test_set with the expected label provided as a column of the test data. The name of the column containing the expected label is provided in label_column. The example then retrieves the models, runs the model with the test data and updates the model with the metrics and results of the test data.\nfrom pickle import load\nfrom mlrun.execution import MLClientCtx\nfrom mlrun.datastore import DataItem\nfrom mlrun.artifacts import get_model, update_model\nfrom mlrun.mlutils import eval_model_v2\ndef test_model(context: MLClientCtx,\nmodels_path: DataItem,\ntest_set: DataItem,\nlabel_column: str):\nif models_path is None:\nmodels_path = context.artifact_subpath(\"models\")\nxtest = test_set.as_df()\nytest = xtest.pop(label_column)\nmodel_file, model_obj, _ = get_model(models_path)\nmodel = load(open(model_file, 'rb'))\nextra_data = eval_model_v2(context, xtest, ytest.values, model)\nupdate_model(model_artifact=model_obj, extra_data=extra_data,\nmetrics=context.results, key_prefix='validation-')\nTo run the code, place the code above in test_model.py and use the following snippet. The model from the previous step is provided as the models_path:\nfrom mlrun.platforms import auto_mount\ngen_func = code_to_function(name='test_model',\nfilename='test_model.py',\nhandler='test_model',\nkind='job',\nimage='mlrun/ml-models')\nfunc = project.set_function(gen_func).apply(auto_mount())\nrun = func.run(name='test_model',\nhandler='test_model',\nparams={'label_column': 'label'},\ninputs={'models_path': train_iris.outputs['model'],\n'test_set': 'https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv'}),\nartifact_path=artifact_path)"}
{"text": "Using the built-in training function#\nThe MLRun Function Hub includes, among other things, training functions. The most commonly used function for training is auto_trainer, which includes the following handlers:\nTrain\nEvaluate\nTrain#\nThe main and default handler of any training function is called \"train\". In the Auto Trainer this handler performs\nan ML training function using SciKit-Learn’s API, meaning the function follows the structure below:\nGet the data: Get the dataset passed to a local path.\nSplit the data into datasets: Split the given data into a training set and a testing set.\nGet the model: Initialize a model instance out of a given class or load a provided model\nThe supported classes are anything based on sklearn.Estimator, xgboost.XGBModel, lightgbm.LGBMModel, including custom code as well.\nTrain: Call the model’s fit method to train it on the training set.\nTest: Test the model on the testing set.\nLog: Calculate the metrics and produce the artifacts to log the results and plots.\nMLRun orchestrates all of the above steps. The training is done with the shortcut function apply_mlrun that\nenables the automatic logging and additional features.\nTo start, run import mlrun and create a project:\nimport mlrun\n# Set the base project name\nproject_name_base = 'training-test'\n# Initialize the MLRun project object\nproject = mlrun.get_or_create_project(project_name_base, context=\"./\", user_project=True)\nNext, import the Auto Trainer from the Function Hub using MLRun’s import_function function:\nauto_trainer = project.set_function(mlrun.import_function(\"hub://auto_trainer\"))\nThe following example trains a Random Forest model:\ndataset_url = \"https://s3.wasabisys.com/iguazio/data/function-marketplace-data/xgb_trainer/classifier-data.csv\"\ntrain_run = auto_trainer.run(\nhandler=\"train\",\ninputs={\"dataset\": dataset_url},\nparams={\n# Model parameters:\n\"model_class\": \"sklearn.ensemble.RandomForestClassifier\",\n\"model_kwargs\": {\"max_depth\": 8},  # Could be also passed as \"MODEL_max_depth\": 8\n\"model_name\": \"MyModel\",\n# Dataset parameters:\n\"drop_columns\": [\"feat_0\", \"feat_2\"],\n\"train_test_split_size\": 0.2,\n\"random_state\": 42,\n\"label_columns\": \"labels\",\n}\n)\nOutputs#\ntrain_run.outputs returns all the outputs. The outputs are:\nTrained model: The trained model is logged as a ModelArtifact with all the following artifacts registered\nto it.\nTest dataset: The test set used to test the model post training is logged as a DatasetArtifact.\nPlots: Informative plots regarding the model like confusion matrix and features importance are drawn and logged\nas PlotArtifacts.\nResults: List of all the calculations of metrics tested on the testing set.\nFor instance, calling train_run.artifact('confusion-matrix').show() shows the following confusion matrix:\nParameters#\nTo view the parameters of train, expand the section below:\ntrain handler parameters:\nModel Parameters\nParameters to initialize a new model object or load a logged one for retraining.\nmodel_class: str — The class of the model to initialize. Can be a module path like\n\"sklearn.linear_model.LogisticRegression\" or a custom model passed through the custom objects parameters below.\nOnly one of model_class and model_path can be given.\nmodel_path: str — A ModelArtifact URI to load and retrain. Only one of model_class and model_path can be\ngiven.\nmodel_kwargs: dict — Additional parameters to pass onto the initialization of the model object (the model’s class\n__init__ method).\nData parameters\nParameters to get a dataset and prepare it for training, splitting into training and testing if required.\ndataset: Union[str, list, dict] — The dataset to train the model on.\nCan be passed as part of inputs to be parsed as mlrun.DataItem, meaning it supports either a URI or a\nFeatureVector.\nCan be passed as part of params, meaning it can be a list or a dict.\ndrop_columns: Union[str, int, List[str], List[int]] — Columns to drop from the dataset. Can be passed as strings\nrepresenting the column names or integers representing the column numbers.\ntest_set: Union[str, list, dict] — The test set to test the model with post training. Notice only one of\ntest_set or train_test_split_size is expected.\nCan be passed as part of inputs to be parsed as mlrun.DataItem, meaning it supports either a URI or a\nFeatureVector.\nCan be passed as part of params, meaning it can be a list or a dict.\ntrain_test_split_size: float = 0.2 — The proportion of the dataset to include in the test split. The size of the\nTraining set is set to the complement of this value. Must be between 0.0 and 1.0. Defaults to 0.2\nlabel_columns: Union[str, int, List[str], List[int]] — The target label(s) of the column(s) in the dataset. Can\nbe passed as strings representing the column names or integers representing the column numbers.\nrandom_state: int - Random state (seed) for train_test_split.\nTrain parameters\nParameters to pass to the fit method of the model object.\ntrain_kwargs: dict — Additional parameters to pass onto the fit method.\nLogging parameters\nParameters to control the automatic logging feature of MLRun. You can adjust the logging outputs as relevant and if\nnot passed, a default list of artifacts and metrics is produced and calculated.\nmodel_name: str = \"model” — The model’s name to use for storing the model artifact, defaults to ‘model’.\ntag: str — The model’s tag to log with.\nsample_set: Union[str, list, dict] — A sample set of inputs for the model for logging its stats alongside the model in\nfavor of model monitoring. If not given, the training set is used instead.\nCan be passed as part of inputs to be parsed as mlrun.DataItem, meaning it supports either a URI or a\nFeatureVector.\nCan be passed as part of params, meaning it can be a list or a dict.\n_artifacts: Dict[str, Union[list, dict]] — Additional artifacts to produce post training. See the\nArtifactsLibrary of the desired framework to see the available list of artifacts.\n_metrics: Union[List[str], Dict[str, Union[list, dict]]] — Additional metrics to calculate post training. See how\nto pass metrics and custom metrics in the MetricsLibrary of the desired framework.\napply_mlrun_kwargs: dict — Framework specific apply_mlrun key word arguments. Refer to the framework of choice\nto know more (SciKit-Learn, XGBoost or LightGBM)\nCustom objects parameters\nParameters to include custom objects like custom model class, metric code and artifact plan. Keep in mind that the\nmodel artifact created is logged with the custom objects, so if model_path is used, the custom objects used to\ntrain it are not required for loading it, it happens automatically.\ncustom_objects_map: Union[str, Dict[str, Union[str, List[str]]]] — A map of all the custom objects required for\nloading, training and testing the model. Can be passed as a dictionary or a json file path. Each key is a path to a\npython file and its value is the custom object name to import from it. If multiple objects needed to be imported from\nthe same py file a list can be given. For example:\n{\n\"/.../custom_model.py\": \"MyModel\",\n\"/.../custom_objects.py\": [\"object1\", \"object2\"]\n}\nAll the paths are accessed from the given ‘custom_objects_directory’, meaning each py file is read from\n‘custom_objects_directory/’. If the model path given is of a store object, the custom objects map is\nread from the logged custom object map artifact of the model.\nNote\nThe custom objects are imported in the order they came in this dictionary (or json). If a custom\nobject is dependant on another, make sure to put it below the one it relies on.\ncustom_objects_directory: Path to the directory with all the python files required for the custom objects. Can be\npassed as a zip file as well (and are extracted during the start of the run).\nNote\nThe parameters for additional arguments model_kwargs, train_kwargs and apply_mlrun_kwargs can be\nalso passed in the global kwargs with the matching prefixes: \"MODEL_\", \"TRAIN_\", \"MLRUN_\".\nEvaluate#\nThe \"evaluate\" handler is used to test the model on a given testing set and log its results. This is a common phase in\nevery model lifecycle and should be done periodically on updated testing sets to confirm that your model is still relevant.\nThe function uses SciKit-Learn’s API for evaluation, meaning the function follows the structure below:\nGet the data: Get the testing dataset passed to a local path.\nGet the model: Get the model object out of the ModelArtifact URI.\nPredict: Call the model’s predict (and predict_proba if needed) method to test it on the testing set.\nLog: Test the model on the testing set and log the results and artifacts.\nMLRun orchestrates all of the above steps. The evaluation is done with the shortcut function apply_mlrun that\nenables the automatic logging and further features.\nTo evaluate the test-set, use the following command:\nevaluate_run = auto_trainer.run(\nhandler=\"evaluate\",\ninputs={\"dataset\": train_run.outputs['test_set']},\nparams={\n\"model\": train_run.outputs['model'],\n\"label_columns\": \"labels\",\n},\n)\nOutputs#\nevaluate_run.outputs returns all the outputs. The outputs are:\nEvaluated model: The evaluated model’s ModelArtifact  is updated with all the following artifacts registered\nto it.\nTest dataset: The test set used to test the model post-training is logged as a DatasetArtifact.\nPlots: Informative plots regarding the model like confusion matrix and features importance are drawn and logged\nas PlotArtifacts.\nResults: List of all the calculations of metrics tested on the testing set.\nParameters#\nTo view the parameters of evaluate, expand the section below:\nevaluate handler parameters:\nModel Parameters\nParameters to load a logged model.\nmodel_path: str — A ModelArtifact URI to load.\nData parameters\nParameters to get a dataset and prepare it for training, splitting into training and testing if required.\ndataset: Union[str, list, dict] — The dataset to train the model on.\nCan be passed as part of inputs to be parsed as mlrun.DataItem, meaning it supports either a URI or a\nFeatureVector.\nCan be passed as part of params, meaning it can be a list or a dict.\ndrop_columns: Union[str, int, List[str], List[int]] — columns to drop from the dataset. Can be passed as strings\nrepresenting the column names or integers representing the column numbers.\nlabel_columns: Union[str, int, List[str], List[int]] — The target label(s) of the column(s) in the dataset. Can\nbe passed as strings representing the column names or integers representing the column numbers.\nPredict parameters\nParameters to pass to the predict method of the model object.\npredict_kwargs: dict — Additional parameters to pass onto the predict method.\nLogging parameters\nParameters to control the automatic logging feature of MLRun. You can adjust the logging outputs as relervant, and if\nnot passed, a default list of artifacts and metrics is produced and calculated.\n_artifacts: Dict[str, Union[list, dict]] — Additional artifacts to produce post training. See the\nArtifactsLibrary of the desired framework to see the available list of artifacts.\n_metrics: Union[List[str], Dict[str, Union[list, dict]]] — Additional metrics to calculate post training. See how\nto pass metrics and custom metrics in the MetricsLibrary of the desired framework.\napply_mlrun_kwargs: dict — Framework specific apply_mlrun key word arguments. Refer to the framework of choice\nto know more (SciKit-Learn, XGBoost or LightGBM).\nCustom objects parameters\nParameters to include custom objects for the evaluation like custom metric code and artifact plans. Keep in mind that\nthe custom objects used to train the model are not required for loading it, it happens automatically.\ncustom_objects_map: Union[str, Dict[str, Union[str, List[str]]]] — A map of all the custom objects required for\nloading, training and testing the model. Can be passed as a dictionary or a json file path. Each key is a path to a\npython file and its value is the custom object name to import from it. If multiple objects needed to be imported from\nthe same py file a list can be given. For example:\n{\n\"/.../custom_metric.py\": \"MyMetric\",\n\"/.../custom_plans.py\": [\"plan1\", \"plan2\"]\n}\nAll the paths are accessed from the given ‘custom_objects_directory’, meaning each py file is read from the\n‘custom_objects_directory/’. If the model path given is of a store object, the custom objects map is\nread from the logged custom object map artifact of the model.\nNote\nThe custom objects are imported in the order they came in this dictionary (or json). If a\ncustom object is depended on another, make sure to put it below the one it relies on.\ncustom_objects_directory — Path to the directory with all the python files required for the custom objects. Can be\npassed as a zip file as well (iti is extracted during the start of the run).\nNote\nThe parameters for additional arguments predict_kwargs and apply_mlrun_kwargs can be also\npassed in the global kwargs with the matching prefixes: \"PREDICT_\", \"MLRUN_\"."}
{"text": "Create a basic training job#\nIn this section, you create a simple job to train a model and log metrics, logs, and plots using MLRun’s auto-logging:\nDefine the training code\nCreate the job\nRun the job\nView job results\nDefine the training code#\nThe code you run is as follows. Notice, there is only a single line from MLRun to add all the MLOps capabilities:\n%%writefile trainer.py\nfrom sklearn import ensemble\nfrom sklearn.model_selection import train_test_split\nimport mlrun\nfrom mlrun.frameworks.sklearn import apply_mlrun\ndef train(\ndataset: mlrun.DataItem,  # data inputs are of type DataItem (abstract the data source)\nlabel_column: str = \"label\",\nn_estimators: int = 100,\nlearning_rate: float = 0.1,\nmax_depth: int = 3,\nmodel_name: str = \"cancer_classifier\",\n):\n# Get the input dataframe (Use DataItem.as_df() to access any data source)\ndf = dataset.as_df()\n# Initialize the x & y data\nX = df.drop(label_column, axis=1)\ny = df[label_column]\n# Train/Test split the dataset\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42\n)\n# Pick an ideal ML model\nmodel = ensemble.GradientBoostingClassifier(\nn_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth\n)\n# -------------------- The only line you need to add for MLOps -------------------------\n# Wraps the model with MLOps (test set is provided for analysis & accuracy measurements)\napply_mlrun(model=model, model_name=model_name, x_test=X_test, y_test=y_test)\n# --------------------------------------------------------------------------------------\n# Train the model\nmodel.fit(X_train, y_train)\nWriting trainer.py\nCreate the job#\nNext, use code_to_function to package up the Job to get ready to execute on the cluster:\nimport mlrun\ntraining_job = mlrun.code_to_function(\nname=\"basic-training\",\nfilename=\"trainer.py\",\nkind=\"job\",\nimage=\"mlrun/mlrun\",\nhandler=\"train\"\n)\nRun the job#\nFinally, run the job. The dataset is from S3, but usually it is the output from a previous step in a pipeline.\nrun = training_job.run(\ninputs={\"dataset\": \"https://igz-demo-datasets.s3.us-east-2.amazonaws.com/cancer-dataset.csv\"},\nparams = {\"n_estimators\": 100, \"learning_rate\": 1e-1, \"max_depth\": 3}\n)\n> 2022-07-22 22:27:15,162 [info] starting run basic-training-train uid=bc1c6ad491c340e1a3b9b91bb520454f DB=http://mlrun-api:8080\n> 2022-07-22 22:27:15,349 [info] Job is running in the background, pod: basic-training-train-kkntj\n> 2022-07-22 22:27:20,927 [info] run executed, status=completed\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ndefault\n...b520454f\n0\nJul 22 22:27:18\ncompleted\nbasic-training-train\nv3io_user=nickkind=jobowner=nickmlrun/client_version=1.0.4host=basic-training-train-kkntj\ndataset\nn_estimators=100learning_rate=0.1max_depth=3\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\nfeature-importancetest_setconfusion-matrixroc-curvescalibration-curvemodel\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2022-07-22 22:27:21,640 [info] run executed, status=completed\nView job results#\nOnce the job is complete, you can view the output metrics and visualize the artifacts.\nrun.outputs\n{'accuracy': 0.956140350877193,\n'f1_score': 0.965034965034965,\n'precision_score': 0.9583333333333334,\n'recall_score': 0.971830985915493,\n'feature-importance': 'v3io:///projects/default/artifacts/feature-importance.html',\n'test_set': 'store://artifacts/default/basic-training-train_test_set:bc1c6ad491c340e1a3b9b91bb520454f',\n'confusion-matrix': 'v3io:///projects/default/artifacts/confusion-matrix.html',\n'roc-curves': 'v3io:///projects/default/artifacts/roc-curves.html',\n'calibration-curve': 'v3io:///projects/default/artifacts/calibration-curve.html',\n'model': 'store://artifacts/default/cancer_classifier:bc1c6ad491c340e1a3b9b91bb520454f'}\nrun.artifact(\"confusion-matrix\").show()\nrun.artifact(\"feature-importance\").show()\nrun.artifact(\"test_set\").show()\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\nlabel\n0\n12.47\n18.60\n81.09\n481.9\n0.09965\n0.10580\n0.08005\n0.03821\n0.1925\n0.06373\n...\n24.64\n96.05\n677.9\n0.1426\n0.2378\n0.2671\n0.10150\n0.3014\n0.08750\n1\n1\n18.94\n21.31\n123.60\n1130.0\n0.09009\n0.10290\n0.10800\n0.07951\n0.1582\n0.05461\n...\n26.58\n165.90\n1866.0\n0.1193\n0.2336\n0.2687\n0.17890\n0.2551\n0.06589\n0\n2\n15.46\n19.48\n101.70\n748.9\n0.10920\n0.12230\n0.14660\n0.08087\n0.1931\n0.05796\n...\n26.00\n124.90\n1156.0\n0.1546\n0.2394\n0.3791\n0.15140\n0.2837\n0.08019\n0\n3\n12.40\n17.68\n81.47\n467.8\n0.10540\n0.13160\n0.07741\n0.02799\n0.1811\n0.07102\n...\n22.91\n89.61\n515.8\n0.1450\n0.2629\n0.2403\n0.07370\n0.2556\n0.09359\n1\n4\n11.54\n14.44\n74.65\n402.9\n0.09984\n0.11200\n0.06737\n0.02594\n0.1818\n0.06782\n...\n19.68\n78.78\n457.8\n0.1345\n0.2118\n0.1797\n0.06918\n0.2329\n0.08134\n1\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n109\n14.64\n16.85\n94.21\n666.0\n0.08641\n0.06698\n0.05192\n0.02791\n0.1409\n0.05355\n...\n25.44\n106.00\n831.0\n0.1142\n0.2070\n0.2437\n0.07828\n0.2455\n0.06596\n1\n110\n16.07\n19.65\n104.10\n817.7\n0.09168\n0.08424\n0.09769\n0.06638\n0.1798\n0.05391\n...\n24.56\n128.80\n1223.0\n0.1500\n0.2045\n0.2829\n0.15200\n0.2650\n0.06387\n0\n111\n11.52\n14.93\n73.87\n406.3\n0.10130\n0.07808\n0.04328\n0.02929\n0.1883\n0.06168\n...\n21.19\n80.88\n491.8\n0.1389\n0.1582\n0.1804\n0.09608\n0.2664\n0.07809\n1\n112\n14.22\n27.85\n92.55\n623.9\n0.08223\n0.10390\n0.11030\n0.04408\n0.1342\n0.06129\n...\n40.54\n102.50\n764.0\n0.1081\n0.2426\n0.3064\n0.08219\n0.1890\n0.07796\n1\n113\n20.73\n31.12\n135.70\n1419.0\n0.09469\n0.11430\n0.13670\n0.08646\n0.1769\n0.05674\n...\n47.16\n214.00\n3432.0\n0.1401\n0.2644\n0.3442\n0.16590\n0.2868\n0.08218\n0\n114 rows × 31 columns"}
{"text": "Working with data and model artifacts#\nWhen running a training job, you need to pass in the data used for training, and save the resulting model. Both the data and model can be considered artifacts in MLRun. In the context of an ML pipeline, the data is an input and the model is an output.\nConsider the following snippet from a pipeline in the Build and run automated ML pipelines and CI/CD section of the docs:\n# Ingest data\n...\n# Train a model using the auto_trainer hub function\ntrain = mlrun.run_function(\n\"hub://auto_trainer\",\ninputs={\"dataset\": ingest.outputs[\"dataset\"]},\nparams = {\n\"model_class\": \"sklearn.ensemble.RandomForestClassifier\",\n\"train_test_split_size\": 0.2,\n\"label_columns\": \"label\",\n\"model_name\": 'cancer',\n},\nhandler='train',\noutputs=[\"model\"],\n)\n### Deploy model\n...\nThis snippet trains a model using the data provided into inputs and passes the model to the rest of the pipeline using the outputs.\nInput data#\nThe inputs parameter is a dictionary of key-value mappings. In this case, the input is the dataset (which is actually an output from a previous step). Within the training job, you can access the dataset input as an MLRun Data items (essentially a smart data pointer that provides convenience methods).\nFor example, this Python training function is expecting a parameter called dataset that is of type DataItem. Within the function, you can get the training set as a Pandas dataframe via the following:\nimport mlrun\ndef train(context: mlrun.MLClientCtx, dataset: mlrun.DataItem, ...):\ndf = dataset.as_df()\nNotice how this maps to the parameter datasets that you passed into your inputs.\nOutput model#\nThe outputs parameter is a list of artifacts that were logged during the job. In this case, it is your newly trained model, however it could also be a dataset or plot. These artifacts are logged using the experiment tracking hooks via the MLRun execution context.\nOne way to log models is via MLRun auto-logging with apply_mlrun. This saves the model, test sets, visualizations, and more as outputs. Additionally, you can use manual hooks to save datasets and models. For example, this Python training function uses both auto logging and manual logging:\nimport mlrun\nfrom mlrun.frameworks.sklearn import apply_mlrun\nfrom sklearn import ensemble\nimport cloudpickle\ndef train(context: mlrun.MLClientCtx, dataset: mlrun.DataItem, ...):\n# Prep data using df\ndf = dataset.as_df()\nX_train, X_test, y_train, y_test = ...\n# Apply auto logging\nmodel = ensemble.GradientBoostingClassifier(...)\napply_mlrun(model=model, model_name=model_name, x_test=X_test, y_test=y_test)\n# Train\nmodel.fit(X_train, y_train)\n# Manual logging\ncontext.log_dataset(key=\"X_test_dataset\", df=X_test)\ncontext.log_model(key=\"my_model\", body=cloudpickle.dumps(model), model_file=\"model.pkl\")\nOnce your artifact is logged, it can be accessed throughout the rest of the pipeline. For example, for the pipeline snippet from the Build and run automated ML pipelines and CI/CD section of the docs, you can access your model like the following:\n# Train a model using the auto_trainer hub function\ntrain = mlrun.run_function(\n\"hub://auto_trainer\",\ninputs={\"dataset\": ingest.outputs[\"dataset\"]},\n...\noutputs=[\"model\"],\n)\n# Get trained model\nmodel = train.outputs[\"model\"]\nNotice how this maps to the parameter model that you passed into your outputs.\nModel artifacts#\nBy storing multiple models, you can experiment with them,\nand compare their performance, without having to worry about losing the previous results.\nThe simplest way to store a model named my_model is with the following code:\nfrom pickle import dumps\nmodel_data = dumps(model)\ncontext.log_model(key='my_model', body=model_data, model_file='my_model.pkl')\nYou can also store any related metrics by providing a dictionary in the metrics parameter, such as metrics={'accuracy': 0.9}.\nFurthermore, any additional data that you would like to store along with the model can be specified in the extra_data parameter. For example extra_data={'confusion': confusion.target_path}\nA convenient utility method, eval_model_v2, which calculates mode metrics is available in mlrun.utils.\nSee example below for a simple model trained using scikit-learn (normally, you would send the data as input to the function). The last two\nlines evaluate the model and log the model.\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom pickle import dumps\nfrom mlrun.execution import MLClientCtx\nfrom mlrun.mlutils import eval_model_v2\ndef train_iris(context: MLClientCtx):\n# Basic scikit-learn iris SVM model\nX, y = datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42)\nmodel = linear_model.LogisticRegression(max_iter=10000)\nmodel.fit(X_train, y_train)\n# Evaluate model results and get the evaluation metrics\neval_metrics = eval_model_v2(context, X_test, y_test, model)\n# Log model\ncontext.log_model(\"model\",\nbody=dumps(model),\nartifact_path=context.artifact_subpath(\"models\"),\nextra_data=eval_metrics,\nmodel_file=\"model.pkl\",\nmetrics=context.results,\nlabels={\"class\": \"sklearn.linear_model.LogisticRegression\"})\nSave the code above to train_iris.py. The following code loads the function and runs it as a job. See the Quick start tutorial to learn how to create the project and set the artifact path.\nfrom mlrun import code_to_function\ngen_func = code_to_function(name='train_iris',\nfilename='train_iris.py',\nhandler='train_iris',\nkind='job',\nimage='mlrun/ml-models')\ntrain_iris_func = project.set_function(gen_func).apply(auto_mount())\ntrain_iris = train_iris_func.run(name='train_iris',\nhandler='train_iris',\nartifact_path=artifact_path)\nYou can now use get_model to read the model and run it. This function gets the model file, metadata, and extra data. The input can be\neither the path of the model, or the directory where the model resides. If you provide a directory, the function searches for the model file\n(by default it searches for .pkl files)\nThe following example gets the model from models_path and test data in test_set with the expected label provided as a column of the test\ndata. The name of the column containing the expected label is provided in label_column. The example then retrieves the models, runs the\nmodel with the test data and updates the model with the metrics and results of the test data.\nfrom pickle import load\nfrom mlrun.execution import MLClientCtx\nfrom mlrun.datastore import DataItem\nfrom mlrun.artifacts import get_model, update_model\nfrom mlrun.mlutils import eval_model_v2\ndef test_model(context: MLClientCtx,\nmodels_path: DataItem,\ntest_set: DataItem,\nlabel_column: str):\nif models_path is None:\nmodels_path = context.artifact_subpath(\"models\")\nxtest = test_set.as_df()\nytest = xtest.pop(label_column)\nmodel_file, model_obj, _ = get_model(models_path)\nmodel = load(open(model_file, 'rb'))\nextra_data = eval_model_v2(context, xtest, ytest.values, model)\nupdate_model(model_artifact=model_obj, extra_data=extra_data,\nmetrics=context.results, key_prefix='validation-')\nTo run the code, place the code above in test_model.py and use the following snippet. The model from the previous step is provided as the models_path:\nfrom mlrun.platforms import auto_mount\ngen_func = code_to_function(name='test_model',\nfilename='test_model.py',\nhandler='test_model',\nkind='job',\nimage='mlrun/ml-models')\nfunc = project.set_function(gen_func).apply(auto_mount())\nrun = func.run(name='test_model',\nhandler='test_model',\nparams={'label_column': 'label'},\ninputs={'models_path': train_iris.outputs['model'],\n'test_set': 'https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv'}),\nartifact_path=artifact_path)\nPlot artifacts#\nStoring plots is useful to visualize the data and to show any information regarding the model performance. For example, you can store\nscatter plots, histograms and cross-correlation of the data, and for the model store the ROC curve and confusion matrix.\nThe following code creates a confusion matrix plot using sklearn.metrics.plot_confusion_matrix\nand stores the plot in the artifact repository:\nfrom mlrun.artifacts import PlotArtifact\nfrom mlrun.mlutils import gcf_clear\ngcf_clear(plt)\nconfusion_matrix = metrics.plot_confusion_matrix(model,\nxtest,\nytest,\nnormalize='all',\nvalues_format = '.2g',\ncmap=plt.cm.Blues)\nconfusion_matrix = context.log_artifact(PlotArtifact('confusion-matrix', body=confusion_matrix.figure_),\nlocal_path='plots/confusion_matrix.html')\nYou can use the update_dataset_meta function to associate the plot with the dataset by assigning the value of the extra_data parameter:\nfrom mlrun.artifacts import update_dataset_meta\nextra_data = {'confusion_matrix': confusion_matrix}\nupdate_dataset_meta(dataset, extra_data=extra_data)"}
{"text": "Quick start tutorial#\nIntroduction to MLRun - Use serverless functions to train and deploy models\nThis notebook provides a quick overview of developing and deploying machine learning applications using the MLRun MLOps orchestration framework.\nTutorial steps:\nInstall MLRun\nDefine the MLRun project and ML functions\nRun the data processing function and log artifacts\nUse the MLRun built-in Function Hub functions for training\nBuild, test, and deploy model serving functions\nWatch the video tutorial.\nInstall MLRun#\nMLRun has a backend service that can run locally or over Kubernetes (preferred). See the instructions for installing it locally using Docker or over Kubernetes Cluster. Alternatively, you can use Iguazio’s managed MLRun service.\nBefore you start, make sure the MLRun client package is installed and configured properly:\nThis notebook uses sklearn. If it is not installed in your environment run !pip install scikit-learn~=1.0.\n# Install MLRun and sklearn, run this only once (restart the notebook after the install !!!)\n%pip install mlrun scikit-learn~=1.0\nRestart the notebook kernel after the pip installation.\nimport mlrun\nConfigure the client environment#\nMLRun client connects to the local or remote MLRun service/cluster using a REST API. To configure the service address, credentials, and default settings, you use the mlrun.set_environment() method, or environment variables, (see details in Set up your client environment.)\nYou can skip this step when using MLRun Jupyter notebooks or Iguazio’s managed notebooks.\nDefine MLRun project and ML functions#\nMLRun Project is a container for all your work on a particular activity or application. Projects host functions, workflow,\nartifacts, secrets, and more. Projects have access control and can be accessed by one or more users. They are usually associated with a GIT and interact with CI/CD frameworks for automation.\nSee the MLRun Projects documentation.\nCreate a new project\nproject = mlrun.get_or_create_project(\"quick-tutorial\", \"./\", user_project=True)\n> 2022-09-20 13:19:49,414 [info] loaded project quick-tutorial from MLRun DB\nMLRun serverless functions specify the source code, base image, extra package requirements, runtime engine kind (batch job, real-time serving, spark, dask, etc.), and desired resources (cpu, gpu, mem, storage, …). The runtime engines (local, job, Nuclio, Spark, etc.) automatically transform the function code and spec into fully managed and elastic services that run over Kubernetes.\nFunction source code can come from a single file (.py, .ipynb, etc.) or a full archive (git, zip, tar). MLRun can execute an entire file/notebook or specific function classes/handlers.\nNote\nThe @mlrun.handler is a decorator that logs the returning values to MLRun as configured. This example uses the default settings so that it logs a dataset (pd.DataFrame) and a string value by getting the returned objects types. In addition to logging outputs, the decorator can parse incoming inputs to the required type. For more info, see the mlrun.handler documentation.\nFunction code\nRun the following cell to generate the data prep file (or copy it manually):\n%%writefile data-prep.py\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nimport mlrun\n@mlrun.handler(outputs=[\"dataset\", \"label_column\"])\ndef breast_cancer_generator():\n\"\"\"\nA function which generates the breast cancer dataset\n\"\"\"\nbreast_cancer = load_breast_cancer()\nbreast_cancer_dataset = pd.DataFrame(\ndata=breast_cancer.data, columns=breast_cancer.feature_names\n)\nbreast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"label\"])\nbreast_cancer_dataset = pd.concat(\n[breast_cancer_dataset, breast_cancer_labels], axis=1\n)\nreturn breast_cancer_dataset, \"label\"\nOverwriting data-prep.py\nCreate a serverless function object from the code above, and register it in the project\ndata_gen_fn = project.set_function(\"data-prep.py\", name=\"data-prep\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"breast_cancer_generator\")\nproject.save()  # save the project with the latest config\n<mlrun.projects.project.MlrunProject at 0x7ff72063d460>\nRun your data processing function and log artifacts#\nFunctions are executed (using the CLI or SDK run command) with an optional handler, various params, inputs, and resource requirements. This generates a run object that can be tracked through the CLI, UI, and SDK. Multiple functions can be executed and tracked as part of a multi-stage pipeline (workflow).\nNote\nWhen a function has additional package requirements, or needs to include the content of a source archive,\nyou must first build the function using the project.build_function() method.\nThe local flag indicates if the function is executed locally or “teleported” and executed in the Kubernetes cluster. The execution progress and results can be viewed in the UI (see hyperlinks below).\nRun using the SDK\ngen_data_run = project.run_function(\"data-prep\", local=True)\n> 2022-09-20 13:22:59,351 [info] starting run data-prep-breast_cancer_generator uid=1ea3533192364dbc8898ce328988d0a3 DB=http://mlrun-api:8080\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\nquick-tutorial-iguazio\n...8988d0a3\n0\nSep 20 13:22:59\ncompleted\ndata-prep-breast_cancer_generator\nv3io_user=iguaziokind=owner=iguaziohost=jupyter-5654cb444f-c9wk2\nlabel_column=label\ndataset\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2022-09-20 13:22:59,693 [info] run executed, status=completed\nPrint the run state and outputs\ngen_data_run.state()\n'completed'\ngen_data_run.outputs\n{'label_column': 'label',\n'dataset': 'store://artifacts/quick-tutorial-iguazio/data-prep-breast_cancer_generator_dataset:1ea3533192364dbc8898ce328988d0a3'}\nPrint the output dataset artifact (DataItem object) as dataframe\ngen_data_run.artifact(\"dataset\").as_df().head()\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\nlabel\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0\n5 rows × 31 columns\nTrain a model using an MLRun built-in Function Hub#\nMLRun provides a Function Hub that hosts a set of pre-implemented and\nvalidated ML, DL, and data processing functions.\nYou can import the auto-trainer hub function that can: train an ML model using a variety of ML frameworks; generate\nvarious metrics and charts; and log the model along with its metadata into the MLRun model registry.\n# Import the function\ntrainer = mlrun.import_function('hub://auto_trainer')\nSee the auto_trainer function usage instructions in the Function Hub or by typing trainer.doc()\nRun the function on the cluster (if there is)\ntrainer_run = project.run_function(trainer,\ninputs={\"dataset\": gen_data_run.outputs[\"dataset\"]},\nparams = {\n\"model_class\": \"sklearn.ensemble.RandomForestClassifier\",\n\"train_test_split_size\": 0.2,\n\"label_columns\": \"label\",\n\"model_name\": 'cancer',\n},\nhandler='train',\n)\n> 2022-09-20 13:23:14,811 [info] starting run auto-trainer-train uid=84057e1510174611a5d2de0671ee803e DB=http://mlrun-api:8080\n> 2022-09-20 13:23:14,970 [info] Job is running in the background, pod: auto-trainer-train-dzjwz\nMatplotlib created a temporary config/cache directory at /tmp/matplotlib-3pzdch1o because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n> 2022-09-20 13:23:20,953 [info] Sample set not given, using the whole training set as the sample set\n> 2022-09-20 13:23:21,143 [info] training 'cancer'\n> 2022-09-20 13:23:22,561 [info] run executed, status=completed\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\nquick-tutorial-iguazio\n...71ee803e\n0\nSep 20 13:23:20\ncompleted\nauto-trainer-train\nv3io_user=iguaziokind=jobowner=iguaziomlrun/client_version=1.1.0host=auto-trainer-train-dzjwz\ndataset\nmodel_class=sklearn.ensemble.RandomForestClassifiertrain_test_split_size=0.2label_columns=labelmodel_name=cancer\naccuracy=0.956140350877193f1_score=0.967741935483871precision_score=0.9615384615384616recall_score=0.974025974025974\nfeature-importancetest_setconfusion-matrixroc-curvescalibration-curvemodel\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2022-09-20 13:23:24,216 [info] run executed, status=completed\nView the job progress results and the selected run in the MLRun UI\nResults (metrics) and artifacts are generated and tracked automatically by MLRun\ntrainer_run.outputs\n{'accuracy': 0.956140350877193,\n'f1_score': 0.967741935483871,\n'precision_score': 0.9615384615384616,\n'recall_score': 0.974025974025974,\n'feature-importance': 'v3io:///projects/quick-tutorial-iguazio/artifacts/auto-trainer-train/0/feature-importance.html',\n'test_set': 'store://artifacts/quick-tutorial-iguazio/auto-trainer-train_test_set:84057e1510174611a5d2de0671ee803e',\n'confusion-matrix': 'v3io:///projects/quick-tutorial-iguazio/artifacts/auto-trainer-train/0/confusion-matrix.html',\n'roc-curves': 'v3io:///projects/quick-tutorial-iguazio/artifacts/auto-trainer-train/0/roc-curves.html',\n'calibration-curve': 'v3io:///projects/quick-tutorial-iguazio/artifacts/auto-trainer-train/0/calibration-curve.html',\n'model': 'store://artifacts/quick-tutorial-iguazio/cancer:84057e1510174611a5d2de0671ee803e'}\n# Display HTML output artifacts\ntrainer_run.artifact('confusion-matrix').show()\nBuild, test, and deploy the model serving functions#\nMLRun serving can produce managed, real-time, serverless, pipelines composed of various data processing and ML tasks. The pipelines use the Nuclio real-time serverless engine, which can be deployed anywhere. For more details and examples, see MLRun serving graphs.\nCreate a model serving function\nserving_fn = mlrun.new_function(\"serving\", image=\"python:3.8\", kind=\"serving\", requirements=[\"mlrun[complete]\", \"scikit-learn==1.1.2\"])\nAdd a model\nThe basic serving topology supports a router with multiple child models attached to it.\nThe function.add_model() method allows you to add models and specify the name, model_path (to a model file, dir, or artifact), and the serving class (built-in or user defined).\nserving_fn.add_model('cancer-classifier',model_path=trainer_run.outputs[\"model\"], class_name='mlrun.frameworks.sklearn.SklearnModelServer')\n<mlrun.serving.states.TaskStep at 0x7ff6da1ac190>\n# Plot the serving graph topology\nserving_fn.spec.graph.plot(rankdir=\"LR\")\nSimulating the model server locally\n# Create a mock (simulator of the real-time function)\nserver = serving_fn.to_mock_server()\n> 2022-09-20 13:24:24,867 [warning] run command, file or code were not specified\n> 2022-09-20 13:24:25,240 [info] model cancer-classifier was loaded\n> 2022-09-20 13:24:25,241 [info] Loaded ['cancer-classifier']\nTest the mock model server endpoint\nList the served models\nserver.test(\"/v2/models/\", method=\"GET\")\n{'models': ['cancer-classifier']}\nInfer using test data\nmy_data = {\"inputs\"\n:[[\n1.371e+01, 2.083e+01, 9.020e+01, 5.779e+02, 1.189e-01, 1.645e-01,\n9.366e-02, 5.985e-02, 2.196e-01, 7.451e-02, 5.835e-01, 1.377e+00,\n3.856e+00, 5.096e+01, 8.805e-03, 3.029e-02, 2.488e-02, 1.448e-02,\n1.486e-02, 5.412e-03, 1.706e+01, 2.814e+01, 1.106e+02, 8.970e+02,\n1.654e-01, 3.682e-01, 2.678e-01, 1.556e-01, 3.196e-01, 1.151e-01]\n]\n}\nserver.test(\"/v2/models/cancer-classifier/infer\", body=my_data)\nX does not have valid feature names, but RandomForestClassifier was fitted with feature names\n{'id': '27d3f10a36ce465f841d3e19ca404889',\n'model_name': 'cancer-classifier',\n'outputs': [0]}\nRead the model name, ver and schema (input and output features)\nDeploy a real-time serving function (over Kubernetes or Docker)\nThis section requires Nuclio to be installed (over k8s or Docker).\nUse the mlrun deploy_function() method to build and deploy a Nuclio serving function from your serving-function code.\nYou can deploy the function object (serving_fn) or reference pre-registered project functions.\nserving_fn.with_code(body=\" \") # adds the serving wrapper, not required with MLRun >= 1.0.3\nproject.deploy_function(serving_fn)\n> 2022-09-20 13:24:34,823 [info] Starting remote function deploy\n2022-09-20 13:24:35  (info) Deploying function\n2022-09-20 13:24:35  (info) Building\n2022-09-20 13:24:35  (info) Staging files and preparing base images\n2022-09-20 13:24:35  (info) Building processor image\n2022-09-20 13:25:35  (info) Build complete\n2022-09-20 13:26:05  (info) Function deploy complete\n> 2022-09-20 13:26:06,030 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-quick-tutorial-iguazio-serving.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['quick-tutorial-iguazio-serving-quick-tutorial-iguazio.default-tenant.app.alexp-edge.lab.iguazeng.com/']}\nDeployStatus(state=ready, outputs={'endpoint': 'http://quick-tutorial-iguazio-serving-quick-tutorial-iguazio.default-tenant.app.alexp-edge.lab.iguazeng.com/', 'name': 'quick-tutorial-iguazio-serving'})\nTest the live endpoint\nserving_fn.invoke(\"/v2/models/cancer-classifier/infer\", body=my_data)\n> 2022-09-20 13:26:06,094 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-quick-tutorial-iguazio-serving.default-tenant.svc.cluster.local:8080/v2/models/cancer-classifier/infer'}\n{'id': '2533b72a-6d94-4c51-b960-02a2deaf84b6',\n'model_name': 'cancer-classifier',\n'outputs': [0]}\nDone!#\nCongratulation! You’ve completed Part 1 of the MLRun getting-started tutorial.\nProceed to Part 2: Train, Track, Compare, and Register Models to learn how to train an ML model."}
{"text": "Train, compare, and register models#\nThis notebook provides a quick overview of training ML models using MLRun MLOps orchestration framework.\nMake sure you reviewed the basics in MLRun Quick Start Tutorial.\nTutorial steps:\nDefine an MLRun project and a training functions\nRun the function, log the artifacts and model\nHyper-parameter tuning and model/experiment comparison\nBuild and test the model serving functions\nWatch the video tutorial.\nMLRun installation and configuration#\nBefore running this notebook make sure mlrun and sklearn packages are installed (pip install mlrun scikit-learn~=1.0) and that you have configured the access to the MLRun service.\n# Install MLRun if not installed, run this only once (restart the notebook after the install !!!)\n%pip install mlrun\nDefine MLRun project and a training functions#\nYou should create, load, or use (get) an MLRun project that holds all your functions and assets.\nGet or create a new project\nThe get_or_create_project() method tries to load the project from MLRun DB. If the project does not exist, it creates a new one.\nimport mlrun\nproject = mlrun.get_or_create_project(\"tutorial\", context=\"./\", user_project=True)\n> 2022-09-20 13:55:10,543 [info] loaded project tutorial from None or context and saved in MLRun DB\nAdd (auto) MLOps to your training function\nTraining functions generate models and various model statistics. You’ll want to store the models along with all the relevant data,\nmetadata, and measurements. MLRun can apply all the MLOps functionality automatically (“Auto-MLOps”) by simply using the framework-specific apply_mlrun() method.\nThis is the line to add to your code, as shown in the training function below.\napply_mlrun(model=model, model_name=\"my_model\", x_test=x_test, y_test=y_test)\napply_mlrun() manages the training process and automatically logs all the framework-specific model object, details, data, metadata, and metrics.\nIt accepts the model object and various optional parameters. When specifying the x_test and y_test data it generates various plots and calculations to evaluate the model.\nMetadata and parameters are automatically recorded (from MLRun context object) and therefore don’t need to be specified.\nFunction code\nRun the following cell to generate the trainer.py file (or copy it manually):\n%%writefile trainer.py\nimport pandas as pd\nfrom sklearn import ensemble\nfrom sklearn.model_selection import train_test_split\nimport mlrun\nfrom mlrun.frameworks.sklearn import apply_mlrun\n@mlrun.handler()\ndef train(\ndataset: pd.DataFrame,\nlabel_column: str = \"label\",\nn_estimators: int = 100,\nlearning_rate: float = 0.1,\nmax_depth: int = 3,\nmodel_name: str = \"cancer_classifier\",\n):\n# Initialize the x & y data\nx = dataset.drop(label_column, axis=1)\ny = dataset[label_column]\n# Train/Test split the dataset\nx_train, x_test, y_train, y_test = train_test_split(\nx, y, test_size=0.2, random_state=42\n)\n# Pick an ideal ML model\nmodel = ensemble.GradientBoostingClassifier(\nn_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth\n)\n# -------------------- The only line you need to add for MLOps -------------------------\n# Wraps the model with MLOps (test set is provided for analysis & accuracy measurements)\napply_mlrun(model=model, model_name=model_name, x_test=x_test, y_test=y_test)\n# --------------------------------------------------------------------------------------\n# Train the model\nmodel.fit(x_train, y_train)\nOverwriting trainer.py\nCreate a serverless function object from the code above, and register it in the project\ntrainer = project.set_function(\"trainer.py\", name=\"trainer\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"train\")\nRun the training function and log the artifacts and model#\nCreate a dataset for training\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nbreast_cancer = load_breast_cancer()\nbreast_cancer_dataset = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\nbreast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"label\"])\nbreast_cancer_dataset = pd.concat([breast_cancer_dataset, breast_cancer_labels], axis=1)\nbreast_cancer_dataset.to_csv(\"cancer-dataset.csv\", index=False)\nRun the function (locally) using the generated dataset\ntrainer_run = project.run_function(\n\"trainer\",\ninputs={\"dataset\": \"cancer-dataset.csv\"},\nparams = {\"n_estimators\": 100, \"learning_rate\": 1e-1, \"max_depth\": 3},\nlocal=True\n)\n> 2022-09-20 13:56:57,630 [info] starting run trainer-train uid=b3f1bc3379324767bee22f44942b96e4 DB=http://mlrun-api:8080\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ntutorial-iguazio\n...942b96e4\n0\nSep 20 13:56:57\ncompleted\ntrainer-train\nv3io_user=iguaziokind=owner=iguaziohost=jupyter-5654cb444f-c9wk2\ndataset\nn_estimators=100learning_rate=0.1max_depth=3\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\nfeature-importancetest_setconfusion-matrixroc-curvescalibration-curvemodel\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2022-09-20 13:56:59,356 [info] run executed, status=completed\nView the auto generated results and artifacts\ntrainer_run.outputs\n{'accuracy': 0.956140350877193,\n'f1_score': 0.965034965034965,\n'precision_score': 0.9583333333333334,\n'recall_score': 0.971830985915493,\n'feature-importance': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/0/feature-importance.html',\n'test_set': 'store://artifacts/tutorial-iguazio/trainer-train_test_set:b3f1bc3379324767bee22f44942b96e4',\n'confusion-matrix': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/0/confusion-matrix.html',\n'roc-curves': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/0/roc-curves.html',\n'calibration-curve': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/0/calibration-curve.html',\n'model': 'store://artifacts/tutorial-iguazio/cancer_classifier:b3f1bc3379324767bee22f44942b96e4'}\ntrainer_run.artifact('feature-importance').show()\nExport model files + metadata into a zip (requires MLRun 1.1.0 and later)\nYou can export() the model package (files + metadata) into a zip, and load it on a remote system/cluster by running model = project.import_artifact(key, path)).\ntrainer_run.artifact('model').meta.export(\"model.zip\")\nHyper-parameter tuning and model/experiment comparison#\nRun a GridSearch with a couple of parameters, and select the best run with respect to the max accuracy.\n(For more details, see MLRun Hyper-Param and Iterative jobs.)\nFor basic usage you can run the hyperparameters tuning job by using the arguments:\nhyperparams for the hyperparameters options and values of choice.\nselector for specifying how to select the best model.\nRunning a remote function\nTo run the hyper-param task over the cluster you need the input data to be available for the job, using object storage or the MLRun versioned artifact store.\nThe following line logs (and uploads) the dataframe as a project artifact:\ndataset_artifact = project.log_dataset(\"cancer-dataset\", df=breast_cancer_dataset, index=False)\nRun the function over the remote Kubernetes cluster (local is not set):\nhp_tuning_run = project.run_function(\n\"trainer\",\ninputs={\"dataset\": dataset_artifact.uri},\nhyperparams={\n\"n_estimators\": [10, 100, 1000],\n\"learning_rate\": [1e-1, 1e-3],\n\"max_depth\": [2, 8]\n},\nselector=\"max.accuracy\",\n)\n> 2022-09-20 13:57:28,217 [info] starting run trainer-train uid=b7696b221a174f66979be01138797f19 DB=http://mlrun-api:8080\n> 2022-09-20 13:57:28,365 [info] Job is running in the background, pod: trainer-train-xfzfp\nMatplotlib created a temporary config/cache directory at /tmp/matplotlib-zuih5pkq because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n> 2022-09-20 13:58:07,356 [info] best iteration=3, used criteria max.accuracy\n> 2022-09-20 13:58:07,750 [info] run executed, status=completed\nfinal state: completed\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\ntutorial-iguazio\n...38797f19\n0\nSep 20 13:57:33\ncompleted\ntrainer-train\nv3io_user=iguaziokind=jobowner=iguaziomlrun/client_version=1.1.0\ndataset\nbest_iteration=3accuracy=0.9649122807017544f1_score=0.9722222222222222precision_score=0.958904109589041recall_score=0.9859154929577465\nfeature-importancetest_setconfusion-matrixroc-curvescalibration-curvemodeliteration_resultsparallel_coordinates\nTitle\n×\n> to track results use the .show() or .logs() methods  or click here to open in UI> 2022-09-20 13:58:13,925 [info] run executed, status=completed\nView Hyper-param results and the selected run in the MLRun UI\nInteractive Parallel Coordinates Plot\nList the generated models and compare the different runs\nhp_tuning_run.outputs\n{'best_iteration': 3,\n'accuracy': 0.9649122807017544,\n'f1_score': 0.9722222222222222,\n'precision_score': 0.958904109589041,\n'recall_score': 0.9859154929577465,\n'feature-importance': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/3/feature-importance.html',\n'test_set': 'store://artifacts/tutorial-iguazio/trainer-train_test_set:b7696b221a174f66979be01138797f19',\n'confusion-matrix': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/3/confusion-matrix.html',\n'roc-curves': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/3/roc-curves.html',\n'calibration-curve': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/3/calibration-curve.html',\n'model': 'store://artifacts/tutorial-iguazio/cancer_classifier:b7696b221a174f66979be01138797f19',\n'iteration_results': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/0/iteration_results.csv',\n'parallel_coordinates': 'v3io:///projects/tutorial-iguazio/artifacts/trainer-train/0/parallel_coordinates.html'}\n# List the models in the project (can apply filters)\nmodels = project.list_models()\nfor model in models:\nprint(f\"uri: {model.uri}, metrics: {model.metrics}\")\nuri: store://models/tutorial-iguazio/cancer_classifier#0:b3f1bc3379324767bee22f44942b96e4, metrics: {'accuracy': 0.956140350877193, 'f1_score': 0.965034965034965, 'precision_score': 0.9583333333333334, 'recall_score': 0.971830985915493}\nuri: store://models/tutorial-iguazio/cancer_classifier#1:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.956140350877193, 'f1_score': 0.965034965034965, 'precision_score': 0.9583333333333334, 'recall_score': 0.971830985915493}\nuri: store://models/tutorial-iguazio/cancer_classifier#2:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.956140350877193, 'f1_score': 0.965034965034965, 'precision_score': 0.9583333333333334, 'recall_score': 0.971830985915493}\nuri: store://models/tutorial-iguazio/cancer_classifier#3:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.9649122807017544, 'f1_score': 0.9722222222222222, 'precision_score': 0.958904109589041, 'recall_score': 0.9859154929577465}\nuri: store://models/tutorial-iguazio/cancer_classifier#4:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.6228070175438597, 'f1_score': 0.7675675675675676, 'precision_score': 0.6228070175438597, 'recall_score': 1.0}\nuri: store://models/tutorial-iguazio/cancer_classifier#5:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.6228070175438597, 'f1_score': 0.7675675675675676, 'precision_score': 0.6228070175438597, 'recall_score': 1.0}\nuri: store://models/tutorial-iguazio/cancer_classifier#6:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.956140350877193, 'f1_score': 0.965034965034965, 'precision_score': 0.9583333333333334, 'recall_score': 0.971830985915493}\nuri: store://models/tutorial-iguazio/cancer_classifier#7:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.9385964912280702, 'f1_score': 0.951048951048951, 'precision_score': 0.9444444444444444, 'recall_score': 0.9577464788732394}\nuri: store://models/tutorial-iguazio/cancer_classifier#8:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.9473684210526315, 'f1_score': 0.9577464788732394, 'precision_score': 0.9577464788732394, 'recall_score': 0.9577464788732394}\nuri: store://models/tutorial-iguazio/cancer_classifier#9:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.9473684210526315, 'f1_score': 0.9577464788732394, 'precision_score': 0.9577464788732394, 'recall_score': 0.9577464788732394}\nuri: store://models/tutorial-iguazio/cancer_classifier#10:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.6228070175438597, 'f1_score': 0.7675675675675676, 'precision_score': 0.6228070175438597, 'recall_score': 1.0}\nuri: store://models/tutorial-iguazio/cancer_classifier#11:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.6228070175438597, 'f1_score': 0.7675675675675676, 'precision_score': 0.6228070175438597, 'recall_score': 1.0}\nuri: store://models/tutorial-iguazio/cancer_classifier#12:b7696b221a174f66979be01138797f19, metrics: {'accuracy': 0.9385964912280702, 'f1_score': 0.951048951048951, 'precision_score': 0.9444444444444444, 'recall_score': 0.9577464788732394}\n# To view the full model object use:\n# print(models[0].to_yaml())\n# Compare the runs (generate interactive parallel coordinates plot and a table)\nproject.list_runs(name=\"trainer-train\", iter=True).compare()\nuid\niter\nstart\nstate\nname\nparameters\nresults\n...38797f19\n12\nSep 20 13:57:59\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.001max_depth=8\naccuracy=0.9385964912280702f1_score=0.951048951048951precision_score=0.9444444444444444recall_score=0.9577464788732394\n...38797f19\n11\nSep 20 13:57:57\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.001max_depth=8\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n10\nSep 20 13:57:55\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.001max_depth=8\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n9\nSep 20 13:57:53\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.1max_depth=8\naccuracy=0.9473684210526315f1_score=0.9577464788732394precision_score=0.9577464788732394recall_score=0.9577464788732394\n...38797f19\n8\nSep 20 13:57:51\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=8\naccuracy=0.9473684210526315f1_score=0.9577464788732394precision_score=0.9577464788732394recall_score=0.9577464788732394\n...38797f19\n7\nSep 20 13:57:50\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.1max_depth=8\naccuracy=0.9385964912280702f1_score=0.951048951048951precision_score=0.9444444444444444recall_score=0.9577464788732394\n...38797f19\n6\nSep 20 13:57:45\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.001max_depth=2\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\n...38797f19\n5\nSep 20 13:57:43\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.001max_depth=2\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n4\nSep 20 13:57:42\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.001max_depth=2\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n3\nSep 20 13:57:38\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.1max_depth=2\naccuracy=0.9649122807017544f1_score=0.9722222222222222precision_score=0.958904109589041recall_score=0.9859154929577465\n...38797f19\n2\nSep 20 13:57:36\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=2\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\n...38797f19\n1\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.1max_depth=2\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\n...38797f19\n1\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.1max_depth=2\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\n...38797f19\n2\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=2\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\n...38797f19\n3\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.1max_depth=2\naccuracy=0.9649122807017544f1_score=0.9722222222222222precision_score=0.958904109589041recall_score=0.9859154929577465\n...38797f19\n4\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.001max_depth=2\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n5\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.001max_depth=2\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n6\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.001max_depth=2\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\n...38797f19\n7\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.1max_depth=8\naccuracy=0.9385964912280702f1_score=0.951048951048951precision_score=0.9444444444444444recall_score=0.9577464788732394\n...38797f19\n8\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=8\naccuracy=0.9473684210526315f1_score=0.9577464788732394precision_score=0.9577464788732394recall_score=0.9577464788732394\n...38797f19\n9\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.1max_depth=8\naccuracy=0.9473684210526315f1_score=0.9577464788732394precision_score=0.9577464788732394recall_score=0.9577464788732394\n...38797f19\n10\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=10learning_rate=0.001max_depth=8\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n11\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.001max_depth=8\naccuracy=0.6228070175438597f1_score=0.7675675675675676precision_score=0.6228070175438597recall_score=1.0\n...38797f19\n12\nSep 20 13:57:33\ncompleted\ntrainer-train\nn_estimators=1000learning_rate=0.001max_depth=8\naccuracy=0.9385964912280702f1_score=0.951048951048951precision_score=0.9444444444444444recall_score=0.9577464788732394\n...942b96e4\n0\nSep 20 13:56:57\ncompleted\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=3\naccuracy=0.956140350877193f1_score=0.965034965034965precision_score=0.9583333333333334recall_score=0.971830985915493\n...bc1c8a7d\n0\nSep 20 13:56:16\nerror\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=3\n...b28ad36f\n0\nSep 20 13:56:03\nerror\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=3\n...5dee41a3\n0\nSep 20 13:55:25\nrunning\ntrainer-train\nn_estimators=100learning_rate=0.1max_depth=3\nBuild and test the model serving functions#\nMLRun serving can produce managed, real-time, serverless, pipelines composed of various data processing and ML tasks. The pipelines use the Nuclio real-time serverless engine, which can be deployed anywhere. For more details and examples, see the MLRun Serving Graphs.\nCreate a model serving function from your code, and (view it here)\nserving_fn = mlrun.new_function(\"serving\", image=\"mlrun/mlrun\", kind=\"serving\")\nserving_fn.add_model('cancer-classifier',model_path=hp_tuning_run.outputs[\"model\"], class_name='mlrun.frameworks.sklearn.SklearnModelServer')\n<mlrun.serving.states.TaskStep at 0x7feb1f55faf0>\n# Create a mock (simulator of the real-time function)\nserver = serving_fn.to_mock_server()\nmy_data = {\"inputs\"\n:[[\n1.371e+01, 2.083e+01, 9.020e+01, 5.779e+02, 1.189e-01, 1.645e-01,\n9.366e-02, 5.985e-02, 2.196e-01, 7.451e-02, 5.835e-01, 1.377e+00,\n3.856e+00, 5.096e+01, 8.805e-03, 3.029e-02, 2.488e-02, 1.448e-02,\n1.486e-02, 5.412e-03, 1.706e+01, 2.814e+01, 1.106e+02, 8.970e+02,\n1.654e-01, 3.682e-01, 2.678e-01, 1.556e-01, 3.196e-01, 1.151e-01]\n]\n}\nserver.test(\"/v2/models/cancer-classifier/infer\", body=my_data)\n> 2022-09-20 14:12:35,714 [warning] run command, file or code were not specified\n> 2022-09-20 14:12:35,859 [info] model cancer-classifier was loaded\n> 2022-09-20 14:12:35,860 [info] Loaded ['cancer-classifier']\n/conda/envs/mlrun-extended/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning:\nX does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n{'id': 'd9aee47cadc042ebbd9474ec0179a446',\n'model_name': 'cancer-classifier',\n'outputs': [0]}\nDone!#\nCongratulation! You’ve completed Part 2 of the MLRun getting-started tutorial.\nProceed to Part 3: Model serving to learn how to deploy and serve your model using a serverless function."}
{"text": "Serving pre-trained ML/DL models#\nThis notebook demonstrate how to serve standard ML/DL models using MLRun Serving.\nMake sure you went over the basics in MLRun Quick Start Tutorial.\nMLRun serving can produce managed real-time serverless pipelines from various tasks, including MLRun models or standard model files.\nThe pipelines use the Nuclio real-time serverless engine, which can be deployed anywhere.\nNuclio is a high-performance open-source “serverless” framework that’s focused on data, I/O, and compute-intensive workloads.\nMLRun serving supports advanced real-time data processing and model serving pipelines.\nFor more details and examples, see the MLRun serving pipelines documentation.\nTutorial steps:\nUsing pre-built MLRun serving classes and images\nCreate and test the serving function\nDeploy the serving function\nBuild a custom serving class\nBuilding advanced model serving graph\nWatch the video tutorial.\nMLRun installation and configuration#\nBefore running this notebook make sure the mlrun package is installed (pip install mlrun) and that you have configured the access to MLRun service.\n# Install MLRun if not installed, run this only once. Restart the notebook after the install!\n%pip install mlrun\nGet or create a new project\nYou should create, load or use (get) an MLRun Project. The get_or_create_project() method tries to load the project from the MLRun DB. If the project does not exist it creates a new one.\nimport mlrun\nproject = mlrun.get_or_create_project(\"tutorial\", context=\"./\", user_project=True)\n> 2022-06-20 09:07:50,188 [info] loaded project tutorial from MLRun DB\nUsing pre-built MLRun serving classes and images#\nMLRun contains built-in serving functionality for the major ML/DL frameworks (Scikit-Learn, TensorFlow.Keras, ONNX, XGBoost, LightGBM, and PyTorch). In addition, MLRun provides a few container images with the required ML/DL packages pre-installed.\nYou can overwrite the packages in the images, or provide your own image. (You just need to make sure that the mlrun package is installed in it.)\nThe following table specifies, for each framework, the relevant pre-integrated image and the corresponding MLRun ModelServer serving class:\nframework\nimage\nserving class\nSciKit-Learn\nmlrun/mlrun\nmlrun.frameworks.sklearn.SklearnModelServer\nTensorFlow.Keras\nmlrun/ml-models\nmlrun.frameworks.tf_keras.TFKerasModelServer\nONNX\nmlrun/ml-models\nmlrun.frameworks.onnx.ONNXModelServer\nXGBoost\nmlrun/ml-models\nmlrun.frameworks.xgboost.XGBoostModelServer\nLightGBM\nmlrun/ml-models\nmlrun.frameworks.lgbm.LGBMModelServer\nPyTorch\nmlrun/ml-models\nmlrun.frameworks.pytorch.PyTorchModelServer\nFor GPU support use the mlrun/ml-models-gpu image (adding GPU drivers and support)\nExample using SKlearn and TF Keras models\nSee how to specify the parameters in the following two examples. These use standard pre-trained models (using the iris dataset) stored in MLRun samples repository. (You can use your own models instead.)\nmodels_dir = mlrun.get_sample_path('models/serving/')\nframework = 'sklearn'  # change to 'keras' to try the 2nd option\nkwargs = {}\nif framework == \"sklearn\":\nserving_class = 'mlrun.frameworks.sklearn.SklearnModelServer'\nmodel_path = models_dir + 'sklearn.pkl'\nimage = 'mlrun/mlrun'\nelse:\nserving_class = 'mlrun.frameworks.tf_keras.TFKerasModelServer'\nmodel_path = models_dir + 'keras.h5'\nimage = 'mlrun/ml-models'  # or mlrun/ml-models-gpu when using GPUs\nkwargs['labels'] = {'model-format': 'h5'}\nLog the model#\nThe model and its metadata are first registered in MLRun’s Model Registry. Use the log_model() method to specify the model files and metadata (metrics, schema, parameters, etc.).\nmodel_object = project.log_model(f'{framework}-model', model_file=model_path, **kwargs)\nCreate and test the serving function#\nCreate a new serving function, specify its name and the correct image (with your desired framework).\nIf you want to add specific packages to the base image, specify the requirements attribute, example:\nserving_fn = mlrun.new_function(\"serving\", image=image, kind=\"serving\", requirements=[\"tensorflow==2.8.1\"])\nThe following example uses a basic topology of a model router and adds a single model behind it. (You can add multiple models to the same function.)\nserving_fn = mlrun.new_function(\"serving\", image=image, kind=\"serving\", requirements={})\nserving_fn.add_model(framework ,model_path=model_object.uri, class_name=serving_class, to_list=True)\n# Plot the serving topology input -> router -> model\nserving_fn.plot(rankdir=\"LR\")\nSimulate the model server locally (using the mock_server)\n# Create a mock server that represents the serving pipeline\nserver = serving_fn.to_mock_server()\nTest the mock model server endpoint\nList the served models\nserver.test(\"/v2/models/\", method=\"GET\")\n{'models': ['sklearn']}\nInfer using test data\nsample = {\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}\nserver.test(path=f'/v2/models/{framework}/infer',body=sample)\n{'id': '1da64557daa843c1a2d6719eea7d4361',\n'model_name': 'sklearn',\n'outputs': [0, 2]}\nSee more API options and parameters in Model serving API.\nDeploy the serving function#\nDeploy the serving function and use invoke to test it with the provided sample.\nserving_fn.with_code(body=\" \") # adds the serving wrapper, not required with MLRun >= 1.0.3\nproject.deploy_function(serving_fn)\n> 2022-06-20 09:07:56,977 [info] Starting remote function deploy\n2022-06-20 09:07:57  (info) Deploying function\n2022-06-20 09:07:57  (info) Building\n2022-06-20 09:07:57  (info) Staging files and preparing base images\n2022-06-20 09:07:57  (info) Building processor image\n2022-06-20 09:08:32  (info) Build complete\n2022-06-20 09:08:44  (info) Function deploy complete\n> 2022-06-20 09:08:44,641 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-tutorial-yaron-serving.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['tutorial-yaron-serving-tutorial-yaron.default-tenant.app.yh43.iguazio-cd1.com/']}\nDeployStatus(state=ready, outputs={'endpoint': 'http://tutorial-yaron-serving-tutorial-yaron.default-tenant.app.yh43.iguazio-cd1.com/', 'name': 'tutorial-yaron-serving'})\nserving_fn.invoke(path=f'/v2/models/{framework}/infer',body=sample)\n> 2022-06-20 09:08:44,692 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-tutorial-yaron-serving.default-tenant.svc.cluster.local:8080/v2/models/sklearn/infer'}\n{'id': 'a16f00e8-663a-4031-a04e-e42a7d4dd697',\n'model_name': 'sklearn',\n'outputs': [0, 2]}\nBuild a custom serving class#\nModel serving classes implement the full model serving functionality, which include loading models, pre- and post-processing, prediction, explainability, and model monitoring.\nModel serving classes must inherit from mlrun.serving.V2ModelServer, and at the minimum implement the load() (download the model file(s) and load the model into memory) and predict() (accept request payload and return prediction/inference results) methods.\nFor more detailed information on custom serving classes, see Build your own model serving class.\nThe following code demonstrates a minimal scikit-learn (a.k.a. sklearn) serving-class implementation:\nfrom cloudpickle import load\nimport numpy as np\nfrom typing import List\nimport mlrun\nclass ClassifierModel(mlrun.serving.V2ModelServer):\ndef load(self):\n\"\"\"load and initialize the model and/or other elements\"\"\"\nmodel_file, extra_data = self.get_model('.pkl')\nself.model = load(open(model_file, 'rb'))\ndef predict(self, body: dict) -> List:\n\"\"\"Generate model predictions from sample.\"\"\"\nfeats = np.asarray(body['inputs'])\nresult: np.ndarray = self.model.predict(feats)\nreturn result.tolist()\nIn order to create a function that incorporates the code of the new class (in serving.py ) use code_to_function:\nserving_fn = mlrun.code_to_function('serving', filename='serving.py', kind='serving',image='mlrun/mlrun')\nserving_fn.add_model('my_model',model_path=model_file, class_name='ClassifierModel')\nBuild an advanced model serving graph#\nMLRun graphs enable building and running DAGs (directed acyclic graphs). Graphs are composed of individual steps.\nThe first graph element accepts an Event object, transforms/processes the event and passes the result to the next step\nin the graph, and so on. The final result can be written out to a destination (file, DB, stream, etc.) or returned back to the caller\n(one of the graph steps can be marked with .respond()).\nThe serving graphs can be composed of pre-defined graph steps, block-type elements (model servers, routers, ensembles,\ndata readers and writers, data engineering tasks, validators, etc.), custom steps, or from native python\nclasses/functions. A graph can have data processing steps, model ensembles, model servers, post-processing, etc.\nGraphs can auto-scale and span multiple function containers (connected through streaming protocols).\nSee the Advanced Model Serving Graph Notebook Example.\nDone!#\nCongratulations! You’ve completed Part 3 of the MLRun getting-started tutorial.\nProceed to Part 4: ML Pipeline to learn how to create an automated pipeline for your project."}
{"text": "Projects and automated ML pipeline#\nThis notebook demonstrates how to work with projects, source control (git), and automating the ML pipeline.\nMake sure you went over the basics in MLRun Quick Start Tutorial.\nMLRun Project is a container for all your work on a particular activity: all the associated code, functions, jobs, workflows, data, models, and artifacts. Projects can be mapped to git repositories to enable versioning, collaboration, and CI/CD.\nYou can create project definitions using the SDK or a yaml file and store those in the MLRun DB, a file, or an archive.\nOnce the project is loaded you can run jobs/workflows that refer to any project element by name, allowing separation between configuration and code. See load projects for details.\nProjects contain workflows that execute the registered functions in a sequence/graph (DAG), and that can reference project parameters, secrets and artifacts by name. MLRun currently supports two workflow engines, local (for simple tasks) and Kubeflow Pipelines (for more complex/advanced tasks). MLRun also supports a real-time workflow engine (see online serving pipelines (graphs).\nAn ML Engineer can gather the different functions created by the data engineer and data scientist and create this automated pipeline.\nTutorial steps:\nSet up the project and functions\nWork with GIT and archives\nBuild and run automated ML pipelines and CI/CD\nTest the deployed model endpoint\nMLRun installation and configuration#\nBefore running this notebook make sure the mlrun package is installed (pip install mlrun) and that you have configured the access to MLRun service.\n# Install MLRun if not installed, run this only once (restart the notebook after the install !!!)\n%pip install mlrun\nSet up the project and functions#\nGet or create a project\nThere are three ways to create/load MLRun projects:\nmlrun.projects.new_project()  — Create a new MLRun project and optionally load it from a yaml/zip/git template.\nmlrun.projects.load_project() — Load a project from a context directory or remote git/zip/tar archive.\nmlrun.projects.get_or_create_project() — Load a project from the MLRun DB if it exists, or from a specified\ncontext/archive.\nProjects refer to a context directory that holds all the project code and configuration. The context dir is\nusually mapped to a git repository and/or to an IDE (PyCharm, VSCode, etc.) project.\nimport mlrun\nproject = mlrun.get_or_create_project(\"tutorial\", context=\"./\", user_project=True)\n> 2022-09-20 14:59:47,322 [info] loaded project tutorial from MLRun DB\nRegister project functions#\nTo run workflows, you must save the definitions for the functions in the project so that function objects are initialized\nautomatically when you load a project or when running a project version in automated CI/CD workflows. In addition, you might want to set/register other project attributes such as global parameters, secrets, and data.\nFunctions are registered using the set_function() command, where you can specify the code, requirements, image, etc.\nFunctions can be created from a single code/notebook file or have access to the entire project context directory. (By adding the with_repo=True flag, it guarantees that the project context is cloned into the function runtime environment).\nFunction registration examples:\n# Example: register a notebook file as a function\nproject.set_function('mynb.ipynb', name='test-function', image=\"mlrun/mlrun\", handler=\"run_test\")\n# Define a job (batch) function that uses code/libs from the project repo\nproject.set_function(\nname=\"myjob\", handler=\"my_module.job_handler\",\nimage=\"mlrun/mlrun\", kind=\"job\", with_repo=True,\n)\nFunction code\nRun the following cell to generate the data prep file (or copy it manually):\n%%writefile data-prep.py\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nimport mlrun\n@mlrun.handler(outputs=[\"dataset\", \"label_column\"])\ndef breast_cancer_generator():\n\"\"\"\nA function that generates the breast cancer dataset\n\"\"\"\nbreast_cancer = load_breast_cancer()\nbreast_cancer_dataset = pd.DataFrame(\ndata=breast_cancer.data, columns=breast_cancer.feature_names\n)\nbreast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"label\"])\nbreast_cancer_dataset = pd.concat(\n[breast_cancer_dataset, breast_cancer_labels], axis=1\n)\nreturn breast_cancer_dataset, \"label\"\nOverwriting data-prep.py\nRegister the function above in the project\nproject.set_function(\"data-prep.py\", name=\"data-prep\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"breast_cancer_generator\")\n<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fd96c30a0a0>\nRegister additional project objects and metadata\nYou can define other objects (workflows, artifacts, secrets) and parameters in the project and use them in your functions, for example:\n# Register a simple named artifact in the project (to be used in workflows)\ndata_url = 'https://s3.wasabisys.com/iguazio/data/iris/iris.data.raw.csv'\nproject.set_artifact('data', target_path=data_url)\n# Add a multi-stage workflow (./workflow.py) to the project with the name 'main' and save the project\nproject.set_workflow('main', \"./workflow.py\")\n# Read env vars from dict or file and set as project secrets\nproject.set_secrets({\"SECRET1\": \"value\"})\nproject.set_secrets(file_path=\"secrets.env\")\nproject.spec.params = {\"x\": 5}\nSave the project\n# Save the project in the db (and into the project.yaml file)\nproject.save()\n<mlrun.projects.project.MlrunProject at 0x7fd96c2fdb50>\nWhen you save the project it stores the project definitions in the project.yaml. This allows reconstructing the project in a remote cluster or a CI/CD system.\nSee the generated project file: project.yaml.\nWork with GIT and archives#\nPush the project code/metadata into an archive#\nUse standard git commands to push the current project tree into a git archive. Make sure you .save() the project before pushing it.\ngit remote add origin <server>\ngit commit -m \"Commit message\"\ngit push origin master\nAlternatively, you can use MLRun SDK calls:\nproject.create_remote(git_uri, branch=branch) — to register the remote Git path\nproject.push() — save the project state and commit/push updates to the remote git repo\nYou can also save the project content and metadata into a local or remote .zip archive, for example:\nproject.export(\"../archive1.zip\")\nproject.export(\"s3://my-bucket/archive1.zip\")\nproject.export(f\"v3io://projects/{project.name}/archive1.zip\")\nLoad a project from local/remote archive#\nThe project metadata and context (code and configuration) can be loaded and initialized using the load_project() method.\nWhen url (of the git/zip/tar) is specified, it clones a remote repo into the local context dir.\n# Load the project and run the 'main' workflow\nproject = load_project(context=\"./\", name=\"myproj\", url=\"git://github.com/mlrun/project-archive.git\")\nproject.run(\"main\", arguments={'data': data_url})\nProjects can also be loaded and executed using the CLI:\nmlrun project -n myproj -u \"git://github.com/mlrun/project-archive.git\" .\nmlrun project -r main -w -a data=<data-url> .\n# load the project in the current context dir\nproject = mlrun.load_project(\"./\")\nBuild and run automated ML pipelines and CI/CD#\nA pipeline is created by running an MLRun “workflow”.\nThe following code defines a workflow and writes it to a file in your local directory, with the file name workflow.py.\nThe workflow describes a directed acyclic graph (DAG) which is executed using the local, remote, or kubeflow engines.\nSee running a multi-stage workflow.\nThe defined pipeline includes the following steps:\nGenerate/prepare the data (ingest).\nTrain and the model (train).\nDeploy the model as a real-time serverless function (serving).\nNote\nA pipeline can also include continuous build integration and deployment (CI/CD) steps, such as building container images and deploying models.\n%%writefile './workflow.py'\nfrom kfp import dsl\nimport mlrun\n# Create a Kubeflow Pipelines pipeline\n@dsl.pipeline(name=\"breast-cancer-demo\")\ndef pipeline(model_name=\"cancer-classifier\"):\n# Run the ingestion function with the new image and params\ningest = mlrun.run_function(\n\"data-prep\",\nname=\"get-data\",\noutputs=[\"dataset\"],\n)\n# Train a model using the auto_trainer hub function\ntrain = mlrun.run_function(\n\"hub://auto_trainer\",\ninputs={\"dataset\": ingest.outputs[\"dataset\"]},\nparams = {\n\"model_class\": \"sklearn.ensemble.RandomForestClassifier\",\n\"train_test_split_size\": 0.2,\n\"label_columns\": \"label\",\n\"model_name\": model_name,\n},\nhandler='train',\noutputs=[\"model\"],\n)\n# Deploy the trained model as a serverless function\nserving_fn = mlrun.new_function(\"serving\", image=\"mlrun/mlrun\", kind=\"serving\")\nserving_fn.with_code(body=\" \")\nmlrun.deploy_function(\nserving_fn,\nmodels=[\n{\n\"key\": model_name,\n\"model_path\": train.outputs[\"model\"],\n\"class_name\": 'mlrun.frameworks.sklearn.SklearnModelServer',\n}\n],\n)\nWriting ./workflow.py\nRun the workflow\n# Run the workflow\nrun_id = project.run(\nworkflow_path=\"./workflow.py\",\narguments={\"model_name\": \"cancer-classifier\"},\nwatch=True)\nPipeline running (id=6907fa23-dcdc-49bd-adbd-dfb7f8d25997), click here to view the details in MLRun UIRun ResultsWorkflow 6907fa23-dcdc-49bd-adbd-dfb7f8d25997 finished, state=Succeededclick the hyper links below to see detailed results\nuid\nstart\nstate\nname\nparameters\nresults\n...acd812a8\nSep 20 15:00:35\ncompleted\nauto-trainer-train\nmodel_class=sklearn.ensemble.RandomForestClassifiertrain_test_split_size=0.2label_columns=labelmodel_name=cancer-classifier\naccuracy=0.956140350877193f1_score=0.9635036496350365precision_score=0.9565217391304348recall_score=0.9705882352941176\n...02536782\nSep 20 15:00:07\ncompleted\nget-data\nlabel_column=label\nView the pipeline in MLRun UI\nRun workflows using the CLI\nWith MLRun you can use a single command to load the code from local dir or remote archive (Git, zip, …) and execute a pipeline. This can be very useful for integration with CI/CD frameworks and practices. See CI/CD integration for more details.\nThe following command loads the project from the current dir (.) and executes the workflow with an argument, for running locally (without k8s).\nmlrun project -r ./workflow.py -w -a model_name=classifier2 .!mlrun project -r ./workflow.py -w -a model_name=classifier2 .\nTest the deployed model endpoint#\nNow that your model is deployed using the pipeline, you can invoke it as usual:\nserving_fn = project.get_function(\"serving\")\n# Create a mock (simulator of the real-time function)\nmy_data = {\"inputs\"\n:[[\n1.371e+01, 2.083e+01, 9.020e+01, 5.779e+02, 1.189e-01, 1.645e-01,\n9.366e-02, 5.985e-02, 2.196e-01, 7.451e-02, 5.835e-01, 1.377e+00,\n3.856e+00, 5.096e+01, 8.805e-03, 3.029e-02, 2.488e-02, 1.448e-02,\n1.486e-02, 5.412e-03, 1.706e+01, 2.814e+01, 1.106e+02, 8.970e+02,\n1.654e-01, 3.682e-01, 2.678e-01, 1.556e-01, 3.196e-01, 1.151e-01]\n]\n}\nserving_fn.invoke(\"/v2/models/cancer-classifier/infer\", body=my_data)\n> 2022-09-20 15:09:02,664 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-tutorial-iguazio-serving.default-tenant.svc.cluster.local:8080/v2/models/cancer-classifier/infer'}\n{'id': '7ecaf987-bd79-470e-b930-19959808b678',\n'model_name': 'cancer-classifier',\n'outputs': [0]}\nDone!#\nCongratulations! You’ve completed Part 4 of the MLRun getting-started tutorial. To continue, proceed to  Part 5 Model monitoring and drift detection.\nYou might also want to explore the following demos:\nFor an example of distributed training pipeline using TensorFlow, Keras, and PyTorch, see the mask detection demo.\nTo learn more about deploying live endpoints and concept drift, see the network-operations (NetOps) demo.\nTo learn about using the feature store to process raw transactions and events in real-time and respond and block transactions before they occur, see the Fraud prevention demo.\nFor an example of a pipeline that summarizes and extracts keywords from a news article URL, see the News article summarization and keyword extraction via NLP."}
{"text": "Model monitoring and drift detection#\nThis tutorial illustrates leveraging the model monitoring capabilities of MLRun to deploy a model to a live endpoint and calculate data drift.\nMake sure you have reviewed the basics in MLRun Quick Start Tutorial.\nTutorial steps:\nCreate an MLRun project\nLog a model with a given framework and training set\nImport and deploy serving function\nSimulate production traffic\nView drift calculations and status\nView detailed drift dashboards\nMLRun installation and configuration#\nBefore running this notebook make sure mlrun is installed and that you have configured the access to the MLRun service.\n# Install MLRun if not installed, run this only once (restart the notebook after the install !!!)\n%pip install mlrun\nSet up the project#\nFirst, import the dependencies and create an MLRun project. This  contains all of the models, functions, datasets, etc:\nimport os\nimport mlrun\nimport pandas as pd\nproject = mlrun.get_or_create_project(name=\"tutorial\", context=\"./\", user_project=True)\n> 2022-09-21 08:58:03,005 [info] loaded project tutorial from MLRun DB\nNote\nThis tutorial does not focus on training a model. Instead, it starts with a trained model and its corresponding training dataset.\nNext, log the following model file and dataset to deploy and calculate data drift. The model is a AdaBoostClassifier from sklearn, and the dataset is in csv format.\nmodel_path = mlrun.get_sample_path('models/model-monitoring/model.pkl')\ntraining_set_path = mlrun.get_sample_path('data/model-monitoring/iris_dataset.csv')\nLog the model with training data#\nLog the model using MLRun experiment tracking. This is usually done in a training pipeline, but you can also bring in your pre-trained models from other sources. See Working with data and model artifacts and Automated experiment tracking for more information.\nmodel_name = \"RandomForestClassifier\"\nmodel_artifact = project.log_model(\nkey=model_name,\nmodel_file=model_path,\nframework=\"sklearn\",\ntraining_set=pd.read_csv(training_set_path),\nlabel_column=\"label\"\n)\n# the model artifact unique URI\nmodel_artifact.uri\n'store://models/tutorial-nick/RandomForestClassifier#0:latest'\nImport and deploy the serving function#\nImport the model server function from the MLRun Function Hub. Additionally, mount the filesytem, add the model that was logged via experiment tracking, and enable drift detection.\nThe core line here is serving_fn.set_tracking() that creates the required infrastructure behind the scenes to perform drift detection. See the Model monitoring overview for more info on what is deployed.\n# Import the serving function from the Function Hub and mount filesystem\nserving_fn = mlrun.import_function('hub://v2_model_server', new_name=\"serving\")\n# Add the model to the serving function's routing spec\nserving_fn.add_model(model_name, model_path=model_artifact.uri)\n# Enable model monitoring\nserving_fn.set_tracking()\nDeploy the serving function with drift detection#\nDeploy the serving function with drift detection enabled with a single line of code:\nmlrun.deploy_function(serving_fn)\n> 2022-09-21 08:58:08,053 [info] Starting remote function deploy\n2022-09-21 08:58:09  (info) Deploying function\n2022-09-21 08:58:09  (info) Building\n2022-09-21 08:58:10  (info) Staging files and preparing base images\n2022-09-21 08:58:10  (info) Building processor image\n2022-09-21 08:58:55  (info) Build complete\n2022-09-21 08:59:03  (info) Function deploy complete\n> 2022-09-21 08:59:04,232 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-tutorial-nick-serving.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['tutorial-nick-serving-tutorial-nick.default-tenant.app.us-sales-350.iguazio-cd1.com/']}\nDeployStatus(state=ready, outputs={'endpoint': 'http://tutorial-nick-serving-tutorial-nick.default-tenant.app.us-sales-350.iguazio-cd1.com/', 'name': 'tutorial-nick-serving'})\nView deployed resources#\nAt this point, you should see the newly deployed model server, as well as a model-monitoring-stream, and a scheduled job (in yellow). The model-monitoring-stream collects, processes, and saves the incoming requests to the model server. The scheduled job does the actual calculation (by default every hour).\nNote\nYou will not see model-monitoring-batch jobs listed until they actually run (by default every hour).\nSimulate production traffic#\nNext, use the following code to simulate incoming production data using elements from the training set. Because the data is coming from the same training set you logged, you should not expect any data drift.\nNote\nBy default, the drift calculation starts via the scheduled hourly batch job after receiving 10,000 incoming requests.\nimport json\nimport logging\nfrom random import choice, uniform\nfrom time import sleep\nfrom tqdm import tqdm\n# Suppress print messages\nlogging.getLogger(name=\"mlrun\").setLevel(logging.WARNING)\n# Get training set as list\niris_data = pd.read_csv(training_set_path).drop(\"label\", axis=1).to_dict(orient=\"split\")[\"data\"]\n# Simulate traffic using random elements from training set\nfor i in tqdm(range(12_000)):\ndata_point = choice(iris_data)\nserving_fn.invoke(f'v2/models/{model_name}/infer', json.dumps({'inputs': [data_point]}))\n# Resume normal logging\nlogging.getLogger(name=\"mlrun\").setLevel(logging.INFO)\n100%|██████████| 12000/12000 [06:45<00:00, 29.63it/s]\nView drift calculations and status#\nOnce data drift has been calculated, you can view it in the MLRun UI. This includes a high level overview of the model status:\nA more detailed view on model information and overall drift metrics:\nAs well as a view for feature-level distributions and drift metrics:\nView detailed drift dashboards#\nFinally, there are also more detailed Grafana dashboards that show additional information on each model in the project:\nFor more information on accessing these dashboards, see Model monitoring using Grafana dashboards.\nGraphs of individual features over time:\nAs well as drift and operational metrics over time:"}
{"text": "Add MLOps to existing code#\nThis tutorial showcases how easy it is to apply MLRun on your existing code. With only 7 lines of code, you get:\nExperiment tracking — Track every single run of your experiment to learn what yielded the best results.\nAutomatic Logging — Log datasets, metrics results and plots with one line of code. MLRun takes care for all the rest.\nParameterization — Enable running your code with different parameters, run hyperparameters tuning and get the most out of your code.\nResource management — Control the amount of resources available for your experiment.\nUse this kaggle code by Sylas as an example, part of the competition New York City Taxi Fare Prediction.\nTutorial steps:\nGet the data\nCode review\nRun the script with MLRun\nReview outputs\nGet the data#\nYou can download the original data from kaggle. However, since the original data is 5.7GB in size, this demo uses sampled data. Since this demo uses MLRun’s DataItem to pass the datasets, the sampled data is downloaded automatically. However, if you want to look at the data, you can download it: training set, and testing set.\nCode review#\nUse the original code with the minimum changes required to apply MLRun to it. The code itself is straightforward:\nRead the training data and perform feature engineering on it to preprocess it for training.\nTrain a LightGBM regression model using LightGBM’s train function.\nRead the testing data and save the contest expected submission file.\nYou can Download the script.py file[Download here], or copy / paste it from here:\nShow code\nimport gc\nimport lightgbm as lgbm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# [MLRun] Import MLRun:\nimport mlrun\nfrom mlrun.frameworks.lgbm import apply_mlrun\n# [MLRun] Get MLRun's context:\ncontext = mlrun.get_or_create_ctx(\"apply-mlrun-tutorial\")\n# [MLRun] Reading train data from context instead of local file:\ntrain_df = context.get_input(\"train_set\", \"./train.csv\").as_df()\n# train_df =  pd.read_csv('./train.csv')\n# Drop rows with null values\ntrain_df = train_df.dropna(how=\"any\", axis=\"rows\")\ndef clean_df(df):\nreturn df[\n(df.fare_amount > 0)\n& (df.fare_amount <= 500)\n&\n# (df.passenger_count >= 0) & (df.passenger_count <= 8)  &\n(\n(df.pickup_longitude != 0)\n& (df.pickup_latitude != 0)\n& (df.dropoff_longitude != 0)\n& (df.dropoff_latitude != 0)\n)\n]\ntrain_df = clean_df(train_df)\n# To Compute Haversine distance\ndef sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n\"\"\"\nReturn distance along great radius between pickup and dropoff coordinates.\n\"\"\"\n# Define earth radius (km)\nR_earth = 6371\n# Convert degrees to radians\npickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(\nnp.radians, [pickup_lat, pickup_lon, dropoff_lat, dropoff_lon]\n)\n# Compute distances along lat, lon dimensions\ndlat = dropoff_lat - pickup_lat\ndlon = dropoff_lon - pickup_lon\n# Compute haversine distance\na = (\nnp.sin(dlat / 2.0) ** 2\n+ np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon / 2.0) ** 2\n)\nreturn 2 * R_earth * np.arcsin(np.sqrt(a))\ndef sphere_dist_bear(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n\"\"\"\nReturn distance along great radius between pickup and dropoff coordinates.\n\"\"\"\n# Convert degrees to radians\npickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(\nnp.radians, [pickup_lat, pickup_lon, dropoff_lat, dropoff_lon]\n)\n# Compute distances along lat, lon dimensions\ndlon = pickup_lon - dropoff_lon\n# Compute bearing distance\na = np.arctan2(\nnp.sin(dlon * np.cos(dropoff_lat)),\nnp.cos(pickup_lat) * np.sin(dropoff_lat)\n- np.sin(pickup_lat) * np.cos(dropoff_lat) * np.cos(dlon),\n)\nreturn a\ndef radian_conv(degree):\n\"\"\"\nReturn radian.\n\"\"\"\nreturn np.radians(degree)\ndef add_airport_dist(dataset):\n\"\"\"\nReturn minumum distance from pickup or dropoff coordinates to each airport.\nJFK: John F. Kennedy International Airport\nEWR: Newark Liberty International Airport\nLGA: LaGuardia Airport\nSOL: Statue of Liberty\nNYC: Newyork Central\n\"\"\"\njfk_coord = (40.639722, -73.778889)\newr_coord = (40.6925, -74.168611)\nlga_coord = (40.77725, -73.872611)\nsol_coord = (40.6892, -74.0445)  # Statue of Liberty\nnyc_coord = (40.7141667, -74.0063889)\npickup_lat = dataset[\"pickup_latitude\"]\ndropoff_lat = dataset[\"dropoff_latitude\"]\npickup_lon = dataset[\"pickup_longitude\"]\ndropoff_lon = dataset[\"dropoff_longitude\"]\npickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1])\ndropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon)\npickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\ndropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon)\npickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1])\ndropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon)\npickup_sol = sphere_dist(pickup_lat, pickup_lon, sol_coord[0], sol_coord[1])\ndropoff_sol = sphere_dist(sol_coord[0], sol_coord[1], dropoff_lat, dropoff_lon)\npickup_nyc = sphere_dist(pickup_lat, pickup_lon, nyc_coord[0], nyc_coord[1])\ndropoff_nyc = sphere_dist(nyc_coord[0], nyc_coord[1], dropoff_lat, dropoff_lon)\ndataset[\"jfk_dist\"] = pickup_jfk + dropoff_jfk\ndataset[\"ewr_dist\"] = pickup_ewr + dropoff_ewr\ndataset[\"lga_dist\"] = pickup_lga + dropoff_lga\ndataset[\"sol_dist\"] = pickup_sol + dropoff_sol\ndataset[\"nyc_dist\"] = pickup_nyc + dropoff_nyc\nreturn dataset\ndef add_datetime_info(dataset):\n# Convert to datetime format\ndataset[\"pickup_datetime\"] = pd.to_datetime(\ndataset[\"pickup_datetime\"], format=\"%Y-%m-%d %H:%M:%S UTC\"\n)\ndataset[\"hour\"] = dataset.pickup_datetime.dt.hour\ndataset[\"day\"] = dataset.pickup_datetime.dt.day\ndataset[\"month\"] = dataset.pickup_datetime.dt.month\ndataset[\"weekday\"] = dataset.pickup_datetime.dt.weekday\ndataset[\"year\"] = dataset.pickup_datetime.dt.year\nreturn dataset\ntrain_df = add_datetime_info(train_df)\ntrain_df = add_airport_dist(train_df)\ntrain_df[\"distance\"] = sphere_dist(\ntrain_df[\"pickup_latitude\"],\ntrain_df[\"pickup_longitude\"],\ntrain_df[\"dropoff_latitude\"],\ntrain_df[\"dropoff_longitude\"],\n)\ntrain_df[\"bearing\"] = sphere_dist_bear(\ntrain_df[\"pickup_latitude\"],\ntrain_df[\"pickup_longitude\"],\ntrain_df[\"dropoff_latitude\"],\ntrain_df[\"dropoff_longitude\"],\n)\ntrain_df[\"pickup_latitude\"] = radian_conv(train_df[\"pickup_latitude\"])\ntrain_df[\"pickup_longitude\"] = radian_conv(train_df[\"pickup_longitude\"])\ntrain_df[\"dropoff_latitude\"] = radian_conv(train_df[\"dropoff_latitude\"])\ntrain_df[\"dropoff_longitude\"] = radian_conv(train_df[\"dropoff_longitude\"])\ntrain_df.drop(columns=[\"key\", \"pickup_datetime\"], inplace=True)\ny = train_df[\"fare_amount\"]\ntrain_df = train_df.drop(columns=[\"fare_amount\"])\nprint(train_df.head())\nx_train, x_test, y_train, y_test = train_test_split(\ntrain_df, y, random_state=123, test_size=0.10\n)\ndel train_df\ndel y\ngc.collect()\nparams = {\n\"boosting_type\": \"gbdt\",\n\"objective\": \"regression\",\n\"nthread\": 4,\n\"num_leaves\": 31,\n\"learning_rate\": 0.05,\n\"max_depth\": -1,\n\"subsample\": 0.8,\n\"bagging_fraction\": 1,\n\"max_bin\": 5000,\n\"bagging_freq\": 20,\n\"colsample_bytree\": 0.6,\n\"metric\": \"rmse\",\n\"min_split_gain\": 0.5,\n\"min_child_weight\": 1,\n\"min_child_samples\": 10,\n\"scale_pos_weight\": 1,\n\"zero_as_missing\": True,\n\"seed\": 0,\n\"num_rounds\": 50000,\n}\ntrain_set = lgbm.Dataset(\nx_train,\ny_train,\nsilent=False,\ncategorical_feature=[\"year\", \"month\", \"day\", \"weekday\"],\n)\nvalid_set = lgbm.Dataset(\nx_test,\ny_test,\nsilent=False,\ncategorical_feature=[\"year\", \"month\", \"day\", \"weekday\"],\n)\n# [MLRun] Apply MLRun on the LightGBM module:\napply_mlrun(context=context)\nmodel = lgbm.train(\nparams,\ntrain_set=train_set,\nnum_boost_round=10000,\nearly_stopping_rounds=500,\nvalid_sets=[valid_set],\n)\ndel x_train\ndel y_train\ndel x_test\ndel y_test\ngc.collect()\n# [MLRun] Reading test data from context instead of local file:\ntest_df = context.get_input(\"test_set\", \"./test.csv\").as_df()\n# test_df =  pd.read_csv('./test.csv')\nprint(test_df.head())\ntest_df = add_datetime_info(test_df)\ntest_df = add_airport_dist(test_df)\ntest_df[\"distance\"] = sphere_dist(\ntest_df[\"pickup_latitude\"],\ntest_df[\"pickup_longitude\"],\ntest_df[\"dropoff_latitude\"],\ntest_df[\"dropoff_longitude\"],\n)\ntest_df[\"bearing\"] = sphere_dist_bear(\ntest_df[\"pickup_latitude\"],\ntest_df[\"pickup_longitude\"],\ntest_df[\"dropoff_latitude\"],\ntest_df[\"dropoff_longitude\"],\n)\ntest_df[\"pickup_latitude\"] = radian_conv(test_df[\"pickup_latitude\"])\ntest_df[\"pickup_longitude\"] = radian_conv(test_df[\"pickup_longitude\"])\ntest_df[\"dropoff_latitude\"] = radian_conv(test_df[\"dropoff_latitude\"])\ntest_df[\"dropoff_longitude\"] = radian_conv(test_df[\"dropoff_longitude\"])\ntest_key = test_df[\"key\"]\ntest_df = test_df.drop(columns=[\"key\", \"pickup_datetime\"])\n# Predict from test set\nprediction = model.predict(test_df, num_iteration=model.best_iteration)\nsubmission = pd.DataFrame({\"key\": test_key, \"fare_amount\": prediction})\n# [MLRun] Log the submission instead of saving it locally:\ncontext.log_dataset(key=\"taxi_fare_submission\", df=submission, format=\"csv\")\n# submission.to_csv('taxi_fare_submission.csv',index=False)\nThis demo focuses on reviewing the changes / additions made to the original code so that you can apply MLRun on top of it. Seven lines of code are added / replaced as you can see in the sections below:\nInitialization#\nImports#\nOn lines 9-10, add 2 imports:\nmlrun — Import MLRun of course.\napply_mlrun — Use the apply_mlrun function from MLRun’s frameworks, a sub-package for common ML/DL frameworks integrations with MLRun.\nimport mlrun\nfrom mlrun.frameworks.lgbm import apply_mlrun\nMLRun context#\nTo get parameters and inputs into the code, you need to get MLRun’s context. Use the function get_or_create_ctx.\nLine 13:\ncontext = mlrun.get_or_create_ctx(\"apply-mlrun-tutorial\")\nGet Training Set#\nIn the original code the training set was read from a local file. Now you want to get it from the user who runs the code.\nUse the context to get the \"training_set\" input by using the get_input method. To maintain the original logic, include the default path for when the training set was not provided by the user.\nLine 16:\ntrain_df = context.get_input(\"train_set\", \"./train.csv\").as_df()\n# Instead of: `train_df =  pd.read_csv('./train.csv')`\nApply MLRun#\nNow use the apply_mlrun function from MLRun’s LightGBM framework integration. MLRun automatically wraps the LightGBM module and enables automatic logging and evaluation.\nLine 219:\napply_mlrun(context=context)\nLogging the dataset#\nSimilar to the way you got the training set, you get the test dataset as an input from the MLRun content.\nLine 235:\ntest_df = context.get_input(\"test_set\", \"./test.csv\").as_df()\n# Instead of: `test_df =  pd.read_csv('./test.csv')`\nSave the submission#\nFinally, instead of saving the result locally, log the submission to MLRun.\nLine 267:\ncontext.log_dataset(key=\"taxi_fare_submission\", df=submission, format=\"csv\")\n# Instead of: `submission.to_csv('taxi_fare_submission.csv',index=False)`\nRun the script with MLRun#\nNow you can run the script and see MLRun in action.\nimport mlrun\nCreate a project#\nCreate a project using the function get_or_create_project. To read more about MLRun projects, see Projects.\nproject = mlrun.get_or_create_project(name=\"apply-mlrun-tutorial\", context=\"./\", user_project=True)\n> 2022-08-09 18:21:26,785 [info] loaded project apply-mlrun-tutorial from MLRun DB\nCreate a function#\nCreate an MLRun function using the function code_to_function. To read more about MLRun functions, see Serverless functions.\nscript_function = mlrun.code_to_function(\nfilename=\"./src/script.py\",\nname=\"apply-mlrun-tutorial-function\",\nkind=\"job\",\nimage=\"mlrun/ml-models\"\n)\n<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f20a5dfe250>\nRun the function#\nNow you can run the function, providing it with the inputs you want. Use the datasets links to send them to the function. MLRun downloads and reads them into pd.DataFrame automatically.\nscript_run = script_function.run(\ninputs={\n\"train_set\": \"https://s3.us-east-1.wasabisys.com/iguazio/data/nyc-taxi/train.csv\",\n\"test_set\": \"https://s3.us-east-1.wasabisys.com/iguazio/data/nyc-taxi/test.csv\"\n},\n)\n> 2022-08-09 18:21:26,851 [info] starting run apply-mlrun-tutorial-function uid=8d82ef16a15d4151a16060c13b133170 DB=http://mlrun-api:8080\n> 2022-08-09 18:21:27,017 [info] handler was not provided running main (./script.py)\n> 2022-08-09 18:21:39,330 [info] logging run results to: http://mlrun-api:8080\npickup_longitude  pickup_latitude  ...  distance   bearing\n0         -1.288826         0.710721  ...  1.030764 -2.918897\n1         -1.291824         0.710546  ...  8.450134 -0.375217\n2         -1.291242         0.711418  ...  1.389525  2.599961\n3         -1.291319         0.710927  ...  2.799270  0.133905\n4         -1.290987         0.711536  ...  1.999157 -0.502703\n[5 rows x 17 columns]\n[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008352 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 55092\n[LightGBM] [Info] Number of data points in the train set: 194071, number of used features: 17\n[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n[LightGBM] [Info] Start training from score 11.335573\nkey  ... passenger_count\n0  2015-01-27 13:08:24.0000002  ...               1\n1  2015-01-27 13:08:24.0000003  ...               1\n2  2011-10-08 11:53:44.0000002  ...               1\n3  2012-12-01 21:12:12.0000002  ...               1\n4  2012-12-01 21:12:12.0000003  ...               1\n[5 rows x 7 columns]\nproject\nuid\niter\nstart\nstate\nname\nlabels\ninputs\nparameters\nresults\nartifacts\napply-mlrun-tutorial-guyl\n...3b133170\n0\nAug 09 18:21:39\ncompleted\napply-mlrun-tutorial-function\nv3io_user=guylkind=owner=guylhost=jupyter-guyl-66857b7999-xnncv\ntrain_settest_set\nvalid_0_rmse=3.905279481685527\nvalid_0_rmse_plotvalid_0-feature-importancevalid_0modeltaxi_fare_submission\nTitle\n×\n> 2022-08-09 18:22:02,987 [info] run executed, status=completed\nReview outputs#\nTo view the outputs yielded by the MLRun automatic logging and evaluation, call the outputs property on the run object:\nscript_run.outputs\n{'valid_0_rmse': 3.905279481685527,\n'valid_0_rmse_plot': 'v3io:///projects/apply-mlrun-tutorial-guyl/artifacts/apply-mlrun-tutorial-function/0/valid_0_rmse_plot.html',\n'valid_0-feature-importance': 'v3io:///projects/apply-mlrun-tutorial-guyl/artifacts/apply-mlrun-tutorial-function/0/valid_0-feature-importance.html',\n'valid_0': 'store://artifacts/apply-mlrun-tutorial-guyl/apply-mlrun-tutorial-function_valid_0:8d82ef16a15d4151a16060c13b133170',\n'model': 'store://artifacts/apply-mlrun-tutorial-guyl/model:8d82ef16a15d4151a16060c13b133170',\n'taxi_fare_submission': 'store://artifacts/apply-mlrun-tutorial-guyl/apply-mlrun-tutorial-function_taxi_fare_submission:8d82ef16a15d4151a16060c13b133170'}\nMLRun automatically detects all the metrics calculated and collects the data along with the training. Here there was one validation set named valid_0 and the RMSE metric was calculated on it. You can see the RMSE values per iteration plot and the final score including the features importance plot.\nYou can explore the different artifacts by calling the artifact function like this:\nscript_run.artifact('valid_0_rmse_plot').show()\nscript_run.artifact('valid_0-feature-importance').show()\nAnd of course, you can also see the submission that was logged:\nscript_run.artifact('taxi_fare_submission').show()\nkey\nfare_amount\n0\n2015-01-27 13:08:24.0000002\n10.281408\n1\n2015-01-27 13:08:24.0000003\n11.019641\n2\n2011-10-08 11:53:44.0000002\n4.898061\n3\n2012-12-01 21:12:12.0000002\n7.758042\n4\n2012-12-01 21:12:12.0000003\n15.298775\n...\n...\n...\n9909\n2015-05-10 12:37:51.0000002\n9.117569\n9910\n2015-01-12 17:05:51.0000001\n10.850885\n9911\n2015-04-19 20:44:15.0000001\n55.048856\n9912\n2015-01-31 01:05:19.0000005\n20.110280\n9913\n2015-01-18 14:06:23.0000006\n7.081041\n9914 rows × 2 columns"}
{"text": "Batch inference and drift detection#\nThis tutorial leverages a function from the MLRun Function Hub to perform batch inference using a logged model and a new prediction dataset. The function also calculates data drift by comparing the new prediction dataset with the original training set.\nMake sure you have reviewed the basics in MLRun Quick Start Tutorial.\nTutorial steps:\nSet up an MLRun project\nView the data\nLog a model with a given framework and training set\nImport and run the batch inference function\nView predictions and drift status\nMLRun installation and Configuration#\nBefore running this notebook make sure mlrun is installed and that you have configured the access to the MLRun service.\n# Install MLRun if not installed, run this only once (restart the notebook after the install !!!)\n%pip install mlrun\nSet up a project#\nFirst, import the dependencies and create an MLRun project. The project contains all of your models, functions, datasets, etc:\nimport mlrun\nimport os\nimport pandas as pd\nproject = mlrun.get_or_create_project(\"tutorial\", context=\"./\", user_project=True)\n> 2022-09-20 19:15:36,113 [info] Created and saved project batch-predict: {'from_template': None, 'overwrite': False, 'context': './', 'save': True}\nNote\nThis tutorial does not focus on training a model. Instead, it starts with a trained model and its corresponding training and prediction dataset.\nYou will use the following model files and datasets to perform the batch prediction. The model is a DecisionTreeClassifier from sklearn and the datasets are in parquet format.\nmodel_path = mlrun.get_sample_path('models/batch-predict/model.pkl')\ntraining_set_path = mlrun.get_sample_path('data/batch-predict/training_set.parquet')\nprediction_set_path = mlrun.get_sample_path('data/batch-predict/prediction_set.parquet')\nView the data#\nThe training data has 20 numerical features and a binary (0,1) label:\npd.read_parquet(training_set_path).head()\nfeature_0\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\nfeature_7\nfeature_8\nfeature_9\n...\nfeature_11\nfeature_12\nfeature_13\nfeature_14\nfeature_15\nfeature_16\nfeature_17\nfeature_18\nfeature_19\nlabel\n0\n0.572754\n0.171079\n0.403080\n0.955429\n0.272039\n0.360277\n-0.995429\n0.437239\n0.991556\n0.010004\n...\n0.112194\n-0.319256\n-0.392631\n-0.290766\n1.265054\n1.037082\n-1.200076\n0.820992\n0.834868\n0\n1\n0.623733\n-0.149823\n-1.410537\n-0.729388\n-1.996337\n-1.213348\n1.461307\n1.187854\n-1.790926\n-0.981600\n...\n0.428653\n-0.503820\n-0.798035\n2.038105\n-3.080463\n0.408561\n1.647116\n-0.838553\n0.680983\n1\n2\n0.814168\n-0.221412\n0.020822\n1.066718\n-0.573164\n0.067838\n0.923045\n0.338146\n0.981413\n1.481757\n...\n-1.052559\n-0.241873\n-1.232272\n-0.010758\n0.806800\n0.661162\n0.589018\n0.522137\n-0.924624\n0\n3\n1.062279\n-0.966309\n0.341471\n-0.737059\n1.460671\n0.367851\n-0.435336\n0.445308\n-0.655663\n-0.196220\n...\n0.641017\n0.099059\n1.902592\n-1.024929\n0.030703\n-0.198751\n-0.342009\n-1.286865\n-1.118373\n1\n4\n0.195755\n0.576332\n-0.260496\n0.841489\n0.398269\n-0.717972\n0.810550\n-1.058326\n0.368610\n0.606007\n...\n0.195267\n0.876144\n0.151615\n0.094867\n0.627353\n-0.389023\n0.662846\n-0.857000\n1.091218\n1\n5 rows × 21 columns\nThe prediciton data has 20 numerical features, but no label - this is what you will predict:\npd.read_parquet(prediction_set_path).head()\nfeature_0\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\nfeature_7\nfeature_8\nfeature_9\nfeature_10\nfeature_11\nfeature_12\nfeature_13\nfeature_14\nfeature_15\nfeature_16\nfeature_17\nfeature_18\nfeature_19\n0\n-2.059506\n-1.314291\n2.721516\n-2.132869\n-0.693963\n0.376643\n3.017790\n3.876329\n-1.294736\n0.030773\n0.401491\n2.775699\n2.361580\n0.173441\n0.879510\n1.141007\n4.608280\n-0.518388\n0.129690\n2.794967\n1\n-1.190382\n0.891571\n3.726070\n0.673870\n-0.252565\n-0.729156\n2.646563\n4.782729\n0.318952\n-0.781567\n1.473632\n1.101721\n3.723400\n-0.466867\n-0.056224\n3.344701\n0.194332\n0.463992\n0.292268\n4.665876\n2\n-0.996384\n-0.099537\n3.421476\n0.162771\n-1.143458\n-1.026791\n2.114702\n2.517553\n-0.154620\n-0.465423\n-1.723025\n1.729386\n2.820340\n-1.041428\n-0.331871\n2.909172\n2.138613\n-0.046252\n-0.732631\n4.716266\n3\n-0.289976\n-1.680019\n3.126478\n-0.704451\n-1.149112\n1.174962\n2.860341\n3.753661\n-0.326119\n2.128411\n-0.508000\n2.328688\n3.397321\n-0.932060\n-1.442370\n2.058517\n3.881936\n2.090635\n-0.045832\n4.197315\n4\n-0.294866\n1.044919\n2.924139\n0.814049\n-1.455054\n-0.270432\n3.380195\n2.339669\n1.029101\n-1.171018\n-1.459395\n1.283565\n0.677006\n-2.147444\n-0.494150\n3.222041\n6.219348\n-1.914110\n0.317786\n4.143443\nLog the model with training data#\nNext, log the model using MLRun experiment tracking. This is usually done in a training pipeline, but you can also bring in your pre-trained models from other sources. See Working with data and model artifacts and Automated experiment tracking for more information.\nIn this example, you are logging a training set with the model for future comparison, however you can also directly pass in your training set to the batch prediction function.\nmodel_artifact = project.log_model(\nkey=\"model\",\nmodel_file=model_path,\nframework=\"sklearn\",\ntraining_set=pd.read_parquet(training_set_path),\nlabel_column=\"label\"\n)\n# the model artifact unique URI\nmodel_artifact.uri\n'store://models/batch-predict/model#0:latest'\nImport and run the batch inference function#\nNext, import the batch inference function from the MLRun Function Hub:\nfn = mlrun.import_function(\"hub://batch_inference\")\nRun batch inference#\nFinally, perform the batch prediction by passing in your model and datasets. See the corresponding batch inference example notebook for an exhaustive list of other parameters that are supported:\nrun = project.run_function(\nfn,\ninputs={\n\"dataset\": prediction_set_path,\n# If you do not log a dataset with your model, you can pass it in here:\n#       \"sample_set\" : training_set_path\n},\nparams={\n\"model\": model_artifact.uri,\n\"perform_drift_analysis\" : True,\n},\n)\nView predictions and drift status#\nThese are the batch predicictions on the prediction set from the model:\nrun.artifact(\"prediction\").as_df().head()\nfeature_0\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\nfeature_7\nfeature_8\nfeature_9\n...\nfeature_11\nfeature_12\nfeature_13\nfeature_14\nfeature_15\nfeature_16\nfeature_17\nfeature_18\nfeature_19\nlabel\n0\n-2.059506\n-1.314291\n2.721516\n-2.132869\n-0.693963\n0.376643\n3.017790\n3.876329\n-1.294736\n0.030773\n...\n2.775699\n2.361580\n0.173441\n0.879510\n1.141007\n4.608280\n-0.518388\n0.129690\n2.794967\n0\n1\n-1.190382\n0.891571\n3.726070\n0.673870\n-0.252565\n-0.729156\n2.646563\n4.782729\n0.318952\n-0.781567\n...\n1.101721\n3.723400\n-0.466867\n-0.056224\n3.344701\n0.194332\n0.463992\n0.292268\n4.665876\n0\n2\n-0.996384\n-0.099537\n3.421476\n0.162771\n-1.143458\n-1.026791\n2.114702\n2.517553\n-0.154620\n-0.465423\n...\n1.729386\n2.820340\n-1.041428\n-0.331871\n2.909172\n2.138613\n-0.046252\n-0.732631\n4.716266\n0\n3\n-0.289976\n-1.680019\n3.126478\n-0.704451\n-1.149112\n1.174962\n2.860341\n3.753661\n-0.326119\n2.128411\n...\n2.328688\n3.397321\n-0.932060\n-1.442370\n2.058517\n3.881936\n2.090635\n-0.045832\n4.197315\n1\n4\n-0.294866\n1.044919\n2.924139\n0.814049\n-1.455054\n-0.270432\n3.380195\n2.339669\n1.029101\n-1.171018\n...\n1.283565\n0.677006\n-2.147444\n-0.494150\n3.222041\n6.219348\n-1.914110\n0.317786\n4.143443\n1\n5 rows × 21 columns\nThere is also a drift table plot that compares the drift between the training data and prediction data per feature:\nrun.artifact(\"drift_table_plot\").show()\nFinally, you also get a numerical drift metric and boolean flag denoting whether or not data drift is detected:\nrun.status.results\n{'drift_status': False, 'drift_metric': 0.29934242566253266}\n# Data/concept drift per feature\nimport json\njson.loads(run.artifact(\"features_drift_results\").get())\n{'feature_6': 0.7262042202197605,\n'feature_2': 0.7391279921664593,\n'feature_16': 0.7181622588902428,\n'feature_0': 0.028086840976606773,\n'feature_3': 0.043769819014849734,\n'feature_18': 0.04443732609382538,\n'feature_15': 0.6329075683793959,\n'feature_1': 0.04485072701663093,\n'feature_12': 0.7034787615778625,\n'feature_14': 0.046364723781764774,\n'feature_8': 0.039060131873550404,\n'feature_10': 0.042567035578799796,\n'feature_11': 0.7221431701127441,\n'feature_5': 0.05184219833790496,\n'feature_9': 0.04468363504674985,\n'feature_19': 0.7902698698155215,\n'feature_4': 0.042755641152500176,\n'feature_13': 0.04239724655474124,\n'feature_7': 0.7297906294873706,\n'feature_17': 0.03587785749574268,\n'label': 0.017413285340161608}\nNext steps#\nIn a production setting, you probably want to incorporate this as part of a larger pipeline or application.\nFor example, if you use this function for the prediction capabilities, you can pass the prediction output as the input to another pipeline step, store it in an external location like S3, or send to an application or user.\nIf you use this function for the drift detection capabilities, you can use the drift_status and drift_metrics outputs to automate further pipeline steps, send a notification, or kick off a re-training pipeline."}
{"text": "Tutorials and Examples#\nThe following tutorials provide a hands-on introduction to using MLRun to implement a data science workflow and automate machine-learning operations (MLOps).\nQuick-start Tutorial ( watch video)\nTargeted Tutorials\nEnd to End Demos\nMake sure you start with the Quick start tutorial to understand the basics\nIntroduction to MLRun - Use serverless functions to train and deploy models\nTargeted Tutorials#\nEach of the following tutorials is a dedicated Jupyter notebook. You can download them by clicking the download icon at the top of each page.\nTrain, compare, and register Models\nDemo of training ML models, hyper-parameters, track and compare experiments, register and use the models.\nServing pre-trained ML/DL models\nHow to deploy real-time serving pipelines with MLRun Serving and different types of pre-trained ML/DL models.\nProjects & automated ML pipeline\nHow to work with projects, source control (git), CI/CD, to easily build and deploy multi-stage ML pipelines.\nReal-time monitoring & drift detection\nDemonstrate MLRun Serving pipelines, MLRun model monitoring, and automated drift detection.\nAdd MLOps to existing code\nTurn a Kaggle research notebook to a production ML micro-service with minimal code changes using MLRun.\nBasic feature store example (stocks)\nUnderstand MLRun feature store with a simple example: build, transform, and serve features in batch and in real-time.\nBatch inference and drift detection\nUse MLRun batch inference function (from MLRun Function Hub), run it as a batch job, and generate drift reports.\nAdvanced real-time pipeline\nDemonstrates a multi-step online pipeline with data prep, ensemble, model serving, and post processing.\nFeature store end-to-end demo\nUse the feature store with data ingestion, model training, model serving, and automated pipeline.\nEnd to End Demos#\nYou can find the different end-to-end demos in the MLRun demos repository: github.com/mlrun/demos.\nRunning the demos in Open Source MLRun#\nBy default, these demos work with the online feature store, which is currently not part of the Open Source MLRun default deployment:\nfraud-prevention-feature-store\nnetwork-operations\nazureml_demo"}
