{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf3f934",
   "metadata": {},
   "source": [
    "# LLM Application Pipeline\n",
    "\n",
    "This notebook demonstrates how to serve [**Hugging Face**](https://huggingface.co/) LLM models using [**MLRun Serving**](https://docs.mlrun.org/en/stable/serving/serving-graph.html).\n",
    "\n",
    "> Make sure you went over the basics in MLRun [**Quick Start Tutorial**](https://docs.mlrun.org/en/stable/tutorial/01-mlrun-basics.html) to understand the MLRun basics.\n",
    "\n",
    "[MLRun serving](https://docs.mlrun.org/en/stable/serving/serving-graph.html) can produce managed ML application pipelines using real-time auto-scaling [Nuclio](https://nuclio.io/) serverless functions. The application pipeline includes all the steps from accepting events or data, preparing the required model features, inferencing results using one or more models, and driving actions.\n",
    "\n",
    "The tutorial has 4 main steps:\n",
    "1. [Define MLRun project and set all mlrun function](#project-setup)\n",
    "2. [Create and test an application pipeline](#serving-function)\n",
    "3. [Deploy the application pipeline](#deploy-serving)\n",
    "4. [Create web front-end using Gradio](#deploy-gradio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b5ab02-6818-448b-9b18-54e6f9285bba",
   "metadata": {},
   "source": [
    "But first, please install the following requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7566549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6996f0c3",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"project-setup\"></a>\n",
    "## 1. Define MLRun project and set all mlrun function\n",
    "\n",
    "Create or load an MLRun project that holds all your functions and configuration (see [project_setup.py](./src/project_setup.py))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "952631a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-26 10:34:33,644 [info] loaded project huggingface from MLRun DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Names with underscore '_' are about to be deprecated, use dashes '-' instead. Replacing underscores with dashes.\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "from src.project_setup import create_and_set_project\n",
    "\n",
    "project = create_and_set_project(\n",
    "    git_source=\"git://github.com/yonishelach/learn-docs.git#main\",\n",
    "    name=\"mlopspedia\",\n",
    "    default_image=\"yonishelach/mlrun-hf-gpu\",\n",
    "    user_project=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9cb1f",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"serving-function\"></a>\n",
    "## 2. Create and test an application pipeline\n",
    "\n",
    "The application pipeline intercepts the user request through HTTP and responds with a processed result. It consists the following steps:\n",
    "1. **Data Preprocessing** - Accept and process the incoming event, reformatting the input prompt as the LLM expects.\n",
    "2. **LLM Inference** - Use the LLM to generate response by the given prompt.\n",
    "3. **Data Postprocessing** - Process the prediction onto the next model - the toxicity classifier.\n",
    "4. **Toxicity Classification** - Turn the model response into a user-friendly output message in case toxicity was detected.\n",
    "\n",
    "You first create an MLRun serving function (specify the code, type, image, etc.). Then define a graph with the three steps. Finally, use the `plot()` method to visualize the application graph.\n",
    "\n",
    "See the data `pre/post-processing` functions in [**serving.py**](./src/serving.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a8789-f278-42af-b6a4-f9afd28a9e93",
   "metadata": {},
   "source": [
    "### 2.1. Define and Build the Serving Graph:\n",
    "Get the serving function from our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96d6261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function = project.get_function(\"serving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122db8d",
   "metadata": {},
   "source": [
    "Define and visualize the serving graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a826d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the topology and get the graph object:\n",
    "graph = serving_function.set_topology(\"flow\", engine=\"async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d923bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"723pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 723.05 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 719.05,-40 719.05,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"38.55,-0.05 40.7,-0.15 42.83,-0.3 44.92,-0.49 46.98,-0.74 48.99,-1.03 50.95,-1.36 52.84,-1.75 54.66,-2.18 56.4,-2.65 58.06,-3.16 59.63,-3.71 61.11,-4.31 62.49,-4.94 63.76,-5.61 64.93,-6.31 65.99,-7.04 66.93,-7.8 67.77,-8.59 68.48,-9.41 69.09,-10.25 69.58,-11.11 69.95,-11.99 70.21,-12.89 70.36,-13.8 70.4,-14.72 70.33,-15.65 70.16,-16.59 69.89,-17.53 69.53,-18.47 69.07,-19.41 68.52,-20.35 67.89,-21.28 67.18,-22.2 66.4,-23.11 65.55,-24.01 64.63,-24.89 63.65,-25.75 62.62,-26.59 61.53,-27.41 60.4,-28.2 59.23,-28.96 58.02,-29.69 56.78,-30.39 55.5,-31.06 54.2,-31.69 52.88,-32.29 51.53,-32.84 50.17,-33.35 48.79,-33.82 47.4,-34.25 46,-34.64 44.59,-34.97 43.17,-35.26 41.75,-35.51 40.32,-35.7 38.89,-35.85 37.45,-35.95 36.02,-36 34.58,-36 33.15,-35.95 31.71,-35.85 30.28,-35.7 28.85,-35.51 27.43,-35.26 26.01,-34.97 24.6,-34.64 23.2,-34.25 21.81,-33.82 20.43,-33.35 19.07,-32.84 17.72,-32.29 16.4,-31.69 15.1,-31.06 13.82,-30.39 12.58,-29.69 11.37,-28.96 10.2,-28.2 9.07,-27.41 7.98,-26.59 6.95,-25.75 5.97,-24.89 5.05,-24.01 4.2,-23.11 3.42,-22.2 2.71,-21.28 2.08,-20.35 1.53,-19.41 1.07,-18.47 0.71,-17.53 0.44,-16.59 0.27,-15.65 0.2,-14.72 0.24,-13.8 0.39,-12.89 0.65,-11.99 1.02,-11.11 1.51,-10.25 2.11,-9.41 2.83,-8.59 3.66,-7.8 4.61,-7.04 5.67,-6.31 6.84,-5.61 8.11,-4.94 9.49,-4.31 10.97,-3.71 12.54,-3.16 14.2,-2.65 15.94,-2.18 17.76,-1.75 19.65,-1.36 21.61,-1.03 23.62,-0.74 25.68,-0.49 27.77,-0.3 29.9,-0.15 32.05,-0.05 34.22,0 36.38,0 38.55,-0.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.3\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- preprocess -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>preprocess</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"168.34\" cy=\"-18\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.34\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">preprocess</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;preprocess -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;preprocess</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M70.05,-18C78.23,-18 87.29,-18 96.49,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.5,-21.5 106.5,-18 96.5,-14.5 96.5,-21.5\"/>\n",
       "</g>\n",
       "<!-- gpt2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>gpt2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"298.59\" cy=\"-18\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"298.59\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">gpt2</text>\n",
       "</g>\n",
       "<!-- preprocess&#45;&gt;gpt2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>preprocess&#45;&gt;gpt2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.46,-18C238.98,-18 247.58,-18 255.64,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"255.87,-21.5 265.87,-18 255.87,-14.5 255.87,-21.5\"/>\n",
       "</g>\n",
       "<!-- postprocess -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>postprocess</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"433.38\" cy=\"-18\" rx=\"66.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"433.38\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">postprocess</text>\n",
       "</g>\n",
       "<!-- gpt2&#45;&gt;postprocess -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>gpt2&#45;&gt;postprocess</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.39,-18C339.2,-18 347.89,-18 356.8,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"356.93,-21.5 366.93,-18 356.93,-14.5 356.93,-21.5\"/>\n",
       "</g>\n",
       "<!-- toxicity&#45;classifier -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>toxicity&#45;classifier</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"625.36\" cy=\"-18\" rx=\"89.88\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"625.36\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">toxicity&#45;classifier</text>\n",
       "</g>\n",
       "<!-- postprocess&#45;&gt;toxicity&#45;classifier -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>postprocess&#45;&gt;toxicity&#45;classifier</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M499.76,-18C508.01,-18 516.6,-18 525.23,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"525.37,-21.5 535.37,-18 525.37,-14.5 525.37,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fe84437d0d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the serving graph:\n",
    "graph.to(handler=\"clean_preprocess\", name=\"preprocess\")\\\n",
    "     .to(\"LLMModelServer\",\n",
    "         name=\"gpt2\",\n",
    "         model_name=\"gpt2\",\n",
    "         model_class=\"GPT2LMHeadModel\",\n",
    "         tokenizer_name=\"gpt2\",\n",
    "         tokenizer_class=\"GPT2Tokenizer\",\n",
    "         use_deepspeed=False)\\\n",
    "     .to(handler=\"clean_postprocess\", name=\"postprocess\")\\\n",
    "     .to(\"ToxicityClassifierModelServer\",\n",
    "         name=\"toxicity-classifier\",\n",
    "         threshold=0.7).respond()\n",
    "\n",
    "# Plot to graph:\n",
    "serving_function.plot(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc0bfc",
   "metadata": {},
   "source": [
    "Save the serving function in the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce3ffb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.projects.project.MlrunProject at 0x7f4fec582650>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# registering the serving \n",
    "project.set_function(serving_function, with_repo=True)\n",
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463dc84",
   "metadata": {},
   "source": [
    "### 2.2. Simulate the application pipeline locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213120a",
   "metadata": {},
   "source": [
    "Creating a mocking server for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cefcac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-26 10:34:36,557 [warning] run command, file or code were not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-26 10:34:37,589 [info] model sentiment-analysis was loaded\n"
     ]
    }
   ],
   "source": [
    "server = serving_function.to_mock_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac10a9",
   "metadata": {},
   "source": [
    "Test the `GPT2-Medium` knowledge on MLOps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3978e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sentiment is POSITIVE', 'The prediction score is 0.5296593904495239']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.test(path='/predict', body=\"What are training pipelines?\")[\"outputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1e50c",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"deploy-serving\"></a>\n",
    "## 3. Deploy the application pipeline (serving function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b85d8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-26 10:34:39,323 [info] Starting remote function deploy\n",
      "2023-02-26 10:34:39  (info) Deploying function\n",
      "2023-02-26 10:34:39  (info) Building\n",
      "2023-02-26 10:34:40  (info) Staging files and preparing base images\n",
      "2023-02-26 10:34:40  (info) Building processor image\n",
      "2023-02-26 10:36:55  (info) Build complete\n",
      "2023-02-26 10:37:17  (info) Function deploy complete\n",
      "> 2023-02-26 10:37:18,186 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-huggingface-davids-serving-pretrained.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['huggingface-davids-serving-pretrained-huggingface-davids.default-tenant.app.cto-office.iguazio-cd1.com/']}\n"
     ]
    }
   ],
   "source": [
    "deployment = project.deploy_function(serving_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a757e",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"deploy-gradio\"></a>\n",
    "## 4. Create web front-end using Gradio\n",
    "\n",
    "[**Gradio**](https://gradio.app/) provides a fast way to build a friendly web interface to demonstrate your machine-learning model. You can use it to input the text, submit the request to the application pipeline (via HTTP), and show the results.\n",
    "\n",
    "The following code defines a minimal app with an input box, a button, and two output boxes. The `launch()` method generates a local or remote web front-end application.\n",
    "\n",
    "The application pipeline URL is taken from the deployment task output (`deployment.outputs['endpoint']`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd37ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://92916b6c-a3e6-42b3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://92916b6c-a3e6-42b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "serving_url = deployment.outputs['endpoint']\n",
    "\n",
    "def sentiment(text):\n",
    "    # call the serving function with the input text\n",
    "    resp = requests.post(serving_url, json={\"text\": text})\n",
    "    return resp.json()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    input_box = [gr.Textbox(label=\"MLOps Subject to ask about:\", placeholder=\"Please insert text\", value=\"What are training pipelines?\")]\n",
    "    output = [gr.Textbox(label=\"Generated Answer\")]\n",
    "    greet_btn = gr.Button(\"Submit\")\n",
    "    greet_btn.click(fn=sentiment, inputs=input_box, outputs=output)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199e1bb",
   "metadata": {},
   "source": [
    "You can see the model results are not accurate enough. It is not quite the MLOps expert we wish it to be.\n",
    "\n",
    "In the [next notebook](./02-llm-fine-tuning.ipynb), we will fine-tune the model with relevant data and see how its knowledge improve.\n",
    "\n",
    "**Done!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7061b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
